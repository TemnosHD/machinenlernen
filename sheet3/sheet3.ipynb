{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.304227828979492\n"
     ]
    }
   ],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    w = torch.randn(size=shape)*0.01\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h_ = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2_ = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax.transpose(0,1)\n",
    "\n",
    "\n",
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    noise_py_x = noise_py_x.transpose(0,1)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.303811550140381\n",
      "Loss: 2.4472110271453857\n",
      "Loss: 2.292245864868164\n",
      "Loss: 2.394937038421631\n",
      "Loss: 2.237196922302246\n",
      "Loss: 2.2075231075286865\n",
      "Loss: 2.1441283226013184\n",
      "Loss: 2.008239507675171\n",
      "Loss: 1.9664784669876099\n",
      "Loss: 1.8960392475128174\n",
      "Loss: 1.7017848491668701\n",
      "Loss: 1.8484954833984375\n",
      "Loss: 1.740090012550354\n",
      "Loss: 1.3819324970245361\n",
      "Loss: 1.4581434726715088\n",
      "Loss: 1.2676910161972046\n",
      "Loss: 1.4671528339385986\n",
      "Loss: 1.0352412462234497\n",
      "Loss: 1.3838337659835815\n",
      "Loss: 0.9910264611244202\n",
      "Loss: 1.2821317911148071\n",
      "Loss: 0.933592677116394\n",
      "Loss: 0.8181854486465454\n",
      "Loss: 1.0227619409561157\n",
      "Loss: 1.1390184164047241\n",
      "Loss: 0.8551374077796936\n",
      "Loss: 0.858630359172821\n",
      "Loss: 0.8524398803710938\n",
      "Loss: 0.8306090831756592\n",
      "Loss: 0.764746904373169\n",
      "Loss: 0.7497779726982117\n",
      "Loss: 1.2407103776931763\n",
      "Loss: 0.7896282076835632\n",
      "Loss: 1.0625354051589966\n",
      "Loss: 0.7084797620773315\n",
      "Loss: 0.9143081903457642\n",
      "Loss: 0.9913882613182068\n",
      "Loss: 1.1526762247085571\n",
      "Loss: 0.7158167958259583\n",
      "Loss: 0.9760000109672546\n",
      "Loss: 0.5834033489227295\n",
      "Loss: 0.9484937787055969\n",
      "Loss: 0.887826144695282\n",
      "Loss: 0.9631707668304443\n",
      "Loss: 0.6344814896583557\n",
      "Loss: 0.77569180727005\n",
      "Loss: 0.5783310532569885\n",
      "Loss: 0.855557918548584\n",
      "Loss: 0.785605788230896\n",
      "Loss: 0.6604323387145996\n",
      "Loss: 0.6902643442153931\n",
      "Loss: 0.5916829109191895\n",
      "Loss: 0.582550585269928\n",
      "Loss: 0.6255059242248535\n",
      "Loss: 0.5278822183609009\n",
      "Loss: 0.4391552805900574\n",
      "Loss: 0.7104799151420593\n",
      "Loss: 0.6433985829353333\n",
      "Loss: 0.5442997813224792\n",
      "Loss: 0.6188858151435852\n",
      "Loss: 0.8065359592437744\n",
      "Loss: 0.4662952423095703\n",
      "Loss: 0.7971994280815125\n",
      "Loss: 0.6887319684028625\n",
      "Loss: 0.7015193104743958\n",
      "Loss: 0.714817225933075\n",
      "Loss: 0.5963805317878723\n",
      "Loss: 0.6549336910247803\n",
      "Loss: 0.4745970070362091\n",
      "Loss: 0.5492669343948364\n",
      "Loss: 0.63679039478302\n",
      "Loss: 0.6011408567428589\n",
      "Loss: 0.5333707928657532\n",
      "Loss: 0.5540711283683777\n",
      "Loss: 0.5583649277687073\n",
      "Loss: 0.6023623943328857\n",
      "Loss: 0.7517409324645996\n",
      "Loss: 0.6065083742141724\n",
      "Loss: 0.38842618465423584\n",
      "Loss: 0.43182432651519775\n",
      "Loss: 0.5779937505722046\n",
      "Loss: 0.5424186587333679\n",
      "Loss: 0.40734943747520447\n",
      "Loss: 0.6153333783149719\n",
      "Loss: 0.7364652752876282\n",
      "Loss: 0.5251052975654602\n",
      "Loss: 0.5939311385154724\n",
      "Loss: 0.5442988872528076\n",
      "Loss: 0.81600022315979\n",
      "Loss: 0.7795588970184326\n",
      "Loss: 0.5468649864196777\n",
      "Loss: 0.5434294939041138\n",
      "Loss: 0.6576409339904785\n",
      "Loss: 0.8202240467071533\n",
      "Loss: 0.6129579544067383\n",
      "Loss: 0.8258001208305359\n",
      "Loss: 0.44938138127326965\n",
      "Loss: 0.5436639785766602\n",
      "Loss: 0.6669402122497559\n",
      "Loss: 0.48085078597068787\n",
      "Loss: 0.3403793275356293\n",
      "Loss: 0.4843599200248718\n",
      "Loss: 0.6146560907363892\n",
      "Loss: 0.5078871250152588\n",
      "Loss: 0.6409554481506348\n",
      "Loss: 0.5237827897071838\n",
      "Loss: 0.6547241806983948\n",
      "Loss: 0.5119748115539551\n",
      "Loss: 0.4240010976791382\n",
      "Loss: 0.3973073661327362\n",
      "Loss: 0.5343912839889526\n",
      "Loss: 0.6492443084716797\n",
      "Loss: 0.5257025361061096\n",
      "Loss: 0.3524962365627289\n",
      "Loss: 0.36044827103614807\n",
      "Loss: 0.45892035961151123\n",
      "Loss: 0.5448922514915466\n",
      "Loss: 0.6288831233978271\n",
      "Loss: 0.2763536870479584\n",
      "Loss: 0.5150336027145386\n",
      "Loss: 0.47582897543907166\n",
      "Loss: 0.38175392150878906\n",
      "Loss: 0.3889451324939728\n",
      "Loss: 0.2916813790798187\n",
      "Loss: 0.5249477028846741\n",
      "Loss: 0.5515916347503662\n",
      "Loss: 0.6000354886054993\n",
      "Loss: 0.48780936002731323\n",
      "Loss: 0.4001312553882599\n",
      "Loss: 0.6504797339439392\n",
      "Loss: 0.5554597973823547\n",
      "Loss: 0.4943387508392334\n",
      "Loss: 0.6511409878730774\n",
      "Loss: 0.35750094056129456\n",
      "Loss: 0.5012309551239014\n",
      "Loss: 0.5199050307273865\n",
      "Loss: 0.6443434953689575\n",
      "Loss: 0.3283081352710724\n",
      "Loss: 0.8715335130691528\n",
      "Loss: 0.6277564764022827\n",
      "Loss: 0.6185768246650696\n",
      "Loss: 0.5985292196273804\n",
      "Loss: 0.5986067652702332\n",
      "Loss: 0.5082780122756958\n",
      "Loss: 0.6443288922309875\n",
      "Loss: 0.8959285616874695\n",
      "Loss: 0.4078635275363922\n",
      "Loss: 0.3565414845943451\n",
      "Loss: 0.4857374429702759\n",
      "Loss: 0.4404153823852539\n",
      "Loss: 0.5834243297576904\n",
      "Loss: 0.7090566158294678\n",
      "Loss: 0.4836675226688385\n",
      "Loss: 0.3970966041088104\n",
      "Loss: 0.5419777035713196\n",
      "Loss: 0.615688681602478\n",
      "Loss: 0.4910403788089752\n",
      "Loss: 0.45986390113830566\n",
      "Loss: 0.27215343713760376\n",
      "Loss: 0.4641689360141754\n",
      "Loss: 0.5674651861190796\n",
      "Loss: 0.44358694553375244\n",
      "Loss: 0.358264684677124\n",
      "Loss: 0.2430974841117859\n",
      "Loss: 0.48179447650909424\n",
      "Loss: 0.48538878560066223\n",
      "Loss: 0.428857684135437\n",
      "Loss: 0.18337270617485046\n",
      "Loss: 0.6895189881324768\n",
      "Loss: 0.4804358184337616\n",
      "Loss: 0.6260321140289307\n",
      "Loss: 0.5340413451194763\n",
      "Loss: 0.522799551486969\n",
      "Loss: 0.4367396831512451\n",
      "Loss: 0.6099596619606018\n",
      "Loss: 0.4536485970020294\n",
      "Loss: 0.41749873757362366\n",
      "Loss: 0.5368456840515137\n",
      "Loss: 0.4131329357624054\n",
      "Loss: 0.5053245425224304\n",
      "Loss: 0.3956930935382843\n",
      "Loss: 0.35001736879348755\n",
      "Loss: 0.38525184988975525\n",
      "Loss: 0.4006056487560272\n",
      "Loss: 0.45190533995628357\n",
      "Loss: 0.33022570610046387\n",
      "Loss: 0.5745271444320679\n",
      "Loss: 0.4496345818042755\n",
      "Loss: 0.46638351678848267\n",
      "Loss: 0.43902093172073364\n",
      "Loss: 0.3537972569465637\n",
      "Loss: 0.43281278014183044\n",
      "Loss: 0.5157548785209656\n",
      "Loss: 0.5883422493934631\n",
      "Loss: 0.2518303096294403\n",
      "Loss: 0.38858914375305176\n",
      "Loss: 0.46631088852882385\n",
      "Loss: 0.5299976468086243\n",
      "Loss: 0.49465617537498474\n",
      "Loss: 0.39623087644577026\n",
      "Loss: 0.5943689942359924\n",
      "Loss: 0.4889284372329712\n",
      "Loss: 0.5067320466041565\n",
      "Loss: 0.5074146389961243\n",
      "Loss: 0.44838467240333557\n",
      "Loss: 0.4672022759914398\n",
      "Loss: 0.43490660190582275\n",
      "Loss: 0.7769875526428223\n",
      "Loss: 0.48078083992004395\n",
      "Loss: 0.6785175204277039\n",
      "Loss: 0.4815542995929718\n",
      "Loss: 0.41931402683258057\n",
      "Loss: 0.2862737774848938\n",
      "Loss: 0.4187578856945038\n",
      "Loss: 0.38122594356536865\n",
      "Loss: 0.5063826441764832\n",
      "Loss: 0.5142132043838501\n",
      "Loss: 0.43430086970329285\n",
      "Loss: 0.5191208124160767\n",
      "Loss: 0.507032036781311\n",
      "Loss: 0.46150124073028564\n",
      "Loss: 0.49454161524772644\n",
      "Loss: 0.2748764753341675\n",
      "Loss: 0.49135053157806396\n",
      "Loss: 0.47090622782707214\n",
      "Loss: 0.4638882875442505\n",
      "Loss: 0.5314971804618835\n",
      "Loss: 0.608052670955658\n",
      "Loss: 0.272301584482193\n",
      "Loss: 0.31798332929611206\n",
      "Loss: 0.39604976773262024\n",
      "Loss: 0.4917261600494385\n",
      "Loss: 0.3868936598300934\n",
      "Loss: 0.3179500102996826\n",
      "Loss: 0.355057030916214\n",
      "Loss: 0.35496699810028076\n",
      "Loss: 0.5445602536201477\n",
      "Loss: 0.5232430100440979\n",
      "Loss: 0.4029141366481781\n",
      "Loss: 0.37083569169044495\n",
      "Loss: 0.535707950592041\n",
      "Loss: 0.5538980960845947\n",
      "Loss: 0.37350791692733765\n",
      "Loss: 0.3924731910228729\n",
      "Loss: 0.4175334572792053\n",
      "Loss: 0.4258732497692108\n",
      "Loss: 0.49486246705055237\n",
      "Loss: 0.42806750535964966\n",
      "Loss: 0.4762956202030182\n",
      "Loss: 0.3153529167175293\n",
      "Loss: 0.6200377345085144\n",
      "Loss: 0.3949280083179474\n",
      "Loss: 0.39970862865448\n",
      "Loss: 0.6119500398635864\n",
      "Loss: 0.4026046395301819\n",
      "Loss: 0.3794082701206207\n",
      "Loss: 0.33777081966400146\n",
      "Loss: 0.2463855892419815\n",
      "Loss: 0.37831342220306396\n",
      "Loss: 0.33623936772346497\n",
      "Loss: 0.3758199214935303\n",
      "Loss: 0.21494974195957184\n",
      "Loss: 0.38137543201446533\n",
      "Loss: 0.49433887004852295\n",
      "Loss: 0.19977502524852753\n",
      "Loss: 0.4523589313030243\n",
      "Loss: 0.7497135400772095\n",
      "Loss: 0.33381253480911255\n",
      "Loss: 0.46793776750564575\n",
      "Loss: 0.7042276263237\n",
      "Loss: 0.6285188794136047\n",
      "Loss: 0.5140043497085571\n",
      "Loss: 0.31333595514297485\n",
      "Loss: 0.41862913966178894\n",
      "Loss: 0.4887511730194092\n",
      "Loss: 0.3183273375034332\n",
      "Loss: 0.4398829936981201\n",
      "Loss: 0.24264872074127197\n",
      "Loss: 0.4608848989009857\n",
      "Loss: 0.546837568283081\n",
      "Loss: 0.2324102818965912\n",
      "Loss: 0.6244394183158875\n",
      "Loss: 0.430067777633667\n",
      "Loss: 0.15081995725631714\n",
      "Loss: 0.5070884823799133\n",
      "Loss: 0.5562288761138916\n",
      "Loss: 0.5153207778930664\n",
      "Loss: 0.6819117665290833\n",
      "Loss: 0.4384757876396179\n",
      "Loss: 0.48762571811676025\n",
      "Loss: 0.38269713521003723\n",
      "Loss: 0.4348209500312805\n",
      "Loss: 0.44486305117607117\n",
      "Loss: 0.44272568821907043\n",
      "Loss: 0.4825270473957062\n",
      "Loss: 0.380547434091568\n",
      "Loss: 0.7297711372375488\n",
      "Loss: 0.31233009696006775\n",
      "Loss: 0.4143061339855194\n",
      "Loss: 0.5362734794616699\n",
      "Loss: 0.3037816882133484\n",
      "Loss: 0.6022073030471802\n",
      "Loss: 0.2215551882982254\n",
      "Loss: 0.2418030947446823\n",
      "Loss: 0.3051425516605377\n",
      "Loss: 0.606187641620636\n",
      "Loss: 0.20691746473312378\n",
      "Loss: 0.6240139603614807\n",
      "Loss: 0.3093472421169281\n",
      "Loss: 0.45671749114990234\n",
      "Loss: 0.34034308791160583\n",
      "Loss: 0.4970591366291046\n",
      "Loss: 0.6857367157936096\n",
      "Loss: 0.4369327425956726\n",
      "Loss: 0.5443594455718994\n",
      "Loss: 0.6654200553894043\n",
      "Loss: 0.43559154868125916\n",
      "Loss: 0.64671790599823\n",
      "Loss: 0.2032995969057083\n",
      "Loss: 0.48272934556007385\n",
      "Loss: 0.3447946310043335\n",
      "Loss: 0.4146990180015564\n",
      "Loss: 0.6677850484848022\n",
      "Loss: 0.3928903639316559\n",
      "Loss: 0.3775356709957123\n",
      "Loss: 0.49986717104911804\n",
      "Loss: 0.35060974955558777\n",
      "Loss: 0.44222745299339294\n",
      "Loss: 0.5018348097801208\n",
      "Loss: 0.5821858048439026\n",
      "Loss: 0.7040693163871765\n",
      "Loss: 0.2975279688835144\n",
      "Loss: 0.338459849357605\n",
      "Loss: 0.3930247128009796\n",
      "Loss: 0.6220541596412659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.32520249485969543\n",
      "Loss: 0.3866533041000366\n",
      "Loss: 0.30278462171554565\n",
      "Loss: 0.4906283915042877\n",
      "Loss: 0.3926841616630554\n",
      "Loss: 0.4692765474319458\n",
      "Loss: 0.3087824285030365\n",
      "Loss: 0.414774090051651\n",
      "Loss: 0.4170227348804474\n",
      "Loss: 0.3196147382259369\n",
      "Loss: 0.2323639690876007\n",
      "Loss: 0.3461788594722748\n",
      "Loss: 0.3920917809009552\n",
      "Loss: 0.4192280173301697\n",
      "Loss: 0.30518895387649536\n",
      "Loss: 0.4699631631374359\n",
      "Loss: 0.5305116176605225\n",
      "Loss: 0.2777816653251648\n",
      "Loss: 0.3318922519683838\n",
      "Loss: 0.31981778144836426\n",
      "Loss: 0.5919685959815979\n",
      "Loss: 0.4439484775066376\n",
      "Loss: 0.36407792568206787\n",
      "Loss: 0.5106537342071533\n",
      "Loss: 0.7767918109893799\n",
      "Loss: 0.6996302008628845\n",
      "Loss: 0.28753283619880676\n",
      "Loss: 0.3969210088253021\n",
      "Loss: 0.3770645260810852\n",
      "Loss: 0.4471604824066162\n",
      "Loss: 0.36707016825675964\n",
      "Loss: 0.6167523860931396\n",
      "Loss: 0.31431716680526733\n",
      "Loss: 0.3210548460483551\n",
      "Loss: 0.47650691866874695\n",
      "Loss: 0.30715706944465637\n",
      "Loss: 0.2842501401901245\n",
      "Loss: 0.6917865872383118\n",
      "Loss: 0.43185776472091675\n",
      "Loss: 0.41959092020988464\n",
      "Loss: 0.7862992882728577\n",
      "Loss: 0.2574644982814789\n",
      "Loss: 0.31374916434288025\n",
      "Loss: 0.42679259181022644\n",
      "Loss: 0.3746863603591919\n",
      "Loss: 0.3374975621700287\n",
      "Loss: 0.24399562180042267\n",
      "Loss: 0.28372058272361755\n",
      "Loss: 0.5046999454498291\n",
      "Loss: 0.24675248563289642\n",
      "Loss: 0.4864574670791626\n",
      "Loss: 0.724484384059906\n",
      "Loss: 0.2694690525531769\n",
      "Loss: 0.5912125110626221\n",
      "Loss: 0.5213215351104736\n",
      "Loss: 0.3843790292739868\n",
      "Loss: 0.3964659571647644\n",
      "Loss: 0.7011682391166687\n",
      "Loss: 0.20581042766571045\n",
      "Loss: 0.4170544147491455\n",
      "Loss: 0.41034671664237976\n",
      "Loss: 0.30138134956359863\n",
      "Loss: 0.48354971408843994\n",
      "Loss: 0.2768014073371887\n",
      "Loss: 0.5129207372665405\n",
      "Loss: 0.4243081212043762\n",
      "Loss: 0.5057191848754883\n",
      "Loss: 0.556321918964386\n",
      "Loss: 0.440443754196167\n",
      "Loss: 0.539770781993866\n",
      "Loss: 0.24867860972881317\n",
      "Loss: 0.39045965671539307\n",
      "Loss: 0.4011198878288269\n",
      "Loss: 0.3882284462451935\n",
      "Loss: 0.43614327907562256\n",
      "Loss: 0.44589605927467346\n",
      "Loss: 0.3342241644859314\n",
      "Loss: 0.48364657163619995\n",
      "Loss: 0.42185521125793457\n",
      "Loss: 0.4843907952308655\n",
      "Loss: 0.49592822790145874\n",
      "Loss: 0.47619688510894775\n",
      "Loss: 0.5064958333969116\n",
      "Loss: 0.2891749441623688\n",
      "Loss: 0.3674312233924866\n",
      "Loss: 0.2592819631099701\n",
      "Loss: 0.3402899205684662\n",
      "Loss: 0.7622454762458801\n",
      "Loss: 0.6461467742919922\n",
      "Loss: 0.49623697996139526\n",
      "Loss: 0.2517760992050171\n",
      "Loss: 0.38497352600097656\n",
      "Loss: 0.2530459761619568\n",
      "Loss: 0.29458191990852356\n",
      "Loss: 0.29331663250923157\n",
      "Loss: 0.5348180532455444\n",
      "Loss: 0.3824390769004822\n",
      "Loss: 0.4967469871044159\n",
      "Loss: 0.32214218378067017\n",
      "Loss: 0.3266078531742096\n",
      "Loss: 0.503812849521637\n",
      "Loss: 0.38225382566452026\n",
      "Loss: 0.6047716736793518\n",
      "Loss: 0.4308169186115265\n",
      "Loss: 0.415706068277359\n",
      "Loss: 0.5472781658172607\n",
      "Loss: 0.5032640695571899\n",
      "Loss: 0.4846939742565155\n",
      "Loss: 0.3677064776420593\n",
      "Loss: 0.3268585205078125\n",
      "Loss: 0.6573453545570374\n",
      "Loss: 0.5304746031761169\n",
      "Loss: 0.39380142092704773\n",
      "Loss: 0.36142975091934204\n",
      "Loss: 0.57347571849823\n",
      "Loss: 0.546729564666748\n",
      "Loss: 0.41706913709640503\n",
      "Loss: 0.5116623640060425\n",
      "Loss: 0.4196792244911194\n",
      "Loss: 0.4815758764743805\n",
      "Loss: 0.6159100532531738\n",
      "Loss: 0.4243538975715637\n",
      "Loss: 0.26060745120048523\n",
      "Loss: 0.7704609632492065\n",
      "Loss: 0.37757545709609985\n",
      "Loss: 0.28704535961151123\n",
      "Loss: 0.5417618751525879\n",
      "Loss: 0.3616763651371002\n",
      "Loss: 0.4212651550769806\n",
      "Loss: 0.627631664276123\n",
      "Loss: 0.266887366771698\n",
      "Loss: 0.37235018610954285\n",
      "Loss: 0.295294851064682\n",
      "Loss: 0.45845890045166016\n",
      "Loss: 0.33749908208847046\n",
      "Loss: 0.26335644721984863\n",
      "Loss: 0.2481408566236496\n",
      "Loss: 0.43364661931991577\n",
      "Loss: 0.38936737179756165\n",
      "Loss: 0.4322163760662079\n",
      "Loss: 0.259967178106308\n",
      "Loss: 0.4071466326713562\n",
      "Loss: 0.3559643626213074\n",
      "Loss: 0.3686303198337555\n",
      "Loss: 0.4077285826206207\n",
      "Loss: 0.29180946946144104\n",
      "Loss: 0.2537910044193268\n",
      "Loss: 0.4581010341644287\n",
      "Loss: 0.30192339420318604\n",
      "Loss: 0.45850110054016113\n",
      "Loss: 0.39801326394081116\n",
      "Loss: 0.3217315971851349\n",
      "Loss: 0.3097909986972809\n",
      "Loss: 0.7203560471534729\n",
      "Loss: 0.7658122777938843\n",
      "Loss: 0.45839300751686096\n",
      "Loss: 0.2965174615383148\n",
      "Loss: 0.5001672506332397\n",
      "Loss: 0.4465161859989166\n",
      "Loss: 0.3288698196411133\n",
      "Loss: 0.6789761185646057\n",
      "Loss: 0.5081931352615356\n",
      "Loss: 0.415121853351593\n",
      "Loss: 0.41006410121917725\n",
      "Loss: 0.3351350426673889\n",
      "Loss: 0.21081610023975372\n",
      "Loss: 0.40465372800827026\n",
      "Loss: 0.34390124678611755\n",
      "Loss: 0.3022383153438568\n",
      "Loss: 0.24585071206092834\n",
      "Loss: 0.5327871441841125\n",
      "Loss: 0.36872851848602295\n",
      "Loss: 0.32209575176239014\n",
      "Loss: 0.454700231552124\n",
      "Loss: 0.3052055239677429\n",
      "Loss: 0.4505535364151001\n",
      "Loss: 0.4976973831653595\n",
      "Loss: 0.250316858291626\n",
      "Loss: 0.4984755218029022\n",
      "Loss: 0.33241090178489685\n",
      "Loss: 0.6039620637893677\n",
      "Loss: 0.5532684922218323\n",
      "Loss: 0.5159661769866943\n",
      "Loss: 0.3249421715736389\n",
      "Loss: 0.5002386569976807\n",
      "Loss: 0.5969955921173096\n",
      "Loss: 0.3984774649143219\n",
      "Loss: 0.7546885013580322\n",
      "Loss: 0.6040011644363403\n",
      "Loss: 0.37505480647087097\n",
      "Loss: 0.3684942126274109\n",
      "Loss: 0.7511503100395203\n",
      "Loss: 0.3806025981903076\n",
      "Loss: 0.3771302103996277\n",
      "Loss: 0.37015682458877563\n",
      "Loss: 0.3796675503253937\n",
      "Loss: 0.42241206765174866\n",
      "Loss: 0.4468529522418976\n",
      "Loss: 0.30867111682891846\n",
      "Loss: 0.6517702341079712\n",
      "Loss: 0.4761461019515991\n",
      "Loss: 0.5462249517440796\n",
      "Loss: 0.23081396520137787\n",
      "Loss: 0.4789567291736603\n",
      "Loss: 0.5846704840660095\n",
      "Loss: 0.5323551893234253\n",
      "Loss: 0.4275154769420624\n",
      "Loss: 0.4140024185180664\n",
      "Loss: 0.48294028639793396\n",
      "Loss: 0.5403227210044861\n",
      "Loss: 0.43540728092193604\n",
      "Loss: 0.4168626368045807\n",
      "Loss: 0.5571714639663696\n",
      "Loss: 0.3283769488334656\n",
      "Loss: 0.260883092880249\n",
      "Loss: 0.4502139389514923\n",
      "Loss: 0.5097212791442871\n",
      "Loss: 0.5584924817085266\n",
      "Loss: 0.5825048685073853\n",
      "Loss: 0.5491763949394226\n",
      "Loss: 0.3614477515220642\n",
      "Loss: 0.37591874599456787\n",
      "Loss: 0.3728947341442108\n",
      "Loss: 0.2901821434497833\n",
      "Loss: 0.36964473128318787\n",
      "Loss: 0.24938276410102844\n",
      "Loss: 0.5224370360374451\n",
      "Loss: 0.28793564438819885\n",
      "Loss: 0.5008692741394043\n",
      "Loss: 0.37671521306037903\n",
      "Loss: 0.7442277669906616\n",
      "Loss: 0.26543715596199036\n",
      "Loss: 0.6168496608734131\n",
      "Loss: 0.32180720567703247\n",
      "Loss: 0.41449737548828125\n",
      "Loss: 0.36688366532325745\n",
      "Loss: 0.3833615481853485\n",
      "Loss: 0.2207309603691101\n",
      "Loss: 0.5179222226142883\n",
      "Loss: 0.24444246292114258\n",
      "Loss: 0.5152536630630493\n",
      "Loss: 0.3307154178619385\n",
      "Loss: 0.5504682064056396\n",
      "Loss: 0.4961526095867157\n",
      "Loss: 0.34531140327453613\n",
      "Loss: 0.4703405499458313\n",
      "Loss: 0.31161436438560486\n",
      "Loss: 0.7204018235206604\n",
      "Loss: 0.20388290286064148\n",
      "Loss: 0.4383942782878876\n",
      "Loss: 0.7095072865486145\n",
      "Loss: 0.6148251891136169\n",
      "Loss: 0.3128572106361389\n",
      "Loss: 0.24738091230392456\n",
      "Loss: 0.3197855055332184\n",
      "Loss: 0.4381237328052521\n",
      "Loss: 0.49667853116989136\n",
      "Loss: 0.5070154666900635\n",
      "Loss: 0.4550859332084656\n",
      "Loss: 0.6361355185508728\n",
      "Loss: 0.5500924587249756\n",
      "Loss: 0.34525448083877563\n",
      "Loss: 0.5448025465011597\n",
      "Loss: 0.4859658181667328\n",
      "Loss: 0.2999955415725708\n"
     ]
    }
   ],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    w = torch.randn(size=shape)*0.01\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "#(b)\n",
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "#end (b)\n",
    "    \n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h_ = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2_ = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax.transpose(0,1)\n",
    "\n",
    "\n",
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    noise_py_x = noise_py_x.transpose(0,1)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.301126480102539\n",
      "Loss: 2.4601705074310303\n",
      "Loss: 2.3724472522735596\n",
      "Loss: 2.301452159881592\n",
      "Loss: 2.2571959495544434\n",
      "Loss: 2.173217535018921\n",
      "Loss: 2.109001874923706\n",
      "Loss: 1.9273102283477783\n",
      "Loss: 1.8774402141571045\n",
      "Loss: 1.8462146520614624\n",
      "Loss: 1.8301942348480225\n",
      "Loss: 1.6840736865997314\n",
      "Loss: 1.693688988685608\n",
      "Loss: 1.7736531496047974\n",
      "Loss: 1.5892038345336914\n",
      "Loss: 1.3629652261734009\n",
      "Loss: 1.4609931707382202\n",
      "Loss: 1.217271089553833\n",
      "Loss: 1.3732244968414307\n",
      "Loss: 1.1268824338912964\n",
      "Loss: 1.1890673637390137\n",
      "Loss: 1.2752197980880737\n",
      "Loss: 0.9718953967094421\n",
      "Loss: 0.9312899112701416\n",
      "Loss: 0.9585617184638977\n",
      "Loss: 0.7769391536712646\n",
      "Loss: 1.0969319343566895\n",
      "Loss: 0.852713406085968\n",
      "Loss: 0.9407721161842346\n",
      "Loss: 0.9949538707733154\n",
      "Loss: 0.9168722033500671\n",
      "Loss: 1.03035569190979\n",
      "Loss: 0.7389092445373535\n",
      "Loss: 0.9261389970779419\n",
      "Loss: 0.8313544988632202\n",
      "Loss: 0.9051421284675598\n",
      "Loss: 0.6579617261886597\n",
      "Loss: 0.9165657162666321\n",
      "Loss: 0.7095609903335571\n",
      "Loss: 0.9638584852218628\n",
      "Loss: 0.662153959274292\n",
      "Loss: 0.7666606903076172\n",
      "Loss: 0.8319435119628906\n",
      "Loss: 0.6419087052345276\n",
      "Loss: 0.7218811511993408\n",
      "Loss: 0.6446179747581482\n",
      "Loss: 0.6746569275856018\n",
      "Loss: 0.6830608248710632\n",
      "Loss: 0.578683614730835\n",
      "Loss: 0.7237215638160706\n",
      "Loss: 0.5149756669998169\n",
      "Loss: 0.5464887022972107\n",
      "Loss: 0.9094026684761047\n",
      "Loss: 0.6493720412254333\n",
      "Loss: 0.46263423562049866\n",
      "Loss: 0.44294512271881104\n",
      "Loss: 0.48889094591140747\n",
      "Loss: 0.5863811373710632\n",
      "Loss: 0.5343112945556641\n",
      "Loss: 0.7348211407661438\n",
      "Loss: 0.45073840022087097\n",
      "Loss: 0.6968032121658325\n",
      "Loss: 0.4792087972164154\n",
      "Loss: 0.380217581987381\n",
      "Loss: 0.7001582384109497\n",
      "Loss: 0.6546564698219299\n",
      "Loss: 0.44725292921066284\n",
      "Loss: 0.44802582263946533\n",
      "Loss: 0.678769052028656\n",
      "Loss: 0.6324948668479919\n",
      "Loss: 0.43511807918548584\n",
      "Loss: 0.634759783744812\n",
      "Loss: 0.7303605079650879\n",
      "Loss: 0.5247282981872559\n",
      "Loss: 0.6707859635353088\n",
      "Loss: 0.7077295184135437\n",
      "Loss: 0.8058860301971436\n",
      "Loss: 0.6070049405097961\n",
      "Loss: 0.6155197620391846\n",
      "Loss: 0.7023103833198547\n",
      "Loss: 0.4543413519859314\n",
      "Loss: 0.7568843364715576\n",
      "Loss: 0.5659793615341187\n",
      "Loss: 0.38646915555000305\n",
      "Loss: 0.673291027545929\n",
      "Loss: 0.5511245131492615\n",
      "Loss: 0.5753178000450134\n",
      "Loss: 0.49091002345085144\n",
      "Loss: 0.5992605686187744\n",
      "Loss: 0.30847233533859253\n",
      "Loss: 0.6003196835517883\n",
      "Loss: 0.5888442397117615\n",
      "Loss: 0.5811368823051453\n",
      "Loss: 0.6169044971466064\n",
      "Loss: 0.5836926698684692\n",
      "Loss: 0.2866498827934265\n",
      "Loss: 0.2994595468044281\n",
      "Loss: 0.49903053045272827\n",
      "Loss: 0.5831655263900757\n",
      "Loss: 0.5161235332489014\n",
      "Loss: 0.37488868832588196\n",
      "Loss: 0.365141898393631\n",
      "Loss: 0.7510442137718201\n",
      "Loss: 0.49936047196388245\n",
      "Loss: 0.5703888535499573\n",
      "Loss: 0.6260378360748291\n",
      "Loss: 0.6789044141769409\n",
      "Loss: 0.42633968591690063\n",
      "Loss: 0.3223792016506195\n",
      "Loss: 0.3329213857650757\n",
      "Loss: 0.39563655853271484\n",
      "Loss: 0.4062998294830322\n",
      "Loss: 0.49282410740852356\n",
      "Loss: 0.6070568561553955\n",
      "Loss: 0.23391838371753693\n",
      "Loss: 0.4107849597930908\n",
      "Loss: 0.5043958425521851\n",
      "Loss: 0.40748366713523865\n",
      "Loss: 0.36011815071105957\n",
      "Loss: 0.5044244527816772\n",
      "Loss: 0.5171153545379639\n",
      "Loss: 0.448684960603714\n",
      "Loss: 0.3607840836048126\n",
      "Loss: 0.27332133054733276\n",
      "Loss: 0.4386526942253113\n",
      "Loss: 0.32986652851104736\n",
      "Loss: 0.44219112396240234\n",
      "Loss: 0.3967598080635071\n",
      "Loss: 0.47755512595176697\n",
      "Loss: 0.4290638864040375\n",
      "Loss: 0.7401667833328247\n",
      "Loss: 0.5882107019424438\n",
      "Loss: 0.5085282325744629\n",
      "Loss: 0.4987829923629761\n",
      "Loss: 0.679188072681427\n",
      "Loss: 0.40989071130752563\n",
      "Loss: 0.4614936411380768\n",
      "Loss: 0.43505945801734924\n",
      "Loss: 0.7416121959686279\n",
      "Loss: 0.5543237328529358\n",
      "Loss: 0.4096966087818146\n",
      "Loss: 0.6163362264633179\n",
      "Loss: 0.661620020866394\n",
      "Loss: 0.4311319589614868\n",
      "Loss: 0.8712435960769653\n",
      "Loss: 0.5874699354171753\n",
      "Loss: 0.5345125794410706\n",
      "Loss: 0.4875178039073944\n",
      "Loss: 0.3911241292953491\n",
      "Loss: 0.498864084482193\n",
      "Loss: 0.5713288187980652\n",
      "Loss: 0.5881987810134888\n",
      "Loss: 0.37930914759635925\n",
      "Loss: 0.32242435216903687\n",
      "Loss: 0.538797914981842\n",
      "Loss: 0.5014996528625488\n",
      "Loss: 0.4033272862434387\n",
      "Loss: 0.3050323724746704\n",
      "Loss: 0.36640313267707825\n",
      "Loss: 0.45076221227645874\n",
      "Loss: 0.27023133635520935\n",
      "Loss: 0.46226632595062256\n",
      "Loss: 0.5706663131713867\n",
      "Loss: 0.39922410249710083\n",
      "Loss: 0.3010050356388092\n",
      "Loss: 0.3632950484752655\n",
      "Loss: 0.4107532799243927\n",
      "Loss: 0.4305960536003113\n",
      "Loss: 0.32226353883743286\n",
      "Loss: 0.49962911009788513\n",
      "Loss: 0.30147895216941833\n",
      "Loss: 0.32403334975242615\n",
      "Loss: 0.43563976883888245\n",
      "Loss: 0.5202744007110596\n",
      "Loss: 0.33759403228759766\n",
      "Loss: 0.556427538394928\n",
      "Loss: 0.2632487416267395\n",
      "Loss: 0.395915687084198\n",
      "Loss: 0.3385990560054779\n",
      "Loss: 0.2460472136735916\n",
      "Loss: 0.37019816040992737\n",
      "Loss: 0.40342608094215393\n",
      "Loss: 0.49517425894737244\n",
      "Loss: 0.44499653577804565\n",
      "Loss: 0.41282540559768677\n",
      "Loss: 0.4442340135574341\n",
      "Loss: 0.5737335681915283\n",
      "Loss: 0.41280192136764526\n",
      "Loss: 0.32825854420661926\n",
      "Loss: 0.30304595828056335\n",
      "Loss: 0.35700172185897827\n",
      "Loss: 0.6049430966377258\n",
      "Loss: 0.4062860608100891\n",
      "Loss: 0.44970059394836426\n",
      "Loss: 0.26894792914390564\n",
      "Loss: 0.3962348699569702\n",
      "Loss: 0.3937757909297943\n",
      "Loss: 0.36745429039001465\n",
      "Loss: 0.32564330101013184\n",
      "Loss: 0.3132520616054535\n",
      "Loss: 0.5080304145812988\n",
      "Loss: 0.29170340299606323\n",
      "Loss: 0.5122665166854858\n",
      "Loss: 0.3770202696323395\n",
      "Loss: 0.6208174228668213\n",
      "Loss: 0.4653612971305847\n",
      "Loss: 0.42392978072166443\n",
      "Loss: 0.32387691736221313\n",
      "Loss: 0.4427509605884552\n",
      "Loss: 0.41855937242507935\n",
      "Loss: 0.5448260307312012\n",
      "Loss: 0.48778656125068665\n",
      "Loss: 0.33689072728157043\n",
      "Loss: 0.29333558678627014\n",
      "Loss: 0.36160004138946533\n",
      "Loss: 0.3666565716266632\n",
      "Loss: 0.3855780363082886\n",
      "Loss: 0.3202892243862152\n",
      "Loss: 0.38904327154159546\n",
      "Loss: 0.5763677954673767\n",
      "Loss: 0.3048188388347626\n",
      "Loss: 0.34503933787345886\n",
      "Loss: 0.3445274233818054\n",
      "Loss: 0.5838032960891724\n",
      "Loss: 0.48694008588790894\n",
      "Loss: 0.45435240864753723\n",
      "Loss: 0.29751652479171753\n",
      "Loss: 0.3416966199874878\n",
      "Loss: 0.32245272397994995\n",
      "Loss: 0.6319965124130249\n",
      "Loss: 0.4909084737300873\n",
      "Loss: 0.41733643412590027\n",
      "Loss: 0.4643392264842987\n",
      "Loss: 0.3514622747898102\n",
      "Loss: 0.2577364146709442\n",
      "Loss: 0.4497531056404114\n",
      "Loss: 0.3791293203830719\n",
      "Loss: 0.24584484100341797\n",
      "Loss: 0.3421785235404968\n",
      "Loss: 0.34491822123527527\n",
      "Loss: 0.33831751346588135\n",
      "Loss: 0.40409138798713684\n",
      "Loss: 0.45108354091644287\n",
      "Loss: 0.28513574600219727\n",
      "Loss: 0.2778867483139038\n",
      "Loss: 0.49510493874549866\n",
      "Loss: 0.6806665658950806\n",
      "Loss: 0.3236981928348541\n",
      "Loss: 0.39434245228767395\n",
      "Loss: 0.41969218850135803\n",
      "Loss: 0.7552003264427185\n",
      "Loss: 0.4086151421070099\n",
      "Loss: 0.4777745306491852\n",
      "Loss: 0.2244071364402771\n",
      "Loss: 0.2447800487279892\n",
      "Loss: 0.42211735248565674\n",
      "Loss: 0.3775883615016937\n",
      "Loss: 0.5477320551872253\n",
      "Loss: 0.3979766368865967\n",
      "Loss: 0.33186817169189453\n",
      "Loss: 0.42547324299812317\n",
      "Loss: 0.3756190240383148\n",
      "Loss: 0.6367721557617188\n",
      "Loss: 0.5239499807357788\n",
      "Loss: 0.39546048641204834\n",
      "Loss: 0.601010799407959\n",
      "Loss: 0.32150906324386597\n",
      "Loss: 0.36826974153518677\n",
      "Loss: 0.2645096182823181\n",
      "Loss: 0.3749173879623413\n",
      "Loss: 0.4979777932167053\n",
      "Loss: 0.4009847640991211\n",
      "Loss: 0.5106614828109741\n",
      "Loss: 0.3335796594619751\n",
      "Loss: 0.2577025592327118\n",
      "Loss: 0.3111954629421234\n",
      "Loss: 0.4560176432132721\n",
      "Loss: 0.16261790692806244\n",
      "Loss: 0.4367225170135498\n",
      "Loss: 0.27983349561691284\n",
      "Loss: 0.22398796677589417\n",
      "Loss: 0.8113335967063904\n",
      "Loss: 0.3929344415664673\n",
      "Loss: 0.3125118911266327\n",
      "Loss: 0.44518372416496277\n",
      "Loss: 0.38161972165107727\n",
      "Loss: 0.2897089421749115\n",
      "Loss: 0.4426800012588501\n",
      "Loss: 0.36538052558898926\n",
      "Loss: 0.19002605974674225\n",
      "Loss: 0.38674044609069824\n",
      "Loss: 0.4206289052963257\n",
      "Loss: 0.33681929111480713\n",
      "Loss: 0.36090087890625\n",
      "Loss: 0.5881292819976807\n",
      "Loss: 0.2972085475921631\n",
      "Loss: 0.3186677396297455\n",
      "Loss: 0.30575039982795715\n",
      "Loss: 0.2869205176830292\n",
      "Loss: 0.336645245552063\n",
      "Loss: 0.5783373713493347\n",
      "Loss: 0.40258270502090454\n",
      "Loss: 0.5712181329727173\n",
      "Loss: 0.5250813961029053\n",
      "Loss: 0.8492684364318848\n",
      "Loss: 0.5307667255401611\n",
      "Loss: 0.22211231291294098\n",
      "Loss: 0.3992779552936554\n",
      "Loss: 0.5603008270263672\n",
      "Loss: 0.2514041066169739\n",
      "Loss: 0.40660929679870605\n",
      "Loss: 0.4683535397052765\n",
      "Loss: 0.35603317618370056\n",
      "Loss: 0.6703383922576904\n",
      "Loss: 0.598487913608551\n",
      "Loss: 0.5742263793945312\n",
      "Loss: 0.44087040424346924\n",
      "Loss: 0.539097249507904\n",
      "Loss: 0.4572320282459259\n",
      "Loss: 0.25888848304748535\n",
      "Loss: 0.2874743342399597\n",
      "Loss: 0.5151559114456177\n",
      "Loss: 0.18979264795780182\n",
      "Loss: 0.2344147264957428\n",
      "Loss: 0.4318329989910126\n",
      "Loss: 0.3279193043708801\n",
      "Loss: 0.29901444911956787\n",
      "Loss: 0.5226016044616699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2005351483821869\n",
      "Loss: 0.32315367460250854\n",
      "Loss: 0.32866963744163513\n",
      "Loss: 0.3176599442958832\n",
      "Loss: 0.3605559468269348\n",
      "Loss: 0.30380088090896606\n",
      "Loss: 0.3019169867038727\n",
      "Loss: 0.3096800744533539\n",
      "Loss: 0.46932849287986755\n",
      "Loss: 0.3770848512649536\n",
      "Loss: 0.3909054696559906\n",
      "Loss: 0.33027252554893494\n",
      "Loss: 0.2613484859466553\n",
      "Loss: 0.4297531545162201\n",
      "Loss: 0.33085083961486816\n",
      "Loss: 0.3430494964122772\n",
      "Loss: 0.38354820013046265\n",
      "Loss: 0.4369887411594391\n",
      "Loss: 0.38750502467155457\n",
      "Loss: 0.622992753982544\n",
      "Loss: 0.3745587468147278\n",
      "Loss: 0.4353584945201874\n",
      "Loss: 0.29142215847969055\n",
      "Loss: 0.31639865040779114\n",
      "Loss: 0.4172540009021759\n",
      "Loss: 0.4980846047401428\n",
      "Loss: 0.18850763142108917\n",
      "Loss: 0.52095627784729\n",
      "Loss: 0.3495292365550995\n",
      "Loss: 0.5436902642250061\n",
      "Loss: 0.4675484597682953\n",
      "Loss: 0.15053628385066986\n",
      "Loss: 0.2518494725227356\n",
      "Loss: 0.3862777352333069\n",
      "Loss: 0.25665906071662903\n",
      "Loss: 0.467543363571167\n",
      "Loss: 0.37192878127098083\n",
      "Loss: 0.5429825782775879\n",
      "Loss: 0.5663868188858032\n",
      "Loss: 0.38843628764152527\n",
      "Loss: 0.4980009198188782\n",
      "Loss: 0.36211276054382324\n",
      "Loss: 0.4556719958782196\n",
      "Loss: 0.2713097631931305\n",
      "Loss: 0.42358630895614624\n",
      "Loss: 0.3187769949436188\n",
      "Loss: 0.38463878631591797\n",
      "Loss: 0.4545457363128662\n",
      "Loss: 0.33660751581192017\n",
      "Loss: 0.39101332426071167\n",
      "Loss: 0.4144744575023651\n",
      "Loss: 0.7417004108428955\n",
      "Loss: 0.5975088477134705\n",
      "Loss: 0.5802983045578003\n",
      "Loss: 0.2240198701620102\n",
      "Loss: 0.6290751099586487\n",
      "Loss: 0.2891260087490082\n",
      "Loss: 0.30516737699508667\n",
      "Loss: 0.4208706319332123\n",
      "Loss: 0.5929739475250244\n",
      "Loss: 0.44033440947532654\n",
      "Loss: 0.5239188075065613\n",
      "Loss: 0.36883285641670227\n",
      "Loss: 0.3741147518157959\n",
      "Loss: 0.3462315797805786\n",
      "Loss: 0.592366635799408\n",
      "Loss: 0.3130112588405609\n",
      "Loss: 0.38205382227897644\n",
      "Loss: 0.2727598547935486\n",
      "Loss: 0.33948394656181335\n",
      "Loss: 0.23304328322410583\n",
      "Loss: 0.47731032967567444\n",
      "Loss: 0.4650145471096039\n",
      "Loss: 0.3663373589515686\n",
      "Loss: 0.3172144889831543\n",
      "Loss: 0.7305351495742798\n",
      "Loss: 0.4123120903968811\n",
      "Loss: 0.40513500571250916\n",
      "Loss: 0.36512625217437744\n",
      "Loss: 0.26352933049201965\n",
      "Loss: 0.27459365129470825\n",
      "Loss: 0.5230302214622498\n",
      "Loss: 0.37845379114151\n",
      "Loss: 0.29029932618141174\n",
      "Loss: 0.2378348857164383\n",
      "Loss: 0.46650421619415283\n",
      "Loss: 0.40671929717063904\n",
      "Loss: 0.48291081190109253\n",
      "Loss: 0.6939844489097595\n",
      "Loss: 0.6294298768043518\n",
      "Loss: 0.5482478737831116\n",
      "Loss: 0.5082167387008667\n",
      "Loss: 0.46961134672164917\n",
      "Loss: 0.3896120488643646\n",
      "Loss: 0.5265481472015381\n",
      "Loss: 0.215321883559227\n",
      "Loss: 0.3776359558105469\n",
      "Loss: 0.4418434798717499\n",
      "Loss: 0.625440776348114\n",
      "Loss: 0.49663352966308594\n",
      "Loss: 0.31334441900253296\n",
      "Loss: 0.6064430475234985\n",
      "Loss: 0.30261731147766113\n",
      "Loss: 0.27402693033218384\n",
      "Loss: 0.4048449397087097\n",
      "Loss: 0.2596001923084259\n",
      "Loss: 0.46000394225120544\n",
      "Loss: 0.43322357535362244\n",
      "Loss: 0.4215050935745239\n",
      "Loss: 0.36189818382263184\n",
      "Loss: 0.3930676281452179\n",
      "Loss: 0.1812742054462433\n",
      "Loss: 0.27891695499420166\n",
      "Loss: 0.3650369644165039\n",
      "Loss: 0.435102641582489\n",
      "Loss: 0.5050791501998901\n",
      "Loss: 0.35894232988357544\n",
      "Loss: 0.46389415860176086\n",
      "Loss: 0.36829379200935364\n",
      "Loss: 0.647361159324646\n",
      "Loss: 0.4058275520801544\n",
      "Loss: 0.28537610173225403\n",
      "Loss: 0.37932831048965454\n",
      "Loss: 0.5396919846534729\n",
      "Loss: 0.22576148808002472\n",
      "Loss: 0.5462314486503601\n",
      "Loss: 0.20474562048912048\n",
      "Loss: 0.463220477104187\n",
      "Loss: 0.26801905035972595\n",
      "Loss: 0.2570849657058716\n",
      "Loss: 0.3986673057079315\n",
      "Loss: 0.4102998375892639\n",
      "Loss: 0.3995497524738312\n",
      "Loss: 0.43784472346305847\n",
      "Loss: 0.34027573466300964\n",
      "Loss: 0.45038241147994995\n",
      "Loss: 0.1734161525964737\n",
      "Loss: 0.5384104251861572\n",
      "Loss: 0.31023356318473816\n",
      "Loss: 0.2248702496290207\n",
      "Loss: 0.291406512260437\n",
      "Loss: 0.7352352142333984\n",
      "Loss: 0.2245844453573227\n",
      "Loss: 0.35859766602516174\n",
      "Loss: 0.5522767901420593\n",
      "Loss: 0.3322615921497345\n",
      "Loss: 0.3740810751914978\n",
      "Loss: 0.44867390394210815\n",
      "Loss: 0.500927209854126\n",
      "Loss: 0.3543439507484436\n",
      "Loss: 0.3363831639289856\n",
      "Loss: 0.5078815817832947\n",
      "Loss: 0.43899399042129517\n",
      "Loss: 0.4988664388656616\n",
      "Loss: 0.38657596707344055\n",
      "Loss: 0.4080066680908203\n",
      "Loss: 0.610742449760437\n",
      "Loss: 0.3685266971588135\n",
      "Loss: 0.38036736845970154\n",
      "Loss: 0.2422284334897995\n",
      "Loss: 0.23500287532806396\n",
      "Loss: 0.30165988206863403\n",
      "Loss: 0.438655823469162\n",
      "Loss: 0.46190476417541504\n",
      "Loss: 0.44978615641593933\n",
      "Loss: 0.3698502480983734\n",
      "Loss: 0.3563893139362335\n",
      "Loss: 0.38379788398742676\n",
      "Loss: 0.4200773239135742\n",
      "Loss: 0.32816874980926514\n",
      "Loss: 0.35274967551231384\n",
      "Loss: 0.4673599898815155\n",
      "Loss: 0.512395441532135\n",
      "Loss: 0.4436408281326294\n",
      "Loss: 0.3671954870223999\n",
      "Loss: 0.40500178933143616\n",
      "Loss: 0.3596903681755066\n",
      "Loss: 0.4288712441921234\n",
      "Loss: 0.4050036370754242\n",
      "Loss: 0.4714564085006714\n",
      "Loss: 0.4631582200527191\n",
      "Loss: 0.370229572057724\n",
      "Loss: 0.3814973831176758\n",
      "Loss: 0.44097182154655457\n",
      "Loss: 0.41848263144493103\n",
      "Loss: 0.4478209316730499\n",
      "Loss: 0.40440434217453003\n",
      "Loss: 0.6427056789398193\n",
      "Loss: 0.23565183579921722\n",
      "Loss: 0.37390822172164917\n",
      "Loss: 0.3801628053188324\n",
      "Loss: 0.1906878799200058\n",
      "Loss: 0.25100451707839966\n",
      "Loss: 0.3371999263763428\n",
      "Loss: 0.5593971610069275\n",
      "Loss: 0.3612922728061676\n",
      "Loss: 0.18822024762630463\n",
      "Loss: 0.2584329843521118\n",
      "Loss: 0.2988377511501312\n",
      "Loss: 0.6528394818305969\n",
      "Loss: 0.41223248839378357\n",
      "Loss: 0.5348448753356934\n",
      "Loss: 0.2579532861709595\n",
      "Loss: 0.33323830366134644\n",
      "Loss: 0.4111844301223755\n",
      "Loss: 0.35791027545928955\n",
      "Loss: 0.3089846074581146\n",
      "Loss: 0.4960981607437134\n",
      "Loss: 0.2717227041721344\n",
      "Loss: 0.45504477620124817\n",
      "Loss: 0.2649363875389099\n",
      "Loss: 0.4725216329097748\n",
      "Loss: 0.27541106939315796\n",
      "Loss: 0.4147548973560333\n",
      "Loss: 0.36598750948905945\n",
      "Loss: 0.39746201038360596\n",
      "Loss: 0.320870041847229\n",
      "Loss: 0.8489246964454651\n",
      "Loss: 0.34733471274375916\n",
      "Loss: 0.26726970076560974\n",
      "Loss: 0.4263439178466797\n",
      "Loss: 0.4615502655506134\n",
      "Loss: 0.3430577218532562\n",
      "Loss: 0.4594988226890564\n",
      "Loss: 0.3938933312892914\n",
      "Loss: 0.24601535499095917\n",
      "Loss: 0.3244827389717102\n",
      "Loss: 0.286163866519928\n",
      "Loss: 0.3030628263950348\n",
      "Loss: 0.4543066918849945\n",
      "Loss: 0.25023865699768066\n",
      "Loss: 0.2856341302394867\n",
      "Loss: 0.17578105628490448\n",
      "Loss: 0.34369096159935\n",
      "Loss: 0.23913520574569702\n",
      "Loss: 0.38055524230003357\n",
      "Loss: 0.460011750459671\n",
      "Loss: 0.4417622685432434\n",
      "Loss: 0.5033307075500488\n",
      "Loss: 0.34529635310173035\n",
      "Loss: 0.661171555519104\n",
      "Loss: 0.6527035236358643\n",
      "Loss: 0.6122052669525146\n",
      "Loss: 0.34365734457969666\n",
      "Loss: 0.4689024090766907\n",
      "Loss: 0.43885543942451477\n",
      "Loss: 0.5876890420913696\n",
      "Loss: 0.39535292983055115\n",
      "Loss: 0.4153851270675659\n",
      "Loss: 0.32666027545928955\n",
      "Loss: 0.3807373344898224\n",
      "Loss: 0.28421491384506226\n",
      "Loss: 0.3566662669181824\n",
      "Loss: 0.2984950542449951\n",
      "Loss: 0.2631889283657074\n",
      "Loss: 0.4001033902168274\n",
      "Loss: 0.42359206080436707\n",
      "Loss: 0.48119795322418213\n",
      "Loss: 0.16433677077293396\n",
      "Loss: 0.2570485472679138\n",
      "Loss: 0.20754793286323547\n",
      "Loss: 0.6457405090332031\n",
      "Loss: 0.31347155570983887\n",
      "Loss: 0.24477867782115936\n",
      "Loss: 0.4083840847015381\n",
      "Loss: 0.45359161496162415\n",
      "Loss: 0.4026018977165222\n",
      "Loss: 0.32098284363746643\n",
      "Loss: 0.3728693127632141\n",
      "Loss: 0.3356960713863373\n",
      "Loss: 0.3093758523464203\n",
      "Loss: 0.23790530860424042\n"
     ]
    }
   ],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    w = torch.randn(size=shape)*0.01\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h_ = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2_ = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax.transpose(0,1)\n",
    "\n",
    "\n",
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    noise_py_x = noise_py_x.transpose(0,1)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.303811550140381\n",
      "Loss: 2.4472110271453857\n",
      "Loss: 2.292245864868164\n",
      "Loss: 2.394937038421631\n",
      "Loss: 2.237196922302246\n",
      "Loss: 2.2075231075286865\n",
      "Loss: 2.1441283226013184\n",
      "Loss: 2.008239507675171\n",
      "Loss: 1.9664784669876099\n",
      "Loss: 1.8960392475128174\n",
      "Loss: 1.7017848491668701\n",
      "Loss: 1.8484954833984375\n",
      "Loss: 1.740090012550354\n",
      "Loss: 1.3819324970245361\n",
      "Loss: 1.4581434726715088\n",
      "Loss: 1.2676910161972046\n",
      "Loss: 1.4671528339385986\n",
      "Loss: 1.0352412462234497\n",
      "Loss: 1.3838337659835815\n",
      "Loss: 0.9910264611244202\n",
      "Loss: 1.2821317911148071\n",
      "Loss: 0.933592677116394\n",
      "Loss: 0.8181854486465454\n",
      "Loss: 1.0227619409561157\n",
      "Loss: 1.1390184164047241\n",
      "Loss: 0.8551374077796936\n",
      "Loss: 0.858630359172821\n",
      "Loss: 0.8524398803710938\n",
      "Loss: 0.8306090831756592\n",
      "Loss: 0.764746904373169\n",
      "Loss: 0.7497779726982117\n",
      "Loss: 1.2407103776931763\n",
      "Loss: 0.7896282076835632\n",
      "Loss: 1.0625354051589966\n",
      "Loss: 0.7084797620773315\n",
      "Loss: 0.9143081903457642\n",
      "Loss: 0.9913882613182068\n",
      "Loss: 1.1526762247085571\n",
      "Loss: 0.7158167958259583\n",
      "Loss: 0.9760000109672546\n",
      "Loss: 0.5834033489227295\n",
      "Loss: 0.9484937787055969\n",
      "Loss: 0.887826144695282\n",
      "Loss: 0.9631707668304443\n",
      "Loss: 0.6344814896583557\n",
      "Loss: 0.77569180727005\n",
      "Loss: 0.5783310532569885\n",
      "Loss: 0.855557918548584\n",
      "Loss: 0.785605788230896\n",
      "Loss: 0.6604323387145996\n",
      "Loss: 0.6902643442153931\n",
      "Loss: 0.5916829109191895\n",
      "Loss: 0.582550585269928\n",
      "Loss: 0.6255059242248535\n",
      "Loss: 0.5278822183609009\n",
      "Loss: 0.4391552805900574\n",
      "Loss: 0.7104799151420593\n",
      "Loss: 0.6433985829353333\n",
      "Loss: 0.5442997813224792\n",
      "Loss: 0.6188858151435852\n",
      "Loss: 0.8065359592437744\n",
      "Loss: 0.4662952423095703\n",
      "Loss: 0.7971994280815125\n",
      "Loss: 0.6887319684028625\n",
      "Loss: 0.7015193104743958\n",
      "Loss: 0.714817225933075\n",
      "Loss: 0.5963805317878723\n",
      "Loss: 0.6549336910247803\n",
      "Loss: 0.4745970070362091\n",
      "Loss: 0.5492669343948364\n",
      "Loss: 0.63679039478302\n",
      "Loss: 0.6011408567428589\n",
      "Loss: 0.5333707928657532\n",
      "Loss: 0.5540711283683777\n",
      "Loss: 0.5583649277687073\n",
      "Loss: 0.6023623943328857\n",
      "Loss: 0.7517409324645996\n",
      "Loss: 0.6065083742141724\n",
      "Loss: 0.38842618465423584\n",
      "Loss: 0.43182432651519775\n",
      "Loss: 0.5779937505722046\n",
      "Loss: 0.5424186587333679\n",
      "Loss: 0.40734943747520447\n",
      "Loss: 0.6153333783149719\n",
      "Loss: 0.7364652752876282\n",
      "Loss: 0.5251052975654602\n",
      "Loss: 0.5939311385154724\n",
      "Loss: 0.5442988872528076\n",
      "Loss: 0.81600022315979\n",
      "Loss: 0.7795588970184326\n",
      "Loss: 0.5468649864196777\n",
      "Loss: 0.5434294939041138\n",
      "Loss: 0.6576409339904785\n",
      "Loss: 0.8202240467071533\n",
      "Loss: 0.6129579544067383\n",
      "Loss: 0.8258001208305359\n",
      "Loss: 0.44938138127326965\n",
      "Loss: 0.5436639785766602\n",
      "Loss: 0.6669402122497559\n",
      "Loss: 0.48085078597068787\n",
      "Loss: 0.3403793275356293\n",
      "Loss: 0.4843599200248718\n",
      "Loss: 0.6146560907363892\n",
      "Loss: 0.5078871250152588\n",
      "Loss: 0.6409554481506348\n",
      "Loss: 0.5237827897071838\n",
      "Loss: 0.6547241806983948\n",
      "Loss: 0.5119748115539551\n",
      "Loss: 0.4240010976791382\n",
      "Loss: 0.3973073661327362\n",
      "Loss: 0.5343912839889526\n",
      "Loss: 0.6492443084716797\n",
      "Loss: 0.5257025361061096\n",
      "Loss: 0.3524962365627289\n",
      "Loss: 0.36044827103614807\n",
      "Loss: 0.45892035961151123\n",
      "Loss: 0.5448922514915466\n",
      "Loss: 0.6288831233978271\n",
      "Loss: 0.2763536870479584\n",
      "Loss: 0.5150336027145386\n",
      "Loss: 0.47582897543907166\n",
      "Loss: 0.38175392150878906\n",
      "Loss: 0.3889451324939728\n",
      "Loss: 0.2916813790798187\n",
      "Loss: 0.5249477028846741\n",
      "Loss: 0.5515916347503662\n",
      "Loss: 0.6000354886054993\n",
      "Loss: 0.48780936002731323\n",
      "Loss: 0.4001312553882599\n",
      "Loss: 0.6504797339439392\n",
      "Loss: 0.5554597973823547\n",
      "Loss: 0.4943387508392334\n",
      "Loss: 0.6511409878730774\n",
      "Loss: 0.35750094056129456\n",
      "Loss: 0.5012309551239014\n",
      "Loss: 0.5199050307273865\n",
      "Loss: 0.6443434953689575\n",
      "Loss: 0.3283081352710724\n",
      "Loss: 0.8715335130691528\n",
      "Loss: 0.6277564764022827\n",
      "Loss: 0.6185768246650696\n",
      "Loss: 0.5985292196273804\n",
      "Loss: 0.5986067652702332\n",
      "Loss: 0.5082780122756958\n",
      "Loss: 0.6443288922309875\n",
      "Loss: 0.8959285616874695\n",
      "Loss: 0.4078635275363922\n",
      "Loss: 0.3565414845943451\n",
      "Loss: 0.4857374429702759\n",
      "Loss: 0.4404153823852539\n",
      "Loss: 0.5834243297576904\n",
      "Loss: 0.7090566158294678\n",
      "Loss: 0.4836675226688385\n",
      "Loss: 0.3970966041088104\n",
      "Loss: 0.5419777035713196\n",
      "Loss: 0.615688681602478\n",
      "Loss: 0.4910403788089752\n",
      "Loss: 0.45986390113830566\n",
      "Loss: 0.27215343713760376\n",
      "Loss: 0.4641689360141754\n",
      "Loss: 0.5674651861190796\n",
      "Loss: 0.44358694553375244\n",
      "Loss: 0.358264684677124\n",
      "Loss: 0.2430974841117859\n",
      "Loss: 0.48179447650909424\n",
      "Loss: 0.48538878560066223\n",
      "Loss: 0.428857684135437\n",
      "Loss: 0.18337270617485046\n",
      "Loss: 0.6895189881324768\n",
      "Loss: 0.4804358184337616\n",
      "Loss: 0.6260321140289307\n",
      "Loss: 0.5340413451194763\n",
      "Loss: 0.522799551486969\n",
      "Loss: 0.4367396831512451\n",
      "Loss: 0.6099596619606018\n",
      "Loss: 0.4536485970020294\n",
      "Loss: 0.41749873757362366\n",
      "Loss: 0.5368456840515137\n",
      "Loss: 0.4131329357624054\n",
      "Loss: 0.5053245425224304\n",
      "Loss: 0.3956930935382843\n",
      "Loss: 0.35001736879348755\n",
      "Loss: 0.38525184988975525\n",
      "Loss: 0.4006056487560272\n",
      "Loss: 0.45190533995628357\n",
      "Loss: 0.33022570610046387\n",
      "Loss: 0.5745271444320679\n",
      "Loss: 0.4496345818042755\n",
      "Loss: 0.46638351678848267\n",
      "Loss: 0.43902093172073364\n",
      "Loss: 0.3537972569465637\n",
      "Loss: 0.43281278014183044\n",
      "Loss: 0.5157548785209656\n",
      "Loss: 0.5883422493934631\n",
      "Loss: 0.2518303096294403\n",
      "Loss: 0.38858914375305176\n",
      "Loss: 0.46631088852882385\n",
      "Loss: 0.5299976468086243\n",
      "Loss: 0.49465617537498474\n",
      "Loss: 0.39623087644577026\n",
      "Loss: 0.5943689942359924\n",
      "Loss: 0.4889284372329712\n",
      "Loss: 0.5067320466041565\n",
      "Loss: 0.5074146389961243\n",
      "Loss: 0.44838467240333557\n",
      "Loss: 0.4672022759914398\n",
      "Loss: 0.43490660190582275\n",
      "Loss: 0.7769875526428223\n",
      "Loss: 0.48078083992004395\n",
      "Loss: 0.6785175204277039\n",
      "Loss: 0.4815542995929718\n",
      "Loss: 0.41931402683258057\n",
      "Loss: 0.2862737774848938\n",
      "Loss: 0.4187578856945038\n",
      "Loss: 0.38122594356536865\n",
      "Loss: 0.5063826441764832\n",
      "Loss: 0.5142132043838501\n",
      "Loss: 0.43430086970329285\n",
      "Loss: 0.5191208124160767\n",
      "Loss: 0.507032036781311\n",
      "Loss: 0.46150124073028564\n",
      "Loss: 0.49454161524772644\n",
      "Loss: 0.2748764753341675\n",
      "Loss: 0.49135053157806396\n",
      "Loss: 0.47090622782707214\n",
      "Loss: 0.4638882875442505\n",
      "Loss: 0.5314971804618835\n",
      "Loss: 0.608052670955658\n",
      "Loss: 0.272301584482193\n",
      "Loss: 0.31798332929611206\n",
      "Loss: 0.39604976773262024\n",
      "Loss: 0.4917261600494385\n",
      "Loss: 0.3868936598300934\n",
      "Loss: 0.3179500102996826\n",
      "Loss: 0.355057030916214\n",
      "Loss: 0.35496699810028076\n",
      "Loss: 0.5445602536201477\n",
      "Loss: 0.5232430100440979\n",
      "Loss: 0.4029141366481781\n",
      "Loss: 0.37083569169044495\n",
      "Loss: 0.535707950592041\n",
      "Loss: 0.5538980960845947\n",
      "Loss: 0.37350791692733765\n",
      "Loss: 0.3924731910228729\n",
      "Loss: 0.4175334572792053\n",
      "Loss: 0.4258732497692108\n",
      "Loss: 0.49486246705055237\n",
      "Loss: 0.42806750535964966\n",
      "Loss: 0.4762956202030182\n",
      "Loss: 0.3153529167175293\n",
      "Loss: 0.6200377345085144\n",
      "Loss: 0.3949280083179474\n",
      "Loss: 0.39970862865448\n",
      "Loss: 0.6119500398635864\n",
      "Loss: 0.4026046395301819\n",
      "Loss: 0.3794082701206207\n",
      "Loss: 0.33777081966400146\n",
      "Loss: 0.2463855892419815\n",
      "Loss: 0.37831342220306396\n",
      "Loss: 0.33623936772346497\n",
      "Loss: 0.3758199214935303\n",
      "Loss: 0.21494974195957184\n",
      "Loss: 0.38137543201446533\n",
      "Loss: 0.49433887004852295\n",
      "Loss: 0.19977502524852753\n",
      "Loss: 0.4523589313030243\n",
      "Loss: 0.7497135400772095\n",
      "Loss: 0.33381253480911255\n",
      "Loss: 0.46793776750564575\n",
      "Loss: 0.7042276263237\n",
      "Loss: 0.6285188794136047\n",
      "Loss: 0.5140043497085571\n",
      "Loss: 0.31333595514297485\n",
      "Loss: 0.41862913966178894\n",
      "Loss: 0.4887511730194092\n",
      "Loss: 0.3183273375034332\n",
      "Loss: 0.4398829936981201\n",
      "Loss: 0.24264872074127197\n",
      "Loss: 0.4608848989009857\n",
      "Loss: 0.546837568283081\n",
      "Loss: 0.2324102818965912\n",
      "Loss: 0.6244394183158875\n",
      "Loss: 0.430067777633667\n",
      "Loss: 0.15081995725631714\n",
      "Loss: 0.5070884823799133\n",
      "Loss: 0.5562288761138916\n",
      "Loss: 0.5153207778930664\n",
      "Loss: 0.6819117665290833\n",
      "Loss: 0.4384757876396179\n",
      "Loss: 0.48762571811676025\n",
      "Loss: 0.38269713521003723\n",
      "Loss: 0.4348209500312805\n",
      "Loss: 0.44486305117607117\n",
      "Loss: 0.44272568821907043\n",
      "Loss: 0.4825270473957062\n",
      "Loss: 0.380547434091568\n",
      "Loss: 0.7297711372375488\n",
      "Loss: 0.31233009696006775\n",
      "Loss: 0.4143061339855194\n",
      "Loss: 0.5362734794616699\n",
      "Loss: 0.3037816882133484\n",
      "Loss: 0.6022073030471802\n",
      "Loss: 0.2215551882982254\n",
      "Loss: 0.2418030947446823\n",
      "Loss: 0.3051425516605377\n",
      "Loss: 0.606187641620636\n",
      "Loss: 0.20691746473312378\n",
      "Loss: 0.6240139603614807\n",
      "Loss: 0.3093472421169281\n",
      "Loss: 0.45671749114990234\n",
      "Loss: 0.34034308791160583\n",
      "Loss: 0.4970591366291046\n",
      "Loss: 0.6857367157936096\n",
      "Loss: 0.4369327425956726\n",
      "Loss: 0.5443594455718994\n",
      "Loss: 0.6654200553894043\n",
      "Loss: 0.43559154868125916\n",
      "Loss: 0.64671790599823\n",
      "Loss: 0.2032995969057083\n",
      "Loss: 0.48272934556007385\n",
      "Loss: 0.3447946310043335\n",
      "Loss: 0.4146990180015564\n",
      "Loss: 0.6677850484848022\n",
      "Loss: 0.3928903639316559\n",
      "Loss: 0.3775356709957123\n",
      "Loss: 0.49986717104911804\n",
      "Loss: 0.35060974955558777\n",
      "Loss: 0.44222745299339294\n",
      "Loss: 0.5018348097801208\n",
      "Loss: 0.5821858048439026\n",
      "Loss: 0.7040693163871765\n",
      "Loss: 0.2975279688835144\n",
      "Loss: 0.338459849357605\n",
      "Loss: 0.3930247128009796\n",
      "Loss: 0.6220541596412659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.32520249485969543\n",
      "Loss: 0.3866533041000366\n",
      "Loss: 0.30278462171554565\n",
      "Loss: 0.4906283915042877\n",
      "Loss: 0.3926841616630554\n",
      "Loss: 0.4692765474319458\n",
      "Loss: 0.3087824285030365\n",
      "Loss: 0.414774090051651\n",
      "Loss: 0.4170227348804474\n",
      "Loss: 0.3196147382259369\n",
      "Loss: 0.2323639690876007\n",
      "Loss: 0.3461788594722748\n",
      "Loss: 0.3920917809009552\n",
      "Loss: 0.4192280173301697\n",
      "Loss: 0.30518895387649536\n",
      "Loss: 0.4699631631374359\n",
      "Loss: 0.5305116176605225\n",
      "Loss: 0.2777816653251648\n",
      "Loss: 0.3318922519683838\n",
      "Loss: 0.31981778144836426\n",
      "Loss: 0.5919685959815979\n",
      "Loss: 0.4439484775066376\n",
      "Loss: 0.36407792568206787\n",
      "Loss: 0.5106537342071533\n",
      "Loss: 0.7767918109893799\n",
      "Loss: 0.6996302008628845\n",
      "Loss: 0.28753283619880676\n",
      "Loss: 0.3969210088253021\n",
      "Loss: 0.3770645260810852\n",
      "Loss: 0.4471604824066162\n",
      "Loss: 0.36707016825675964\n",
      "Loss: 0.6167523860931396\n",
      "Loss: 0.31431716680526733\n",
      "Loss: 0.3210548460483551\n",
      "Loss: 0.47650691866874695\n",
      "Loss: 0.30715706944465637\n",
      "Loss: 0.2842501401901245\n",
      "Loss: 0.6917865872383118\n",
      "Loss: 0.43185776472091675\n",
      "Loss: 0.41959092020988464\n",
      "Loss: 0.7862992882728577\n",
      "Loss: 0.2574644982814789\n",
      "Loss: 0.31374916434288025\n",
      "Loss: 0.42679259181022644\n",
      "Loss: 0.3746863603591919\n",
      "Loss: 0.3374975621700287\n",
      "Loss: 0.24399562180042267\n",
      "Loss: 0.28372058272361755\n",
      "Loss: 0.5046999454498291\n",
      "Loss: 0.24675248563289642\n",
      "Loss: 0.4864574670791626\n",
      "Loss: 0.724484384059906\n",
      "Loss: 0.2694690525531769\n",
      "Loss: 0.5912125110626221\n",
      "Loss: 0.5213215351104736\n",
      "Loss: 0.3843790292739868\n",
      "Loss: 0.3964659571647644\n",
      "Loss: 0.7011682391166687\n",
      "Loss: 0.20581042766571045\n",
      "Loss: 0.4170544147491455\n",
      "Loss: 0.41034671664237976\n",
      "Loss: 0.30138134956359863\n",
      "Loss: 0.48354971408843994\n",
      "Loss: 0.2768014073371887\n",
      "Loss: 0.5129207372665405\n",
      "Loss: 0.4243081212043762\n",
      "Loss: 0.5057191848754883\n",
      "Loss: 0.556321918964386\n",
      "Loss: 0.440443754196167\n",
      "Loss: 0.539770781993866\n",
      "Loss: 0.24867860972881317\n",
      "Loss: 0.39045965671539307\n",
      "Loss: 0.4011198878288269\n",
      "Loss: 0.3882284462451935\n",
      "Loss: 0.43614327907562256\n",
      "Loss: 0.44589605927467346\n",
      "Loss: 0.3342241644859314\n",
      "Loss: 0.48364657163619995\n",
      "Loss: 0.42185521125793457\n",
      "Loss: 0.4843907952308655\n",
      "Loss: 0.49592822790145874\n",
      "Loss: 0.47619688510894775\n",
      "Loss: 0.5064958333969116\n",
      "Loss: 0.2891749441623688\n",
      "Loss: 0.3674312233924866\n",
      "Loss: 0.2592819631099701\n",
      "Loss: 0.3402899205684662\n",
      "Loss: 0.7622454762458801\n",
      "Loss: 0.6461467742919922\n",
      "Loss: 0.49623697996139526\n",
      "Loss: 0.2517760992050171\n",
      "Loss: 0.38497352600097656\n",
      "Loss: 0.2530459761619568\n",
      "Loss: 0.29458191990852356\n",
      "Loss: 0.29331663250923157\n",
      "Loss: 0.5348180532455444\n",
      "Loss: 0.3824390769004822\n",
      "Loss: 0.4967469871044159\n",
      "Loss: 0.32214218378067017\n",
      "Loss: 0.3266078531742096\n",
      "Loss: 0.503812849521637\n",
      "Loss: 0.38225382566452026\n",
      "Loss: 0.6047716736793518\n",
      "Loss: 0.4308169186115265\n",
      "Loss: 0.415706068277359\n",
      "Loss: 0.5472781658172607\n",
      "Loss: 0.5032640695571899\n",
      "Loss: 0.4846939742565155\n",
      "Loss: 0.3677064776420593\n",
      "Loss: 0.3268585205078125\n",
      "Loss: 0.6573453545570374\n",
      "Loss: 0.5304746031761169\n",
      "Loss: 0.39380142092704773\n",
      "Loss: 0.36142975091934204\n",
      "Loss: 0.57347571849823\n",
      "Loss: 0.546729564666748\n",
      "Loss: 0.41706913709640503\n",
      "Loss: 0.5116623640060425\n",
      "Loss: 0.4196792244911194\n",
      "Loss: 0.4815758764743805\n",
      "Loss: 0.6159100532531738\n",
      "Loss: 0.4243538975715637\n",
      "Loss: 0.26060745120048523\n",
      "Loss: 0.7704609632492065\n",
      "Loss: 0.37757545709609985\n",
      "Loss: 0.28704535961151123\n",
      "Loss: 0.5417618751525879\n",
      "Loss: 0.3616763651371002\n",
      "Loss: 0.4212651550769806\n",
      "Loss: 0.627631664276123\n",
      "Loss: 0.266887366771698\n",
      "Loss: 0.37235018610954285\n",
      "Loss: 0.295294851064682\n",
      "Loss: 0.45845890045166016\n",
      "Loss: 0.33749908208847046\n",
      "Loss: 0.26335644721984863\n",
      "Loss: 0.2481408566236496\n",
      "Loss: 0.43364661931991577\n",
      "Loss: 0.38936737179756165\n",
      "Loss: 0.4322163760662079\n",
      "Loss: 0.259967178106308\n",
      "Loss: 0.4071466326713562\n",
      "Loss: 0.3559643626213074\n",
      "Loss: 0.3686303198337555\n",
      "Loss: 0.4077285826206207\n",
      "Loss: 0.29180946946144104\n",
      "Loss: 0.2537910044193268\n",
      "Loss: 0.4581010341644287\n",
      "Loss: 0.30192339420318604\n",
      "Loss: 0.45850110054016113\n",
      "Loss: 0.39801326394081116\n",
      "Loss: 0.3217315971851349\n",
      "Loss: 0.3097909986972809\n",
      "Loss: 0.7203560471534729\n",
      "Loss: 0.7658122777938843\n",
      "Loss: 0.45839300751686096\n",
      "Loss: 0.2965174615383148\n",
      "Loss: 0.5001672506332397\n",
      "Loss: 0.4465161859989166\n",
      "Loss: 0.3288698196411133\n",
      "Loss: 0.6789761185646057\n",
      "Loss: 0.5081931352615356\n",
      "Loss: 0.415121853351593\n",
      "Loss: 0.41006410121917725\n",
      "Loss: 0.3351350426673889\n",
      "Loss: 0.21081610023975372\n",
      "Loss: 0.40465372800827026\n",
      "Loss: 0.34390124678611755\n",
      "Loss: 0.3022383153438568\n",
      "Loss: 0.24585071206092834\n",
      "Loss: 0.5327871441841125\n",
      "Loss: 0.36872851848602295\n",
      "Loss: 0.32209575176239014\n",
      "Loss: 0.454700231552124\n",
      "Loss: 0.3052055239677429\n",
      "Loss: 0.4505535364151001\n",
      "Loss: 0.4976973831653595\n",
      "Loss: 0.250316858291626\n",
      "Loss: 0.4984755218029022\n",
      "Loss: 0.33241090178489685\n",
      "Loss: 0.6039620637893677\n",
      "Loss: 0.5532684922218323\n",
      "Loss: 0.5159661769866943\n",
      "Loss: 0.3249421715736389\n",
      "Loss: 0.5002386569976807\n",
      "Loss: 0.5969955921173096\n",
      "Loss: 0.3984774649143219\n",
      "Loss: 0.7546885013580322\n",
      "Loss: 0.6040011644363403\n",
      "Loss: 0.37505480647087097\n",
      "Loss: 0.3684942126274109\n",
      "Loss: 0.7511503100395203\n",
      "Loss: 0.3806025981903076\n",
      "Loss: 0.3771302103996277\n",
      "Loss: 0.37015682458877563\n",
      "Loss: 0.3796675503253937\n",
      "Loss: 0.42241206765174866\n",
      "Loss: 0.4468529522418976\n",
      "Loss: 0.30867111682891846\n",
      "Loss: 0.6517702341079712\n",
      "Loss: 0.4761461019515991\n",
      "Loss: 0.5462249517440796\n",
      "Loss: 0.23081396520137787\n",
      "Loss: 0.4789567291736603\n",
      "Loss: 0.5846704840660095\n",
      "Loss: 0.5323551893234253\n",
      "Loss: 0.4275154769420624\n",
      "Loss: 0.4140024185180664\n",
      "Loss: 0.48294028639793396\n",
      "Loss: 0.5403227210044861\n",
      "Loss: 0.43540728092193604\n",
      "Loss: 0.4168626368045807\n",
      "Loss: 0.5571714639663696\n",
      "Loss: 0.3283769488334656\n",
      "Loss: 0.260883092880249\n",
      "Loss: 0.4502139389514923\n",
      "Loss: 0.5097212791442871\n",
      "Loss: 0.5584924817085266\n",
      "Loss: 0.5825048685073853\n",
      "Loss: 0.5491763949394226\n",
      "Loss: 0.3614477515220642\n",
      "Loss: 0.37591874599456787\n",
      "Loss: 0.3728947341442108\n",
      "Loss: 0.2901821434497833\n",
      "Loss: 0.36964473128318787\n",
      "Loss: 0.24938276410102844\n",
      "Loss: 0.5224370360374451\n",
      "Loss: 0.28793564438819885\n",
      "Loss: 0.5008692741394043\n",
      "Loss: 0.37671521306037903\n",
      "Loss: 0.7442277669906616\n",
      "Loss: 0.26543715596199036\n",
      "Loss: 0.6168496608734131\n",
      "Loss: 0.32180720567703247\n",
      "Loss: 0.41449737548828125\n",
      "Loss: 0.36688366532325745\n",
      "Loss: 0.3833615481853485\n",
      "Loss: 0.2207309603691101\n",
      "Loss: 0.5179222226142883\n",
      "Loss: 0.24444246292114258\n",
      "Loss: 0.5152536630630493\n",
      "Loss: 0.3307154178619385\n",
      "Loss: 0.5504682064056396\n",
      "Loss: 0.4961526095867157\n",
      "Loss: 0.34531140327453613\n",
      "Loss: 0.4703405499458313\n",
      "Loss: 0.31161436438560486\n",
      "Loss: 0.7204018235206604\n",
      "Loss: 0.20388290286064148\n",
      "Loss: 0.4383942782878876\n",
      "Loss: 0.7095072865486145\n",
      "Loss: 0.6148251891136169\n",
      "Loss: 0.3128572106361389\n",
      "Loss: 0.24738091230392456\n",
      "Loss: 0.3197855055332184\n",
      "Loss: 0.4381237328052521\n",
      "Loss: 0.49667853116989136\n",
      "Loss: 0.5070154666900635\n",
      "Loss: 0.4550859332084656\n",
      "Loss: 0.6361355185508728\n",
      "Loss: 0.5500924587249756\n",
      "Loss: 0.34525448083877563\n",
      "Loss: 0.5448025465011597\n",
      "Loss: 0.4859658181667328\n",
      "Loss: 0.2999955415725708\n"
     ]
    }
   ],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    w = torch.randn(size=shape)*0.01\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "#(b)\n",
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "#end (b)\n",
    "    \n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h_ = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2_ = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax.transpose(0,1)\n",
    "\n",
    "\n",
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    noise_py_x = noise_py_x.transpose(0,1)\n",
    "    #print(np.shape(noise_py_x), np.shape(y))\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu(X,a):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

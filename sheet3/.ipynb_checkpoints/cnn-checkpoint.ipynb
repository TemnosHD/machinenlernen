{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3650574684143066\n",
      "Loss: 2.4331960678100586\n",
      "Loss: 2.265251398086548\n",
      "Loss: 2.1515004634857178\n",
      "Loss: 2.122255563735962\n",
      "Loss: 2.0780086517333984\n",
      "Loss: 1.9510952234268188\n",
      "Loss: 1.7534270286560059\n",
      "Loss: 1.7162812948226929\n",
      "Loss: 1.6557480096817017\n",
      "Loss: 1.5992196798324585\n",
      "Loss: 1.4189345836639404\n",
      "Loss: 1.5806589126586914\n",
      "Loss: 1.5623953342437744\n",
      "Loss: 1.2064529657363892\n",
      "Loss: 1.1244980096817017\n",
      "Loss: 1.1663738489151\n",
      "Loss: 1.2691872119903564\n",
      "Loss: 1.3251729011535645\n",
      "Loss: 0.9300680756568909\n",
      "Loss: 0.9207238554954529\n",
      "Loss: 0.9960853457450867\n",
      "Loss: 0.8971188068389893\n",
      "Loss: 0.9928513169288635\n",
      "Loss: 0.9217621088027954\n",
      "Loss: 1.1288933753967285\n",
      "Loss: 0.9542943835258484\n",
      "Loss: 0.7582323551177979\n",
      "Loss: 1.0451061725616455\n",
      "Loss: 1.059401512145996\n",
      "Loss: 0.8125346302986145\n",
      "Loss: 0.8085698485374451\n",
      "Loss: 0.8077687621116638\n",
      "Loss: 0.7307479381561279\n",
      "Loss: 0.9974827766418457\n",
      "Loss: 0.8421211242675781\n",
      "Loss: 1.032852053642273\n",
      "Loss: 0.8811557292938232\n",
      "Loss: 1.1567177772521973\n",
      "Loss: 1.06706702709198\n",
      "Loss: 0.8559035658836365\n",
      "Loss: 1.0791888236999512\n",
      "Loss: 0.8036449551582336\n",
      "Loss: 0.6844608187675476\n",
      "Loss: 0.9883559942245483\n",
      "Loss: 1.1506540775299072\n",
      "Loss: 1.0397189855575562\n",
      "Loss: 0.6055870056152344\n",
      "Loss: 0.7405663132667542\n",
      "Loss: 0.9825933575630188\n",
      "Loss: 0.7988519072532654\n",
      "Loss: 0.9820525646209717\n",
      "Loss: 0.8285650610923767\n",
      "Loss: 1.0368586778640747\n",
      "Loss: 1.0683485269546509\n",
      "Loss: 1.154941201210022\n",
      "Loss: 0.9600487351417542\n",
      "Loss: 0.6293689608573914\n",
      "Loss: 0.564500093460083\n",
      "Loss: 0.8129035234451294\n",
      "Loss: 0.6928654313087463\n",
      "Loss: 0.6508433818817139\n",
      "Loss: 0.7288529872894287\n",
      "Loss: 0.6771632432937622\n",
      "Loss: 0.6286119818687439\n",
      "Loss: 0.9556434154510498\n",
      "Loss: 0.8439106941223145\n",
      "Loss: 0.7661811113357544\n",
      "Loss: 0.5859904885292053\n",
      "Loss: 0.6611115336418152\n",
      "Loss: 0.7714277505874634\n",
      "Loss: 0.632220447063446\n",
      "Loss: 0.7729300856590271\n",
      "Loss: 0.6190295219421387\n",
      "Loss: 0.8990380764007568\n",
      "Loss: 0.6540015339851379\n",
      "Loss: 0.6201560497283936\n",
      "Loss: 0.7160980105400085\n",
      "Loss: 0.6847814917564392\n",
      "Loss: 0.5895481705665588\n",
      "Loss: 0.6408910155296326\n",
      "Loss: 0.6109510660171509\n",
      "Loss: 0.6835634708404541\n",
      "Loss: 0.692812979221344\n",
      "Loss: 0.5034008622169495\n",
      "Loss: 0.5447413921356201\n",
      "Loss: 0.7732231020927429\n",
      "Loss: 0.791365385055542\n",
      "Loss: 0.7652235627174377\n",
      "Loss: 0.8104759454727173\n",
      "Loss: 0.6577518582344055\n",
      "Loss: 0.6512376666069031\n",
      "Loss: 0.6451898217201233\n",
      "Loss: 0.684539258480072\n",
      "Loss: 0.5042752623558044\n",
      "Loss: 0.7702072858810425\n",
      "Loss: 0.5851987600326538\n",
      "Loss: 0.5781981945037842\n",
      "Loss: 0.6168696880340576\n",
      "Loss: 0.7753382325172424\n",
      "Loss: 0.4587059020996094\n",
      "Loss: 0.6291112899780273\n",
      "Loss: 0.533973753452301\n",
      "Loss: 0.7789945006370544\n",
      "Loss: 0.6855491399765015\n",
      "Loss: 0.6686776876449585\n",
      "Loss: 0.5873626470565796\n",
      "Loss: 0.33462753891944885\n",
      "Loss: 0.6334778070449829\n",
      "Loss: 0.6278411746025085\n",
      "Loss: 0.6651527285575867\n",
      "Loss: 0.5597331523895264\n",
      "Loss: 0.45526495575904846\n",
      "Loss: 0.5389357209205627\n",
      "Loss: 0.5272625088691711\n",
      "Loss: 0.4685162603855133\n",
      "Loss: 0.7639849781990051\n",
      "Loss: 0.6656653881072998\n",
      "Loss: 0.6456759572029114\n",
      "Loss: 0.4453032612800598\n",
      "Loss: 0.43424704670906067\n",
      "Loss: 0.5852906107902527\n",
      "Loss: 0.47679123282432556\n",
      "Loss: 0.7452694177627563\n",
      "Loss: 0.7225393056869507\n",
      "Loss: 0.7032426595687866\n",
      "Loss: 0.4811435341835022\n",
      "Loss: 0.6189815998077393\n",
      "Loss: 0.7163064479827881\n",
      "Loss: 0.6587616205215454\n",
      "Loss: 0.5449330806732178\n",
      "Loss: 0.6396982073783875\n",
      "Loss: 0.4499250650405884\n",
      "Loss: 0.6364572048187256\n",
      "Loss: 0.37947237491607666\n",
      "Loss: 0.4630916714668274\n",
      "Loss: 0.5744653344154358\n",
      "Loss: 0.6387542486190796\n",
      "Loss: 0.5287184715270996\n",
      "Loss: 0.6385812759399414\n",
      "Loss: 0.5127446055412292\n",
      "Loss: 0.5618216395378113\n",
      "Loss: 0.7225258350372314\n",
      "Loss: 0.45201846957206726\n",
      "Loss: 0.721106767654419\n",
      "Loss: 0.6852401494979858\n",
      "Loss: 0.37845003604888916\n",
      "Loss: 0.5617449283599854\n",
      "Loss: 0.36711472272872925\n",
      "Loss: 0.6054775714874268\n",
      "Loss: 0.47314757108688354\n",
      "Loss: 0.46259385347366333\n",
      "Loss: 0.412568062543869\n",
      "Loss: 0.3601912260055542\n",
      "Loss: 0.7341428399085999\n",
      "Loss: 0.553292989730835\n",
      "Loss: 0.34219038486480713\n",
      "Loss: 0.6733389496803284\n",
      "Loss: 0.6352821588516235\n",
      "Loss: 0.5677583813667297\n",
      "Loss: 0.3381854295730591\n",
      "Loss: 0.44535884261131287\n",
      "Loss: 0.6133540868759155\n",
      "Loss: 0.5945792198181152\n",
      "Loss: 0.6529228091239929\n",
      "Loss: 0.6352332830429077\n",
      "Loss: 0.5425738096237183\n",
      "Loss: 0.4083259105682373\n",
      "Loss: 0.509526252746582\n",
      "Loss: 0.5691691040992737\n",
      "Loss: 0.22077573835849762\n",
      "Loss: 0.9056119322776794\n",
      "Loss: 0.537041187286377\n",
      "Loss: 0.3875855505466461\n",
      "Loss: 0.6468822956085205\n",
      "Loss: 0.4897061288356781\n",
      "Loss: 0.6064438819885254\n",
      "Loss: 0.347486674785614\n",
      "Loss: 0.3994227945804596\n",
      "Loss: 0.47996777296066284\n",
      "Loss: 0.25438565015792847\n",
      "Loss: 0.6974520087242126\n",
      "Loss: 0.6072283387184143\n",
      "Loss: 0.5223544239997864\n",
      "Loss: 0.4216822385787964\n",
      "Loss: 0.4069603979587555\n",
      "Loss: 0.48031002283096313\n",
      "Loss: 0.4957202076911926\n",
      "Loss: 0.5229082703590393\n",
      "Loss: 0.4940219223499298\n",
      "Loss: 0.69359290599823\n",
      "Loss: 0.26708799600601196\n",
      "Loss: 0.42114636301994324\n",
      "Loss: 0.42234596610069275\n",
      "Loss: 0.6391850709915161\n",
      "Loss: 0.44184571504592896\n",
      "Loss: 0.6529497504234314\n",
      "Loss: 0.4917890429496765\n",
      "Loss: 0.4363159239292145\n",
      "Loss: 0.570524275302887\n",
      "Loss: 0.5075458288192749\n",
      "Loss: 0.47657641768455505\n",
      "Loss: 0.5528730154037476\n",
      "Loss: 0.28546473383903503\n",
      "Loss: 0.29306769371032715\n",
      "Loss: 0.6023327708244324\n",
      "Loss: 0.4849349856376648\n",
      "Loss: 0.7168842554092407\n",
      "Loss: 0.5415575504302979\n",
      "Loss: 0.6548671722412109\n",
      "Loss: 0.2951251268386841\n",
      "Loss: 0.18846163153648376\n",
      "Loss: 0.4648306369781494\n",
      "Loss: 0.3907495141029358\n",
      "Loss: 0.5814256072044373\n",
      "Loss: 0.3388889729976654\n",
      "Loss: 0.47552451491355896\n",
      "Loss: 1.1211563348770142\n",
      "Loss: 0.48804762959480286\n",
      "Loss: 0.5485422611236572\n",
      "Loss: 0.6147031188011169\n",
      "Loss: 0.5625352263450623\n",
      "Loss: 0.5146719217300415\n",
      "Loss: 0.42221206426620483\n",
      "Loss: 0.3585098683834076\n",
      "Loss: 0.6060096025466919\n",
      "Loss: 0.23304355144500732\n",
      "Loss: 0.5501836538314819\n",
      "Loss: 0.5149670243263245\n",
      "Loss: 0.40778055787086487\n",
      "Loss: 0.590813159942627\n",
      "Loss: 0.3336839973926544\n",
      "Loss: 0.4750367999076843\n",
      "Loss: 0.719468891620636\n",
      "Loss: 0.4458407461643219\n",
      "Loss: 0.6704366207122803\n",
      "Loss: 0.4697405993938446\n",
      "Loss: 0.41843825578689575\n",
      "Loss: 0.4971904754638672\n",
      "Loss: 0.30161431431770325\n",
      "Loss: 0.4082678258419037\n",
      "Loss: 0.47146713733673096\n",
      "Loss: 0.3059348165988922\n",
      "Loss: 0.32713109254837036\n",
      "Loss: 0.45287859439849854\n",
      "Loss: 0.4473413825035095\n",
      "Loss: 0.5287097692489624\n",
      "Loss: 0.4739595651626587\n",
      "Loss: 0.3712783753871918\n",
      "Loss: 0.4780649244785309\n",
      "Loss: 0.2829945385456085\n",
      "Loss: 0.2540278732776642\n",
      "Loss: 0.3392035961151123\n",
      "Loss: 0.39597782492637634\n",
      "Loss: 0.31442761421203613\n",
      "Loss: 0.2663291096687317\n",
      "Loss: 0.3821934461593628\n",
      "Loss: 0.3601236045360565\n",
      "Loss: 0.5823522806167603\n",
      "Loss: 0.39646485447883606\n",
      "Loss: 0.504083514213562\n",
      "Loss: 0.37505611777305603\n",
      "Loss: 0.5243967175483704\n",
      "Loss: 0.3995676040649414\n",
      "Loss: 0.5160486698150635\n",
      "Loss: 0.389894038438797\n",
      "Loss: 0.5172825455665588\n",
      "Loss: 0.3714984655380249\n",
      "Loss: 0.5091200470924377\n",
      "Loss: 0.37327101826667786\n",
      "Loss: 0.4434426426887512\n",
      "Loss: 0.7227074503898621\n",
      "Loss: 0.499289870262146\n",
      "Loss: 0.7013443112373352\n",
      "Loss: 0.3415789306163788\n",
      "Loss: 0.638639509677887\n",
      "Loss: 0.6647087335586548\n",
      "Loss: 0.670081615447998\n",
      "Loss: 0.4969065487384796\n",
      "Loss: 0.7542670369148254\n",
      "Loss: 0.35829639434814453\n",
      "Loss: 0.43808040022850037\n",
      "Loss: 0.3624533712863922\n",
      "Loss: 0.3744394779205322\n",
      "Loss: 0.49683624505996704\n",
      "Loss: 0.5152036547660828\n",
      "Loss: 0.5353504419326782\n",
      "Loss: 0.623234748840332\n",
      "Loss: 0.4849754571914673\n",
      "Loss: 0.4262237250804901\n",
      "Loss: 0.4175715744495392\n",
      "Loss: 0.36343488097190857\n",
      "Loss: 0.6033088564872742\n",
      "Loss: 0.6853214502334595\n",
      "Loss: 0.6239199638366699\n",
      "Loss: 0.3933919072151184\n",
      "Loss: 0.2755245864391327\n",
      "Loss: 0.4803830087184906\n",
      "Loss: 0.3743283450603485\n",
      "Loss: 0.4452108144760132\n",
      "Loss: 0.4996219277381897\n",
      "Loss: 0.5498732924461365\n",
      "Loss: 0.5256993770599365\n",
      "Loss: 0.4069792926311493\n",
      "Loss: 0.5765613913536072\n",
      "Loss: 0.36130088567733765\n",
      "Loss: 0.4936230480670929\n",
      "Loss: 0.6213726997375488\n",
      "Loss: 0.5142908692359924\n",
      "Loss: 0.6712819933891296\n",
      "Loss: 0.3642176687717438\n",
      "Loss: 0.5157058238983154\n",
      "Loss: 0.3666820526123047\n",
      "Loss: 0.43993499875068665\n",
      "Loss: 0.42840179800987244\n",
      "Loss: 0.51747065782547\n",
      "Loss: 0.2857414782047272\n",
      "Loss: 0.4117826521396637\n",
      "Loss: 0.3992210626602173\n",
      "Loss: 0.6473966836929321\n",
      "Loss: 0.3458259701728821\n",
      "Loss: 0.3499215245246887\n",
      "Loss: 0.3012178838253021\n",
      "Loss: 0.293722927570343\n",
      "Loss: 0.4627702236175537\n",
      "Loss: 0.3555641174316406\n",
      "Loss: 0.38779008388519287\n",
      "Loss: 0.19039921462535858\n",
      "Loss: 0.3846890330314636\n",
      "Loss: 0.40622252225875854\n",
      "Loss: 0.30134662985801697\n",
      "Loss: 0.5024200081825256\n",
      "Loss: 0.5311226844787598\n",
      "Loss: 0.6692376136779785\n",
      "Loss: 0.29277074337005615\n",
      "Loss: 0.2722848057746887\n",
      "Loss: 0.2279292643070221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5127851366996765\n",
      "Loss: 0.5655394196510315\n",
      "Loss: 0.34945568442344666\n",
      "Loss: 0.38680610060691833\n",
      "Loss: 0.3107341229915619\n",
      "Loss: 0.7074310183525085\n",
      "Loss: 0.3330828547477722\n",
      "Loss: 0.676852822303772\n",
      "Loss: 0.49547135829925537\n",
      "Loss: 0.6491640210151672\n",
      "Loss: 0.4973950982093811\n",
      "Loss: 0.4866896867752075\n",
      "Loss: 0.5007063150405884\n",
      "Loss: 0.3366341292858124\n",
      "Loss: 0.8163063526153564\n",
      "Loss: 0.664827287197113\n",
      "Loss: 0.38241255283355713\n",
      "Loss: 0.43316036462783813\n",
      "Loss: 0.8723129034042358\n",
      "Loss: 0.8584028482437134\n",
      "Loss: 0.639012336730957\n",
      "Loss: 0.7487651109695435\n",
      "Loss: 0.7680003643035889\n",
      "Loss: 0.779984712600708\n",
      "Loss: 0.4549174904823303\n",
      "Loss: 0.5873608589172363\n",
      "Loss: 0.28681111335754395\n",
      "Loss: 0.3197105824947357\n",
      "Loss: 0.3368431031703949\n",
      "Loss: 0.533292293548584\n",
      "Loss: 0.5929557085037231\n",
      "Loss: 0.4101201593875885\n",
      "Loss: 0.4817691743373871\n",
      "Loss: 0.25115570425987244\n",
      "Loss: 0.45354145765304565\n",
      "Loss: 0.5120304226875305\n",
      "Loss: 0.3162699341773987\n",
      "Loss: 0.42274218797683716\n",
      "Loss: 0.5852655172348022\n",
      "Loss: 0.5165359973907471\n",
      "Loss: 0.3206394612789154\n",
      "Loss: 0.5534145832061768\n",
      "Loss: 0.6419504284858704\n",
      "Loss: 0.47534045577049255\n",
      "Loss: 0.4604980945587158\n",
      "Loss: 0.5247328281402588\n",
      "Loss: 0.5915282964706421\n",
      "Loss: 0.5447443127632141\n",
      "Loss: 0.6337142586708069\n",
      "Loss: 0.6616935133934021\n",
      "Loss: 0.36085402965545654\n",
      "Loss: 0.46135643124580383\n",
      "Loss: 0.49538353085517883\n",
      "Loss: 0.33782854676246643\n",
      "Loss: 0.4512167274951935\n",
      "Loss: 0.5049334168434143\n",
      "Loss: 0.3554404377937317\n",
      "Loss: 0.29196086525917053\n",
      "Loss: 0.6224295496940613\n",
      "Loss: 0.41949915885925293\n",
      "Loss: 0.3771205544471741\n",
      "Loss: 0.42212650179862976\n",
      "Loss: 0.38212573528289795\n",
      "Loss: 0.4919183850288391\n",
      "Loss: 0.4852111041545868\n",
      "Loss: 0.35674378275871277\n",
      "Loss: 0.38559815287590027\n",
      "Loss: 0.5891015529632568\n",
      "Loss: 0.43118804693222046\n",
      "Loss: 0.4804955720901489\n",
      "Loss: 0.7766368985176086\n",
      "Loss: 0.41090676188468933\n",
      "Loss: 0.5626564025878906\n",
      "Loss: 0.4092235267162323\n",
      "Loss: 0.3006955683231354\n",
      "Loss: 0.5894263386726379\n",
      "Loss: 0.5752329230308533\n",
      "Loss: 0.46727830171585083\n",
      "Loss: 0.6244351863861084\n",
      "Loss: 0.3917973041534424\n",
      "Loss: 0.4301028549671173\n",
      "Loss: 0.6796953082084656\n",
      "Loss: 0.25053903460502625\n",
      "Loss: 0.2772796154022217\n",
      "Loss: 0.5091433525085449\n",
      "Loss: 0.36742374300956726\n",
      "Loss: 0.5138311386108398\n",
      "Loss: 0.7410765886306763\n",
      "Loss: 0.7004266977310181\n",
      "Loss: 0.40223896503448486\n",
      "Loss: 0.3367856740951538\n",
      "Loss: 0.6097464561462402\n",
      "Loss: 0.4221581220626831\n",
      "Loss: 0.33417820930480957\n",
      "Loss: 0.3148976266384125\n",
      "Loss: 0.6042286157608032\n",
      "Loss: 0.6066534519195557\n",
      "Loss: 0.28491854667663574\n",
      "Loss: 0.5514196157455444\n",
      "Loss: 0.46771249175071716\n",
      "Loss: 0.5538370013237\n",
      "Loss: 0.48993366956710815\n",
      "Loss: 0.2602291405200958\n",
      "Loss: 0.5345669388771057\n",
      "Loss: 0.5451562404632568\n",
      "Loss: 0.390532523393631\n",
      "Loss: 0.6315757036209106\n",
      "Loss: 0.4599429965019226\n",
      "Loss: 0.3483264446258545\n",
      "Loss: 0.44473227858543396\n",
      "Loss: 0.33266207575798035\n",
      "Loss: 0.4077163338661194\n",
      "Loss: 0.4727291166782379\n",
      "Loss: 0.3664938807487488\n",
      "Loss: 0.5129612684249878\n",
      "Loss: 0.27089327573776245\n",
      "Loss: 0.36887723207473755\n",
      "Loss: 0.5765495896339417\n",
      "Loss: 0.3422292470932007\n",
      "Loss: 0.5155162215232849\n",
      "Loss: 0.5142764449119568\n",
      "Loss: 0.40369048714637756\n",
      "Loss: 0.4797927737236023\n",
      "Loss: 0.3323989510536194\n",
      "Loss: 0.3206058442592621\n",
      "Loss: 0.42555567622184753\n",
      "Loss: 0.4186936616897583\n",
      "Loss: 0.5940144658088684\n",
      "Loss: 0.3479961156845093\n",
      "Loss: 0.45766255259513855\n",
      "Loss: 0.5772507786750793\n",
      "Loss: 0.5475040078163147\n",
      "Loss: 0.24843564629554749\n",
      "Loss: 0.6562725305557251\n",
      "Loss: 0.563555121421814\n",
      "Loss: 0.35747063159942627\n",
      "Loss: 0.4235590100288391\n",
      "Loss: 0.40465328097343445\n",
      "Loss: 0.3204059302806854\n",
      "Loss: 0.48152831196784973\n",
      "Loss: 0.4527026414871216\n",
      "Loss: 0.5519655346870422\n",
      "Loss: 0.624286413192749\n",
      "Loss: 0.3011092245578766\n",
      "Loss: 0.5267050266265869\n",
      "Loss: 0.6247853636741638\n",
      "Loss: 0.49066784977912903\n",
      "Loss: 0.8511314988136292\n",
      "Loss: 0.6174619793891907\n",
      "Loss: 0.3294063210487366\n",
      "Loss: 0.27064287662506104\n",
      "Loss: 0.2617306709289551\n",
      "Loss: 0.5097780823707581\n",
      "Loss: 0.31029069423675537\n",
      "Loss: 0.31919944286346436\n",
      "Loss: 0.3240244686603546\n",
      "Loss: 0.41227665543556213\n",
      "Loss: 0.2692432105541229\n",
      "Loss: 0.4175680875778198\n",
      "Loss: 0.3521176278591156\n",
      "Loss: 0.4349735975265503\n",
      "Loss: 0.41332173347473145\n",
      "Loss: 0.5656170845031738\n",
      "Loss: 0.40436074137687683\n",
      "Loss: 0.6273259520530701\n",
      "Loss: 0.5046689510345459\n",
      "Loss: 0.6561998724937439\n",
      "Loss: 0.3590623438358307\n",
      "Loss: 0.48644179105758667\n",
      "Loss: 0.4392489194869995\n",
      "Loss: 0.4171540439128876\n",
      "Loss: 0.4496398866176605\n",
      "Loss: 0.3343767821788788\n",
      "Loss: 0.6074291467666626\n",
      "Loss: 0.4407690763473511\n",
      "Loss: 0.30286335945129395\n",
      "Loss: 0.8047800660133362\n",
      "Loss: 0.47833651304244995\n",
      "Loss: 0.3231460154056549\n",
      "Loss: 0.43450236320495605\n",
      "Loss: 0.46201837062835693\n",
      "Loss: 0.5528594851493835\n",
      "Loss: 0.22315986454486847\n",
      "Loss: 0.4840676188468933\n",
      "Loss: 0.7568973302841187\n",
      "Loss: 0.47001010179519653\n",
      "Loss: 0.40325015783309937\n",
      "Loss: 0.7828872799873352\n",
      "Loss: 0.786093533039093\n",
      "Loss: 0.34034788608551025\n",
      "Loss: 0.6181748509407043\n",
      "Loss: 0.39463549852371216\n",
      "Loss: 0.35728490352630615\n",
      "Loss: 0.4416646957397461\n",
      "Loss: 0.4110030233860016\n",
      "Loss: 0.31634634733200073\n",
      "Loss: 0.28162992000579834\n",
      "Loss: 0.5872890949249268\n",
      "Loss: 0.5958981513977051\n",
      "Loss: 0.4230750799179077\n",
      "Loss: 0.43159958720207214\n",
      "Loss: 0.38264521956443787\n",
      "Loss: 0.4128115475177765\n",
      "Loss: 0.17790551483631134\n",
      "Loss: 0.36861762404441833\n",
      "Loss: 0.35932457447052\n",
      "Loss: 0.7262014746665955\n",
      "Loss: 0.6555560827255249\n",
      "Loss: 0.5893847346305847\n",
      "Loss: 0.8175399303436279\n",
      "Loss: 0.4939069449901581\n",
      "Loss: 0.41707366704940796\n",
      "Loss: 0.3979678452014923\n",
      "Loss: 0.46222835779190063\n",
      "Loss: 0.6369987726211548\n",
      "Loss: 0.6383286118507385\n",
      "Loss: 0.3133610188961029\n",
      "Loss: 0.3389568030834198\n",
      "Loss: 0.5108773708343506\n",
      "Loss: 0.4366106688976288\n",
      "Loss: 0.38590967655181885\n",
      "Loss: 0.46587231755256653\n",
      "Loss: 0.6324616074562073\n",
      "Loss: 0.2886861562728882\n",
      "Loss: 0.24748088419437408\n",
      "Loss: 0.24724259972572327\n",
      "Loss: 0.4162887930870056\n",
      "Loss: 0.5034471154212952\n",
      "Loss: 0.503518283367157\n",
      "Loss: 0.5020281076431274\n",
      "Loss: 0.3540687561035156\n",
      "Loss: 0.36178871989250183\n",
      "Loss: 0.29333630204200745\n",
      "Loss: 0.3986876308917999\n",
      "Loss: 0.3953399956226349\n",
      "Loss: 0.3443760573863983\n",
      "Loss: 0.4521641135215759\n",
      "Loss: 0.4735855758190155\n",
      "Loss: 0.32675567269325256\n",
      "Loss: 0.35927045345306396\n",
      "Loss: 0.39387673139572144\n",
      "Loss: 0.2506846487522125\n",
      "Loss: 0.6316465735435486\n",
      "Loss: 0.4687082767486572\n",
      "Loss: 0.5147429704666138\n",
      "Loss: 0.37622061371803284\n",
      "Loss: 0.634743332862854\n",
      "Loss: 0.30583998560905457\n",
      "Loss: 0.562686026096344\n",
      "Loss: 0.48055610060691833\n",
      "Loss: 0.5809935927391052\n",
      "Loss: 0.37032073736190796\n",
      "Loss: 0.53501296043396\n",
      "Loss: 0.5394855737686157\n",
      "Loss: 0.2410304993391037\n",
      "Loss: 0.4839496910572052\n",
      "Loss: 0.9208887219429016\n",
      "Loss: 0.5016876459121704\n",
      "Loss: 0.3468455970287323\n",
      "Loss: 0.5018131136894226\n",
      "Loss: 0.33203375339508057\n",
      "Loss: 0.44290706515312195\n",
      "Loss: 0.5873661637306213\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7761785984039307\n",
      "Loss: 2.6229982376098633\n",
      "Loss: 2.3505163192749023\n",
      "Loss: 2.4562995433807373\n",
      "Loss: 2.2963004112243652\n",
      "Loss: 2.2877907752990723\n",
      "Loss: 2.326465368270874\n",
      "Loss: 2.2519524097442627\n",
      "Loss: 2.257910966873169\n",
      "Loss: 2.294045925140381\n",
      "Loss: 2.233964204788208\n",
      "Loss: 2.2303404808044434\n",
      "Loss: 2.2559640407562256\n",
      "Loss: 2.2133121490478516\n",
      "Loss: 2.2026357650756836\n",
      "Loss: 2.190990686416626\n",
      "Loss: 2.220290422439575\n",
      "Loss: 2.1194369792938232\n",
      "Loss: 2.217381238937378\n",
      "Loss: 2.2835404872894287\n",
      "Loss: 2.217271566390991\n",
      "Loss: 2.1934566497802734\n",
      "Loss: 2.1742794513702393\n",
      "Loss: 2.1087615489959717\n",
      "Loss: 2.1302084922790527\n",
      "Loss: 2.1062982082366943\n",
      "Loss: 2.1814534664154053\n",
      "Loss: 2.2340080738067627\n",
      "Loss: 2.0547263622283936\n",
      "Loss: 2.057990074157715\n",
      "Loss: 2.0572774410247803\n",
      "Loss: 1.9716852903366089\n",
      "Loss: 2.0525991916656494\n",
      "Loss: 2.092095136642456\n",
      "Loss: 2.1406333446502686\n",
      "Loss: 1.951910376548767\n",
      "Loss: 1.848122000694275\n",
      "Loss: 2.0032799243927\n",
      "Loss: 2.099607229232788\n",
      "Loss: 2.067258358001709\n",
      "Loss: 1.9320701360702515\n",
      "Loss: 1.9726645946502686\n",
      "Loss: 1.9771926403045654\n",
      "Loss: 1.8770033121109009\n",
      "Loss: 1.8907755613327026\n",
      "Loss: 1.9728175401687622\n",
      "Loss: 2.000509262084961\n",
      "Loss: 1.7934232950210571\n",
      "Loss: 1.9496251344680786\n",
      "Loss: 1.9357458353042603\n",
      "Loss: 1.9000872373580933\n",
      "Loss: 1.8720113039016724\n",
      "Loss: 1.960324764251709\n",
      "Loss: 1.9707434177398682\n",
      "Loss: 1.7768954038619995\n",
      "Loss: 2.065124750137329\n",
      "Loss: 1.8660792112350464\n",
      "Loss: 1.915037989616394\n",
      "Loss: 1.6766194105148315\n",
      "Loss: 1.8622937202453613\n",
      "Loss: 1.881842851638794\n",
      "Loss: 1.9258832931518555\n",
      "Loss: 1.802130937576294\n",
      "Loss: 1.8607308864593506\n",
      "Loss: 1.9005357027053833\n",
      "Loss: 1.8307286500930786\n",
      "Loss: 1.7792598009109497\n",
      "Loss: 1.7618829011917114\n",
      "Loss: 1.736527442932129\n",
      "Loss: 1.816770076751709\n",
      "Loss: 1.8389784097671509\n",
      "Loss: 1.7220661640167236\n",
      "Loss: 1.8376452922821045\n",
      "Loss: 1.707107663154602\n",
      "Loss: 1.663920283317566\n",
      "Loss: 1.6257520914077759\n",
      "Loss: 1.7988008260726929\n",
      "Loss: 1.8563679456710815\n",
      "Loss: 1.7532262802124023\n",
      "Loss: 1.543245553970337\n",
      "Loss: 1.8294804096221924\n",
      "Loss: 1.7639998197555542\n",
      "Loss: 1.6721739768981934\n",
      "Loss: 1.8296372890472412\n",
      "Loss: 1.7845969200134277\n",
      "Loss: 1.8127943277359009\n",
      "Loss: 1.8519117832183838\n",
      "Loss: 1.8449656963348389\n",
      "Loss: 1.8906545639038086\n",
      "Loss: 1.6486183404922485\n",
      "Loss: 1.809994101524353\n",
      "Loss: 1.7419756650924683\n",
      "Loss: 1.8006377220153809\n",
      "Loss: 1.7438676357269287\n",
      "Loss: 1.5806028842926025\n",
      "Loss: 1.583214521408081\n",
      "Loss: 1.7524930238723755\n",
      "Loss: 1.7737038135528564\n",
      "Loss: 1.6557321548461914\n",
      "Loss: 1.7771484851837158\n",
      "Loss: 1.8024941682815552\n",
      "Loss: 1.6213306188583374\n",
      "Loss: 1.4715304374694824\n",
      "Loss: 1.5456717014312744\n",
      "Loss: 1.4665478467941284\n",
      "Loss: 1.6817634105682373\n",
      "Loss: 1.5167316198349\n",
      "Loss: 1.6538474559783936\n",
      "Loss: 1.7058523893356323\n",
      "Loss: 1.4588689804077148\n",
      "Loss: 1.7701730728149414\n",
      "Loss: 1.6554874181747437\n",
      "Loss: 1.6338406801223755\n",
      "Loss: 1.5570696592330933\n",
      "Loss: 1.8326280117034912\n",
      "Loss: 1.587528109550476\n",
      "Loss: 1.6124500036239624\n",
      "Loss: 1.6094468832015991\n",
      "Loss: 1.5516315698623657\n",
      "Loss: 1.6840689182281494\n",
      "Loss: 1.679428219795227\n",
      "Loss: 1.7079211473464966\n",
      "Loss: 1.5571653842926025\n",
      "Loss: 1.6354900598526\n",
      "Loss: 1.7003772258758545\n",
      "Loss: 1.512791633605957\n",
      "Loss: 1.6826539039611816\n",
      "Loss: 1.8110897541046143\n",
      "Loss: 1.7202889919281006\n",
      "Loss: 1.5797975063323975\n",
      "Loss: 1.723537564277649\n",
      "Loss: 1.636834740638733\n",
      "Loss: 1.6808491945266724\n",
      "Loss: 1.7392839193344116\n",
      "Loss: 1.4444971084594727\n",
      "Loss: 1.7004094123840332\n",
      "Loss: 1.6908364295959473\n",
      "Loss: 1.572078824043274\n",
      "Loss: 1.626466989517212\n",
      "Loss: 1.6525421142578125\n",
      "Loss: 1.6239421367645264\n",
      "Loss: 1.462048053741455\n",
      "Loss: 1.6433939933776855\n",
      "Loss: 1.8903087377548218\n",
      "Loss: 1.5401743650436401\n",
      "Loss: 1.6263736486434937\n",
      "Loss: 1.528706669807434\n",
      "Loss: 1.6734346151351929\n",
      "Loss: 1.5962586402893066\n",
      "Loss: 1.595068097114563\n",
      "Loss: 1.52561616897583\n",
      "Loss: 1.819730520248413\n",
      "Loss: 1.6733849048614502\n",
      "Loss: 1.817211627960205\n",
      "Loss: 1.4844944477081299\n",
      "Loss: 1.751998782157898\n",
      "Loss: 1.6968834400177002\n",
      "Loss: 1.518936038017273\n",
      "Loss: 1.5036094188690186\n",
      "Loss: 1.4877314567565918\n",
      "Loss: 1.8428317308425903\n",
      "Loss: 1.6342304944992065\n",
      "Loss: 1.594912052154541\n",
      "Loss: 1.5100491046905518\n",
      "Loss: 1.6239559650421143\n",
      "Loss: 1.5958971977233887\n",
      "Loss: 1.7587298154830933\n",
      "Loss: 1.6967289447784424\n",
      "Loss: 1.6746734380722046\n",
      "Loss: 1.7997050285339355\n",
      "Loss: 1.5136351585388184\n",
      "Loss: 1.7643330097198486\n",
      "Loss: 1.8106151819229126\n",
      "Loss: 1.6323562860488892\n",
      "Loss: 1.5722110271453857\n",
      "Loss: 1.5515551567077637\n",
      "Loss: 1.416326880455017\n",
      "Loss: 1.7000635862350464\n",
      "Loss: 1.4369713068008423\n",
      "Loss: 1.6291558742523193\n",
      "Loss: 1.541536569595337\n",
      "Loss: 1.607566475868225\n",
      "Loss: 1.584220290184021\n",
      "Loss: 1.3659470081329346\n",
      "Loss: 1.6521391868591309\n",
      "Loss: 1.6615324020385742\n",
      "Loss: 1.4404128789901733\n",
      "Loss: 1.5067371129989624\n",
      "Loss: 1.2626413106918335\n",
      "Loss: 1.5025233030319214\n",
      "Loss: 1.6143734455108643\n",
      "Loss: 1.5208206176757812\n",
      "Loss: 1.735862135887146\n",
      "Loss: 1.576749563217163\n",
      "Loss: 1.3646061420440674\n",
      "Loss: 1.5571105480194092\n",
      "Loss: 1.5071966648101807\n",
      "Loss: 1.7666889429092407\n",
      "Loss: 1.507231593132019\n",
      "Loss: 1.6360491514205933\n",
      "Loss: 1.505568504333496\n",
      "Loss: 1.383665919303894\n",
      "Loss: 1.736325740814209\n",
      "Loss: 1.4471265077590942\n",
      "Loss: 1.593106985092163\n",
      "Loss: 1.4998126029968262\n",
      "Loss: 1.2213186025619507\n",
      "Loss: 1.2973114252090454\n",
      "Loss: 1.5741902589797974\n",
      "Loss: 1.6257116794586182\n",
      "Loss: 1.4233134984970093\n",
      "Loss: 1.399164080619812\n",
      "Loss: 1.462852120399475\n",
      "Loss: 1.6052166223526\n",
      "Loss: 1.4667857885360718\n",
      "Loss: 1.5968642234802246\n",
      "Loss: 1.7493380308151245\n",
      "Loss: 1.5773197412490845\n",
      "Loss: 1.5953483581542969\n",
      "Loss: 1.6044787168502808\n",
      "Loss: 1.5084611177444458\n",
      "Loss: 1.5916444063186646\n",
      "Loss: 1.7250856161117554\n",
      "Loss: 1.5554431676864624\n",
      "Loss: 1.4115393161773682\n",
      "Loss: 1.4591134786605835\n",
      "Loss: 1.8305299282073975\n",
      "Loss: 1.395514726638794\n",
      "Loss: 1.4815808534622192\n",
      "Loss: 1.538163661956787\n",
      "Loss: 1.5059020519256592\n",
      "Loss: 1.5055125951766968\n",
      "Loss: 1.4645609855651855\n",
      "Loss: 1.8675035238265991\n",
      "Loss: 1.5466854572296143\n",
      "Loss: 1.5062354803085327\n",
      "Loss: 1.5456379652023315\n",
      "Loss: 1.553948163986206\n",
      "Loss: 1.5146870613098145\n",
      "Loss: 1.7647161483764648\n",
      "Loss: 1.5082603693008423\n",
      "Loss: 1.4551100730895996\n",
      "Loss: 1.5703791379928589\n",
      "Loss: 1.4716547727584839\n",
      "Loss: 1.2912379503250122\n",
      "Loss: 1.6472060680389404\n",
      "Loss: 1.600956678390503\n",
      "Loss: 1.5671576261520386\n",
      "Loss: 1.5071845054626465\n",
      "Loss: 1.5075640678405762\n",
      "Loss: 1.5636727809906006\n",
      "Loss: 1.3572016954421997\n",
      "Loss: 1.5184375047683716\n",
      "Loss: 1.4689732789993286\n",
      "Loss: 1.5819975137710571\n",
      "Loss: 1.4406338930130005\n",
      "Loss: 1.8974151611328125\n",
      "Loss: 1.719369649887085\n",
      "Loss: 1.6633858680725098\n",
      "Loss: 1.4323545694351196\n",
      "Loss: 1.3943198919296265\n",
      "Loss: 1.4427450895309448\n",
      "Loss: 1.615565299987793\n",
      "Loss: 1.714080810546875\n",
      "Loss: 1.5154027938842773\n",
      "Loss: 1.5838967561721802\n",
      "Loss: 1.4493129253387451\n",
      "Loss: 1.5035319328308105\n",
      "Loss: 1.820945143699646\n",
      "Loss: 1.4723750352859497\n",
      "Loss: 1.5922569036483765\n",
      "Loss: 1.6271014213562012\n",
      "Loss: 1.4847397804260254\n",
      "Loss: 1.6104986667633057\n",
      "Loss: 1.5725834369659424\n",
      "Loss: 1.4891072511672974\n",
      "Loss: 1.4101393222808838\n",
      "Loss: 1.5976330041885376\n",
      "Loss: 1.5365347862243652\n",
      "Loss: 1.5974735021591187\n",
      "Loss: 1.4917323589324951\n",
      "Loss: 1.5239332914352417\n",
      "Loss: 1.572494387626648\n",
      "Loss: 1.593377709388733\n",
      "Loss: 1.5873990058898926\n",
      "Loss: 1.6570435762405396\n",
      "Loss: 1.4243273735046387\n",
      "Loss: 1.5628372430801392\n",
      "Loss: 1.4515984058380127\n",
      "Loss: 1.7201132774353027\n",
      "Loss: 1.8243991136550903\n",
      "Loss: 1.537463665008545\n",
      "Loss: 1.415650486946106\n",
      "Loss: 1.5504472255706787\n",
      "Loss: 1.52260160446167\n",
      "Loss: 1.3236714601516724\n",
      "Loss: 1.5097945928573608\n",
      "Loss: 1.8192799091339111\n",
      "Loss: 1.772128701210022\n",
      "Loss: 1.408054232597351\n",
      "Loss: 1.6835083961486816\n",
      "Loss: 1.3749375343322754\n",
      "Loss: 1.66214919090271\n",
      "Loss: 1.5398566722869873\n",
      "Loss: 1.4899840354919434\n",
      "Loss: 1.9051909446716309\n",
      "Loss: 1.5038502216339111\n",
      "Loss: 1.4286137819290161\n",
      "Loss: 1.4966223239898682\n",
      "Loss: 1.3705494403839111\n",
      "Loss: 1.329818606376648\n",
      "Loss: 1.5943913459777832\n",
      "Loss: 1.6672313213348389\n",
      "Loss: 1.4939039945602417\n",
      "Loss: 1.4371376037597656\n",
      "Loss: 1.7317304611206055\n",
      "Loss: 1.6196781396865845\n",
      "Loss: 1.6257492303848267\n",
      "Loss: 1.4937913417816162\n",
      "Loss: 1.5238195657730103\n",
      "Loss: 1.597273826599121\n",
      "Loss: 1.5098016262054443\n",
      "Loss: 1.5811847448349\n",
      "Loss: 1.5827651023864746\n",
      "Loss: 1.4454160928726196\n",
      "Loss: 1.5823006629943848\n",
      "Loss: 1.386690616607666\n",
      "Loss: 1.7494556903839111\n",
      "Loss: 1.490159034729004\n",
      "Loss: 1.2917611598968506\n",
      "Loss: 1.586527705192566\n",
      "Loss: 1.5947983264923096\n",
      "Loss: 1.5692976713180542\n",
      "Loss: 1.5671240091323853\n",
      "Loss: 1.5097029209136963\n",
      "Loss: 1.4475985765457153\n",
      "Loss: 1.5411763191223145\n",
      "Loss: 1.598391056060791\n",
      "Loss: 1.5139411687850952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5343044996261597\n",
      "Loss: 1.470085859298706\n",
      "Loss: 1.7409405708312988\n",
      "Loss: 1.3208531141281128\n",
      "Loss: 1.6532695293426514\n",
      "Loss: 1.701149344444275\n",
      "Loss: 1.5775469541549683\n",
      "Loss: 1.6016736030578613\n",
      "Loss: 1.3746775388717651\n",
      "Loss: 1.5244183540344238\n",
      "Loss: 1.4841586351394653\n",
      "Loss: 1.537546992301941\n",
      "Loss: 1.5829201936721802\n",
      "Loss: 1.5313972234725952\n",
      "Loss: 1.5151599645614624\n",
      "Loss: 1.5521986484527588\n",
      "Loss: 1.5350052118301392\n",
      "Loss: 1.6204383373260498\n",
      "Loss: 1.681959867477417\n",
      "Loss: 1.3975515365600586\n",
      "Loss: 1.558091402053833\n",
      "Loss: 1.5642389059066772\n",
      "Loss: 1.472490668296814\n",
      "Loss: 1.7206252813339233\n",
      "Loss: 1.522516131401062\n",
      "Loss: 1.4833563566207886\n",
      "Loss: 1.6760822534561157\n",
      "Loss: 1.4523215293884277\n",
      "Loss: 1.5783722400665283\n",
      "Loss: 1.555077075958252\n",
      "Loss: 1.6597400903701782\n",
      "Loss: 1.6604149341583252\n",
      "Loss: 1.4987215995788574\n",
      "Loss: 1.5114774703979492\n",
      "Loss: 1.4944146871566772\n",
      "Loss: 1.5135207176208496\n",
      "Loss: 1.6269962787628174\n",
      "Loss: 1.5217090845108032\n",
      "Loss: 1.389207363128662\n",
      "Loss: 1.621984601020813\n",
      "Loss: 1.5753686428070068\n",
      "Loss: 1.5409507751464844\n",
      "Loss: 1.5931193828582764\n",
      "Loss: 1.6199302673339844\n",
      "Loss: 1.629512071609497\n",
      "Loss: 1.827525019645691\n",
      "Loss: 1.6532903909683228\n",
      "Loss: 1.3504116535186768\n",
      "Loss: 1.5963494777679443\n",
      "Loss: 1.5406008958816528\n",
      "Loss: 1.638831377029419\n",
      "Loss: 1.528141736984253\n",
      "Loss: 1.7446500062942505\n",
      "Loss: 1.510393500328064\n",
      "Loss: 1.4084275960922241\n",
      "Loss: 1.6090604066848755\n",
      "Loss: 1.5933597087860107\n",
      "Loss: 1.6655994653701782\n",
      "Loss: 1.5397627353668213\n",
      "Loss: 1.458688497543335\n",
      "Loss: 1.5335770845413208\n",
      "Loss: 1.5929964780807495\n",
      "Loss: 1.705870509147644\n",
      "Loss: 1.5552974939346313\n",
      "Loss: 1.5136550664901733\n",
      "Loss: 1.5646765232086182\n",
      "Loss: 1.392611026763916\n",
      "Loss: 1.5995692014694214\n",
      "Loss: 1.4531805515289307\n",
      "Loss: 1.6103997230529785\n",
      "Loss: 1.6716846227645874\n",
      "Loss: 1.549573540687561\n",
      "Loss: 1.4017820358276367\n",
      "Loss: 1.6390647888183594\n",
      "Loss: 1.5101181268692017\n",
      "Loss: 1.5625238418579102\n",
      "Loss: 1.4571006298065186\n",
      "Loss: 1.574476957321167\n",
      "Loss: 1.424648404121399\n",
      "Loss: 1.5549055337905884\n",
      "Loss: 1.4935333728790283\n",
      "Loss: 1.6002296209335327\n",
      "Loss: 1.5620155334472656\n",
      "Loss: 1.6560494899749756\n",
      "Loss: 1.5104341506958008\n",
      "Loss: 1.5055127143859863\n",
      "Loss: 1.7962449789047241\n",
      "Loss: 1.4280176162719727\n",
      "Loss: 1.4739768505096436\n",
      "Loss: 1.3707084655761719\n",
      "Loss: 2.106766700744629\n",
      "Loss: 1.5505783557891846\n",
      "Loss: 1.3097163438796997\n",
      "Loss: 1.777113914489746\n",
      "Loss: 1.3507236242294312\n",
      "Loss: 1.7479238510131836\n",
      "Loss: 1.7770371437072754\n",
      "Loss: 1.390498399734497\n",
      "Loss: 1.6129238605499268\n",
      "Loss: 1.6359107494354248\n",
      "Loss: 1.4551031589508057\n",
      "Loss: 1.5794916152954102\n",
      "Loss: 1.7027055025100708\n",
      "Loss: 1.384650707244873\n",
      "Loss: 1.6227943897247314\n",
      "Loss: 1.5602810382843018\n",
      "Loss: 1.638441562652588\n",
      "Loss: 1.2709004878997803\n",
      "Loss: 1.4861770868301392\n",
      "Loss: 1.663499355316162\n",
      "Loss: 1.4804283380508423\n",
      "Loss: 1.6152435541152954\n",
      "Loss: 1.5918046236038208\n",
      "Loss: 1.4579856395721436\n",
      "Loss: 1.5907222032546997\n",
      "Loss: 1.621343731880188\n",
      "Loss: 1.5673729181289673\n",
      "Loss: 1.4909920692443848\n",
      "Loss: 1.528672218322754\n",
      "Loss: 1.4115391969680786\n",
      "Loss: 1.4243489503860474\n",
      "Loss: 1.6038563251495361\n",
      "Loss: 1.6333248615264893\n",
      "Loss: 1.6235246658325195\n",
      "Loss: 1.6829451322555542\n",
      "Loss: 1.5916616916656494\n",
      "Loss: 1.4439927339553833\n",
      "Loss: 1.6825995445251465\n",
      "Loss: 1.5691903829574585\n",
      "Loss: 1.4316097497940063\n",
      "Loss: 1.7475290298461914\n",
      "Loss: 1.6994175910949707\n",
      "Loss: 1.5749497413635254\n",
      "Loss: 1.659165859222412\n",
      "Loss: 1.6758341789245605\n",
      "Loss: 1.6737756729125977\n",
      "Loss: 1.5939319133758545\n",
      "Loss: 1.6920057535171509\n",
      "Loss: 1.597353219985962\n",
      "Loss: 1.4669060707092285\n",
      "Loss: 1.5401506423950195\n",
      "Loss: 1.5392425060272217\n",
      "Loss: 1.3521902561187744\n",
      "Loss: 1.6511791944503784\n",
      "Loss: 1.64912748336792\n",
      "Loss: 1.5819220542907715\n",
      "Loss: 1.5033568143844604\n",
      "Loss: 1.6022647619247437\n",
      "Loss: 1.4391857385635376\n",
      "Loss: 1.6254489421844482\n",
      "Loss: 1.596197247505188\n",
      "Loss: 1.5687733888626099\n",
      "Loss: 1.5790891647338867\n",
      "Loss: 1.5681740045547485\n",
      "Loss: 1.590164303779602\n",
      "Loss: 1.3778759241104126\n",
      "Loss: 1.3419548273086548\n",
      "Loss: 1.647099256515503\n",
      "Loss: 1.4481297731399536\n",
      "Loss: 1.4846818447113037\n",
      "Loss: 1.5988149642944336\n",
      "Loss: 1.5195213556289673\n",
      "Loss: 1.633169412612915\n",
      "Loss: 1.6118196249008179\n",
      "Loss: 1.428260326385498\n",
      "Loss: 1.4640355110168457\n",
      "Loss: 1.4841848611831665\n",
      "Loss: 1.9670422077178955\n",
      "Loss: 1.4622858762741089\n",
      "Loss: 1.6675320863723755\n",
      "Loss: 1.4284231662750244\n",
      "Loss: 1.4797112941741943\n",
      "Loss: 1.4810982942581177\n",
      "Loss: 1.4590855836868286\n",
      "Loss: 1.4949618577957153\n",
      "Loss: 1.3983267545700073\n",
      "Loss: 1.6099759340286255\n",
      "Loss: 1.6294236183166504\n",
      "Loss: 1.4829295873641968\n",
      "Loss: 1.5615054368972778\n",
      "Loss: 1.537269115447998\n",
      "Loss: 1.400254249572754\n",
      "Loss: 1.5005472898483276\n",
      "Loss: 1.7669275999069214\n",
      "Loss: 1.38670015335083\n",
      "Loss: 1.4989023208618164\n",
      "Loss: 1.8539766073226929\n",
      "Loss: 1.599676251411438\n",
      "Loss: 1.6344985961914062\n",
      "Loss: 1.7219427824020386\n",
      "Loss: 1.515519618988037\n",
      "Loss: 1.5778532028198242\n",
      "Loss: 1.7104159593582153\n",
      "Loss: 1.6265486478805542\n",
      "Loss: 1.5569829940795898\n",
      "Loss: 1.3978215456008911\n",
      "Loss: 1.5287452936172485\n",
      "Loss: 1.5393568277359009\n",
      "Loss: 1.6861801147460938\n",
      "Loss: 1.5201188325881958\n",
      "Loss: 1.5393409729003906\n",
      "Loss: 1.460595965385437\n",
      "Loss: 1.4774037599563599\n",
      "Loss: 1.3996373414993286\n",
      "Loss: 1.6843987703323364\n",
      "Loss: 1.4568686485290527\n",
      "Loss: 1.6146607398986816\n",
      "Loss: 1.4948500394821167\n",
      "Loss: 1.5927820205688477\n",
      "Loss: 1.8413398265838623\n",
      "Loss: 1.5195305347442627\n",
      "Loss: 1.5434958934783936\n",
      "Loss: 1.437991738319397\n",
      "Loss: 1.5307523012161255\n",
      "Loss: 1.4471113681793213\n",
      "Loss: 1.3936467170715332\n",
      "Loss: 1.5155798196792603\n",
      "Loss: 1.5646121501922607\n",
      "Loss: 1.6177020072937012\n",
      "Loss: 1.3290575742721558\n",
      "Loss: 1.4489772319793701\n",
      "Loss: 1.3382295370101929\n",
      "Loss: 1.6492629051208496\n",
      "Loss: 1.4417616128921509\n",
      "Loss: 1.578925609588623\n",
      "Loss: 1.6021395921707153\n",
      "Loss: 1.573384404182434\n",
      "Loss: 1.614251971244812\n",
      "Loss: 1.5287230014801025\n",
      "Loss: 1.605767011642456\n",
      "Loss: 1.5922834873199463\n",
      "Loss: 1.5023020505905151\n",
      "Loss: 1.5897899866104126\n",
      "Loss: 1.6713165044784546\n",
      "Loss: 1.4963496923446655\n",
      "Loss: 1.5939724445343018\n",
      "Loss: 1.537688136100769\n",
      "Loss: 1.5991227626800537\n",
      "Loss: 1.5547136068344116\n",
      "Loss: 1.5811593532562256\n",
      "Loss: 1.7833503484725952\n",
      "Loss: 1.4555660486221313\n",
      "Loss: 1.591063380241394\n",
      "Loss: 1.5137027502059937\n",
      "Loss: 1.367160439491272\n",
      "Loss: 1.2959015369415283\n",
      "Loss: 1.5772099494934082\n",
      "Loss: 1.6178503036499023\n",
      "Loss: 1.6101313829421997\n",
      "Loss: 1.5224809646606445\n",
      "Loss: 1.5360145568847656\n",
      "Loss: 1.4580786228179932\n",
      "Loss: 1.8025439977645874\n",
      "Loss: 1.4006925821304321\n",
      "Loss: 1.5741125345230103\n",
      "Loss: 1.3538310527801514\n",
      "Loss: 1.4688515663146973\n",
      "Loss: 1.4943941831588745\n",
      "Loss: 1.4495532512664795\n",
      "Loss: 1.6634716987609863\n",
      "Loss: 1.4460095167160034\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.289235\n",
      "loss 3.2892\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1158])\n",
      "Loss: 3.820173\n",
      "loss 3.8202\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1728])\n",
      "Loss: 3.088272\n",
      "loss 3.0883\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2261])\n",
      "Loss: 3.668830\n",
      "loss 3.6688\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2717])\n",
      "Loss: 3.334632\n",
      "loss 3.3346\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3077])\n",
      "Loss: 3.254644\n",
      "loss 3.2546\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3325])\n",
      "Loss: 4.470075\n",
      "loss 4.4701\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3446])\n",
      "Loss: 3.857010\n",
      "loss 3.8570\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3458])\n",
      "Loss: 4.010983\n",
      "loss 4.0110\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3295])\n",
      "Loss: 3.515331\n",
      "loss 3.5153\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2943])\n",
      "Loss: 3.096645\n",
      "loss 3.0966\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2424])\n",
      "Loss: 3.232729\n",
      "loss 3.2327\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1754])\n",
      "Loss: 2.795955\n",
      "loss 2.7960\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0916])\n",
      "Loss: 2.906326\n",
      "loss 2.9063\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9936])\n",
      "Loss: 2.518763\n",
      "loss 2.5188\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8840])\n",
      "Loss: 2.892200\n",
      "loss 2.8922\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7669])\n",
      "Loss: 2.107924\n",
      "loss 2.1079\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6413])\n",
      "Loss: 2.683333\n",
      "loss 2.6833\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5133])\n",
      "Loss: 2.089403\n",
      "loss 2.0894\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3832])\n",
      "Loss: 2.052946\n",
      "loss 2.0529\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2507])\n",
      "Loss: 1.837733\n",
      "loss 1.8377\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1210])\n",
      "Loss: 1.966544\n",
      "loss 1.9665\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.9359])\n",
      "Loss: 2.000208\n",
      "loss 2.0002\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7113])\n",
      "Loss: 1.848544\n",
      "loss 1.8485\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4999])\n",
      "Loss: 1.743293\n",
      "loss 1.7433\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3104])\n",
      "Loss: 1.761223\n",
      "loss 1.7612\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1456])\n",
      "Loss: 1.926040\n",
      "loss 1.9260\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0201])\n",
      "Loss: 1.934242\n",
      "loss 1.9342\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9195])\n",
      "Loss: 1.842269\n",
      "loss 1.8423\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8332])\n",
      "Loss: 2.646360\n",
      "loss 2.6464\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.6999])\n",
      "Loss: 4.817539\n",
      "loss 4.8175\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.9168])\n",
      "Loss: 1.803463\n",
      "loss 1.8035\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.4699])\n",
      "Loss: 2.019906\n",
      "loss 2.0199\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.3936])\n",
      "Loss: 1.752431\n",
      "loss 1.7524\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2382])\n",
      "Loss: 2.391263\n",
      "loss 2.3913\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8582])\n",
      "Loss: 1.830263\n",
      "loss 1.8303\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5761])\n",
      "Loss: 2.295207\n",
      "loss 2.2952\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3088])\n",
      "Loss: 1.873163\n",
      "loss 1.8732\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1171])\n",
      "Loss: 1.764886\n",
      "loss 1.7649\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9366])\n",
      "Loss: 2.037889\n",
      "loss 2.0379\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7691])\n",
      "Loss: 1.911190\n",
      "loss 1.9112\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6049])\n",
      "Loss: 1.714860\n",
      "loss 1.7149\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4456])\n",
      "Loss: 1.883193\n",
      "loss 1.8832\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2953])\n",
      "Loss: 1.882673\n",
      "loss 1.8827\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1421])\n",
      "Loss: 2.347191\n",
      "loss 2.3472\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.9852])\n",
      "Loss: 2.223752\n",
      "loss 2.2238\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0858])\n",
      "Loss: 2.169684\n",
      "loss 2.1697\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1704])\n",
      "Loss: 2.014873\n",
      "loss 2.0149\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2495])\n",
      "Loss: 2.071670\n",
      "loss 2.0717\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3250])\n",
      "Loss: 3.137386\n",
      "loss 3.1374\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3973])\n",
      "Loss: 1.822952\n",
      "loss 1.8230\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4716])\n",
      "Loss: 2.361204\n",
      "loss 2.3612\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5425])\n",
      "Loss: 2.652445\n",
      "loss 2.6524\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6062])\n",
      "Loss: 2.431477\n",
      "loss 2.4315\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6596])\n",
      "Loss: 2.741093\n",
      "loss 2.7411\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7041])\n",
      "Loss: 2.805335\n",
      "loss 2.8053\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7386])\n",
      "Loss: 2.699534\n",
      "loss 2.6995\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7594])\n",
      "Loss: 3.040716\n",
      "loss 3.0407\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7657])\n",
      "Loss: 2.811969\n",
      "loss 2.8120\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7492])\n",
      "Loss: 2.767004\n",
      "loss 2.7670\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7142])\n",
      "Loss: 2.484226\n",
      "loss 2.4842\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6587])\n",
      "Loss: 2.000441\n",
      "loss 2.0004\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5833])\n",
      "Loss: 2.740520\n",
      "loss 2.7405\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4962])\n",
      "Loss: 2.350124\n",
      "loss 2.3501\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3901])\n",
      "Loss: 1.941742\n",
      "loss 1.9417\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2783])\n",
      "Loss: 1.962637\n",
      "loss 1.9626\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1606])\n",
      "Loss: 2.213636\n",
      "loss 2.2136\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0407])\n",
      "Loss: 1.962656\n",
      "loss 1.9627\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.2424])\n",
      "Loss: 1.946395\n",
      "loss 1.9464\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.0803])\n",
      "Loss: 1.978626\n",
      "loss 1.9786\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9190])\n",
      "Loss: 2.010835\n",
      "loss 2.0108\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7683])\n",
      "Loss: 1.849548\n",
      "loss 1.8495\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6988])\n",
      "Loss: 2.030262\n",
      "loss 2.0303\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6271])\n",
      "Loss: 1.723334\n",
      "loss 1.7233\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5559])\n",
      "Loss: 1.972499\n",
      "loss 1.9725\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4863])\n",
      "Loss: 1.991137\n",
      "loss 1.9911\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.3896])\n",
      "Loss: 3.628313\n",
      "loss 3.6283\n",
      "tensor(1.00000e-04 *\n",
      "       [ 6.0137])\n",
      "Loss: 2.042464\n",
      "loss 2.0425\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9178])\n",
      "Loss: 2.474898\n",
      "loss 2.4749\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9586])\n",
      "Loss: 2.010999\n",
      "loss 2.0110\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7622])\n",
      "Loss: 1.834726\n",
      "loss 1.8347\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3858])\n",
      "Loss: 2.165302\n",
      "loss 2.1653\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8842])\n",
      "Loss: 2.102261\n",
      "loss 2.1023\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0289])\n",
      "Loss: 2.327880\n",
      "loss 2.3279\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1619])\n",
      "Loss: 2.282524\n",
      "loss 2.2825\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2873])\n",
      "Loss: 2.669498\n",
      "loss 2.6695\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4066])\n",
      "Loss: 2.171051\n",
      "loss 2.1711\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5200])\n",
      "Loss: 2.433591\n",
      "loss 2.4336\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6299])\n",
      "Loss: 2.837921\n",
      "loss 2.8379\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7355])\n",
      "Loss: 3.249131\n",
      "loss 3.2491\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8362])\n",
      "Loss: 2.795559\n",
      "loss 2.7956\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9312])\n",
      "Loss: 3.056768\n",
      "loss 3.0568\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0222])\n",
      "Loss: 3.782925\n",
      "loss 3.7829\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1085])\n",
      "Loss: 4.085754\n",
      "loss 4.0858\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1885])\n",
      "Loss: 3.674487\n",
      "loss 3.6745\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2608])\n",
      "Loss: 3.852817\n",
      "loss 3.8528\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3270])\n",
      "Loss: 3.781129\n",
      "loss 3.7811\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3864])\n",
      "Loss: 4.077371\n",
      "loss 4.0774\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4380])\n",
      "Loss: 4.026168\n",
      "loss 4.0262\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4810])\n",
      "Loss: 4.186193\n",
      "loss 4.1862\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5137])\n",
      "Loss: 3.796576\n",
      "loss 3.7966\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5371])\n",
      "Loss: 4.455619\n",
      "loss 4.4556\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5518])\n",
      "Loss: 3.746738\n",
      "loss 3.7467\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5538])\n",
      "Loss: 3.801232\n",
      "loss 3.8012\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5436])\n",
      "Loss: 3.444519\n",
      "loss 3.4445\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5215])\n",
      "Loss: 4.240220\n",
      "loss 4.2402\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4863])\n",
      "Loss: 3.739149\n",
      "loss 3.7391\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4343])\n",
      "Loss: 4.087193\n",
      "loss 4.0872\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3671])\n",
      "Loss: 3.979402\n",
      "loss 3.9794\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2833])\n",
      "Loss: 3.575886\n",
      "loss 3.5759\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1834])\n",
      "Loss: 2.816405\n",
      "loss 2.8164\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0673])\n",
      "Loss: 2.838164\n",
      "loss 2.8382\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9445])\n",
      "Loss: 2.759661\n",
      "loss 2.7597\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8178])\n",
      "Loss: 2.546854\n",
      "loss 2.5469\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6886])\n",
      "Loss: 2.204187\n",
      "loss 2.2042\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5592])\n",
      "Loss: 2.274554\n",
      "loss 2.2746\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4299])\n",
      "Loss: 2.325512\n",
      "loss 2.3255\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3019])\n",
      "Loss: 1.886900\n",
      "loss 1.8869\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1776])\n",
      "Loss: 1.779867\n",
      "loss 1.7799\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0559])\n",
      "Loss: 2.299423\n",
      "loss 2.2994\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3655])\n",
      "Loss: 1.928550\n",
      "loss 1.9285\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1982])\n",
      "Loss: 1.984587\n",
      "loss 1.9846\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0439])\n",
      "Loss: 1.907200\n",
      "loss 1.9072\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9043])\n",
      "Loss: 1.913734\n",
      "loss 1.9137\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7791])\n",
      "Loss: 1.847167\n",
      "loss 1.8472\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6738])\n",
      "Loss: 1.964343\n",
      "loss 1.9643\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5813])\n",
      "Loss: 1.922961\n",
      "loss 1.9230\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5150])\n",
      "Loss: 1.994771\n",
      "loss 1.9948\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.6231])\n",
      "Loss: 3.166498\n",
      "loss 3.1665\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.7883])\n",
      "Loss: 1.856668\n",
      "loss 1.8567\n",
      "tensor(1.00000e-05 *\n",
      "       [ 3.8005])\n",
      "Loss: 2.056615\n",
      "loss 2.0566\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.3801])\n",
      "Loss: 1.888485\n",
      "loss 1.8885\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1361])\n",
      "Loss: 1.924012\n",
      "loss 1.9240\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7620])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.866323\n",
      "loss 1.8663\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4176])\n",
      "Loss: 1.733698\n",
      "loss 1.7337\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0910])\n",
      "Loss: 1.796145\n",
      "loss 1.7961\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7794])\n",
      "Loss: 1.828395\n",
      "loss 1.8284\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4901])\n",
      "Loss: 1.907035\n",
      "loss 1.9070\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2274])\n",
      "Loss: 1.776491\n",
      "loss 1.7765\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9771])\n",
      "Loss: 2.077062\n",
      "loss 2.0771\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7529])\n",
      "Loss: 1.768762\n",
      "loss 1.7688\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5386])\n",
      "Loss: 1.816923\n",
      "loss 1.8169\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3358])\n",
      "Loss: 1.821556\n",
      "loss 1.8216\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1399])\n",
      "Loss: 1.749877\n",
      "loss 1.7499\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.9551])\n",
      "Loss: 1.911513\n",
      "loss 1.9115\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0781])\n",
      "Loss: 1.882624\n",
      "loss 1.8826\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1592])\n",
      "Loss: 2.141543\n",
      "loss 2.1415\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2381])\n",
      "Loss: 2.288980\n",
      "loss 2.2890\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3136])\n",
      "Loss: 2.559707\n",
      "loss 2.5597\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3864])\n",
      "Loss: 2.402494\n",
      "loss 2.4025\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4537])\n",
      "Loss: 2.327244\n",
      "loss 2.3272\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5151])\n",
      "Loss: 2.537559\n",
      "loss 2.5376\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5707])\n",
      "Loss: 2.484555\n",
      "loss 2.4846\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6178])\n",
      "Loss: 2.300831\n",
      "loss 2.3008\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6566])\n",
      "Loss: 2.383022\n",
      "loss 2.3830\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6863])\n",
      "Loss: 2.931778\n",
      "loss 2.9318\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7095])\n",
      "Loss: 2.448360\n",
      "loss 2.4484\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7191])\n",
      "Loss: 2.548594\n",
      "loss 2.5486\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7161])\n",
      "Loss: 3.145199\n",
      "loss 3.1452\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6985])\n",
      "Loss: 2.507814\n",
      "loss 2.5078\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6572])\n",
      "Loss: 2.497367\n",
      "loss 2.4974\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5819])\n",
      "Loss: 2.162061\n",
      "loss 2.1621\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4873])\n",
      "Loss: 2.440720\n",
      "loss 2.4407\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3833])\n",
      "Loss: 2.573416\n",
      "loss 2.5734\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2683])\n",
      "Loss: 1.922971\n",
      "loss 1.9230\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1447])\n",
      "Loss: 2.124210\n",
      "loss 2.1242\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0193])\n",
      "Loss: 2.098117\n",
      "loss 2.0981\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.9612])\n",
      "Loss: 1.934128\n",
      "loss 1.9341\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7210])\n",
      "Loss: 1.831246\n",
      "loss 1.8312\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5174])\n",
      "Loss: 1.966268\n",
      "loss 1.9663\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3387])\n",
      "Loss: 1.660468\n",
      "loss 1.6605\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1918])\n",
      "Loss: 1.976559\n",
      "loss 1.9766\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0941])\n",
      "Loss: 1.866697\n",
      "loss 1.8667\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0086])\n",
      "Loss: 1.925381\n",
      "loss 1.9254\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.2623])\n",
      "Loss: 1.805304\n",
      "loss 1.8053\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.1396])\n",
      "Loss: 2.676136\n",
      "loss 2.6761\n",
      "tensor(1.00000e-05 *\n",
      "       [ 8.3536])\n",
      "Loss: 1.739103\n",
      "loss 1.7391\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3622])\n",
      "Loss: 1.940040\n",
      "loss 1.9400\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7679])\n",
      "Loss: 2.083908\n",
      "loss 2.0839\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1165])\n",
      "Loss: 3.107512\n",
      "loss 3.1075\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4311])\n",
      "Loss: 2.100163\n",
      "loss 2.1002\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7047])\n",
      "Loss: 1.993136\n",
      "loss 1.9931\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.9923])\n",
      "Loss: 1.934351\n",
      "loss 1.9344\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.2344])\n",
      "Loss: 1.888908\n",
      "loss 1.8889\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0421])\n",
      "Loss: 2.113451\n",
      "loss 2.1135\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1564])\n",
      "Loss: 2.329528\n",
      "loss 2.3295\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2662])\n",
      "Loss: 2.219267\n",
      "loss 2.2193\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3693])\n",
      "Loss: 2.335641\n",
      "loss 2.3356\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4675])\n",
      "Loss: 2.240617\n",
      "loss 2.2406\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5604])\n",
      "Loss: 2.686789\n",
      "loss 2.6868\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6480])\n",
      "Loss: 3.428729\n",
      "loss 3.4287\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7270])\n",
      "Loss: 2.563900\n",
      "loss 2.5639\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8002])\n",
      "Loss: 2.840907\n",
      "loss 2.8409\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8650])\n",
      "Loss: 2.679550\n",
      "loss 2.6795\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9207])\n",
      "Loss: 3.528311\n",
      "loss 3.5283\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9661])\n",
      "Loss: 3.670212\n",
      "loss 3.6702\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9941])\n",
      "Loss: 3.217658\n",
      "loss 3.2177\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0048])\n",
      "Loss: 2.942657\n",
      "loss 2.9427\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9976])\n",
      "Loss: 2.911527\n",
      "loss 2.9115\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9733])\n",
      "Loss: 2.853482\n",
      "loss 2.8535\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9293])\n",
      "Loss: 2.980019\n",
      "loss 2.9800\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8643])\n",
      "Loss: 2.285772\n",
      "loss 2.2858\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7863])\n",
      "Loss: 2.166469\n",
      "loss 2.1665\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6960])\n",
      "Loss: 2.374857\n",
      "loss 2.3749\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5910])\n",
      "Loss: 2.201855\n",
      "loss 2.2019\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4734])\n",
      "Loss: 2.201366\n",
      "loss 2.2014\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3502])\n",
      "Loss: 2.085502\n",
      "loss 2.0855\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2238])\n",
      "Loss: 3.051115\n",
      "loss 3.0511\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0967])\n",
      "Loss: 1.806005\n",
      "loss 1.8060\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7371])\n",
      "Loss: 2.489023\n",
      "loss 2.4890\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5233])\n",
      "Loss: 2.074828\n",
      "loss 2.0748\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3433])\n",
      "Loss: 1.859760\n",
      "loss 1.8598\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2043])\n",
      "Loss: 1.974993\n",
      "loss 1.9750\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1068])\n",
      "Loss: 1.846390\n",
      "loss 1.8464\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0220])\n",
      "Loss: 1.827029\n",
      "loss 1.8270\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9656])\n",
      "Loss: 1.901437\n",
      "loss 1.9014\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9316])\n",
      "Loss: 2.269277\n",
      "loss 2.2693\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.0610])\n",
      "Loss: 2.690538\n",
      "loss 2.6905\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.1023])\n",
      "Loss: 1.768481\n",
      "loss 1.7685\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.2365])\n",
      "Loss: 1.927106\n",
      "loss 1.9271\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.7704])\n",
      "Loss: 1.878958\n",
      "loss 1.8790\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6929])\n",
      "Loss: 1.944171\n",
      "loss 1.9442\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6204])\n",
      "Loss: 2.692082\n",
      "loss 2.6921\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5444])\n",
      "Loss: 1.896052\n",
      "loss 1.8961\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5252])\n",
      "Loss: 1.935141\n",
      "loss 1.9351\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5416])\n",
      "Loss: 1.700301\n",
      "loss 1.7003\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5504])\n",
      "Loss: 1.970992\n",
      "loss 1.9710\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5588])\n",
      "Loss: 1.902427\n",
      "loss 1.9024\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5342])\n",
      "Loss: 1.938650\n",
      "loss 1.9386\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4987])\n",
      "Loss: 1.966521\n",
      "loss 1.9665\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0444])\n",
      "Loss: 1.970150\n",
      "loss 1.9702\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1384])\n",
      "Loss: 1.994898\n",
      "loss 1.9949\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2296])\n",
      "Loss: 2.024595\n",
      "loss 2.0246\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3165])\n",
      "Loss: 2.470688\n",
      "loss 2.4707\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4009])\n",
      "Loss: 2.076530\n",
      "loss 2.0765\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4776])\n",
      "Loss: 2.545648\n",
      "loss 2.5456\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5442])\n",
      "Loss: 2.914744\n",
      "loss 2.9147\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5946])\n",
      "Loss: 2.614279\n",
      "loss 2.6143\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6380])\n",
      "Loss: 2.573665\n",
      "loss 2.5737\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6609])\n",
      "Loss: 2.528522\n",
      "loss 2.5285\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6719])\n",
      "Loss: 2.457372\n",
      "loss 2.4574\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6661])\n",
      "Loss: 2.382118\n",
      "loss 2.3821\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6437])\n",
      "Loss: 2.544687\n",
      "loss 2.5447\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6044])\n",
      "Loss: 2.363132\n",
      "loss 2.3631\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5442])\n",
      "Loss: 2.092702\n",
      "loss 2.0927\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4726])\n",
      "Loss: 2.251331\n",
      "loss 2.2513\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3858])\n",
      "Loss: 2.117064\n",
      "loss 2.1171\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2866])\n",
      "Loss: 2.065635\n",
      "loss 2.0656\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1782])\n",
      "Loss: 1.770803\n",
      "loss 1.7708\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0611])\n",
      "Loss: 2.029634\n",
      "loss 2.0296\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4324])\n",
      "Loss: 2.302643\n",
      "loss 2.3026\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2276])\n",
      "Loss: 1.992235\n",
      "loss 1.9922\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1217])\n",
      "Loss: 1.951748\n",
      "loss 1.9517\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0209])\n",
      "Loss: 1.661139\n",
      "loss 1.6611\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8963])\n",
      "Loss: 1.988900\n",
      "loss 1.9889\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7794])\n",
      "Loss: 2.132270\n",
      "loss 2.1323\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6928])\n",
      "Loss: 2.070462\n",
      "loss 2.0705\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6383])\n",
      "Loss: 2.096473\n",
      "loss 2.0965\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.2683])\n",
      "Loss: 2.219965\n",
      "loss 2.2200\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.2937])\n",
      "Loss: 1.964777\n",
      "loss 1.9648\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9449])\n",
      "Loss: 2.197594\n",
      "loss 2.1976\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8618])\n",
      "Loss: 2.035644\n",
      "loss 2.0356\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5842])\n",
      "Loss: 1.922929\n",
      "loss 1.9229\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1730])\n",
      "Loss: 2.132036\n",
      "loss 2.1320\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6524])\n",
      "Loss: 1.867715\n",
      "loss 1.8677\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0043])\n",
      "Loss: 2.141711\n",
      "loss 2.1417\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1359])\n",
      "Loss: 2.139045\n",
      "loss 2.1390\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2600])\n",
      "Loss: 2.516630\n",
      "loss 2.5166\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3771])\n",
      "Loss: 1.875974\n",
      "loss 1.8760\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4861])\n",
      "Loss: 2.909282\n",
      "loss 2.9093\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5899])\n",
      "Loss: 2.115282\n",
      "loss 2.1153\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6867])\n",
      "Loss: 2.495995\n",
      "loss 2.4960\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7785])\n",
      "Loss: 3.468906\n",
      "loss 3.4689\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8636])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.196856\n",
      "loss 3.1969\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9387])\n",
      "Loss: 3.516930\n",
      "loss 3.5169\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0036])\n",
      "Loss: 3.609951\n",
      "loss 3.6100\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0572])\n",
      "Loss: 3.316198\n",
      "loss 3.3162\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0982])\n",
      "Loss: 3.751873\n",
      "loss 3.7519\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1259])\n",
      "Loss: 3.656576\n",
      "loss 3.6566\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1383])\n",
      "Loss: 3.722814\n",
      "loss 3.7228\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1351])\n",
      "Loss: 3.297795\n",
      "loss 3.2978\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1121])\n",
      "Loss: 3.657249\n",
      "loss 3.6572\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0700])\n",
      "Loss: 3.600127\n",
      "loss 3.6001\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0083])\n",
      "Loss: 2.845869\n",
      "loss 2.8459\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9224])\n",
      "Loss: 2.882328\n",
      "loss 2.8823\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8217])\n",
      "Loss: 2.494658\n",
      "loss 2.4947\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7084])\n",
      "Loss: 2.620872\n",
      "loss 2.6209\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5866])\n",
      "Loss: 2.254149\n",
      "loss 2.2541\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4587])\n",
      "Loss: 2.178135\n",
      "loss 2.1781\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3255])\n",
      "Loss: 2.081063\n",
      "loss 2.0811\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1917])\n",
      "Loss: 2.030510\n",
      "loss 2.0305\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0601])\n",
      "Loss: 1.849862\n",
      "loss 1.8499\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.2982])\n",
      "Loss: 1.700480\n",
      "loss 1.7005\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.0297])\n",
      "Loss: 1.783781\n",
      "loss 1.7838\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7960])\n",
      "Loss: 1.893015\n",
      "loss 1.8930\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5879])\n",
      "Loss: 1.809596\n",
      "loss 1.8096\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4108])\n",
      "Loss: 1.898712\n",
      "loss 1.8987\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2529])\n",
      "Loss: 2.186560\n",
      "loss 2.1866\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1143])\n",
      "Loss: 1.882616\n",
      "loss 1.8826\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0237])\n",
      "Loss: 1.755513\n",
      "loss 1.7555\n",
      "tensor(1.00000e-05 *\n",
      "       [ 5.9580])\n",
      "Loss: 6.195068\n",
      "loss 6.1951\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.6668])\n",
      "Loss: 1.975344\n",
      "loss 1.9753\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4780])\n",
      "Loss: 1.908107\n",
      "loss 1.9081\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3862])\n",
      "Loss: 2.189271\n",
      "loss 2.1893\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0878])\n",
      "Loss: 1.940642\n",
      "loss 1.9406\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6380])\n",
      "Loss: 1.788998\n",
      "loss 1.7890\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.0950])\n",
      "Loss: 3.933738\n",
      "loss 3.9337\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4736])\n",
      "Loss: 1.981539\n",
      "loss 1.9815\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0814])\n",
      "Loss: 1.965093\n",
      "loss 1.9651\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2097])\n",
      "Loss: 1.836384\n",
      "loss 1.8364\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3332])\n",
      "Loss: 2.175874\n",
      "loss 2.1759\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4526])\n",
      "Loss: 2.434323\n",
      "loss 2.4343\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5679])\n",
      "Loss: 3.091310\n",
      "loss 3.0913\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6793])\n",
      "Loss: 2.757624\n",
      "loss 2.7576\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7854])\n",
      "Loss: 3.224186\n",
      "loss 3.2242\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8883])\n",
      "Loss: 3.035054\n",
      "loss 3.0351\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9864])\n",
      "Loss: 3.297702\n",
      "loss 3.2977\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0807])\n",
      "Loss: 3.887159\n",
      "loss 3.8872\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1704])\n",
      "Loss: 3.668653\n",
      "loss 3.6687\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2556])\n",
      "Loss: 4.187480\n",
      "loss 4.1875\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3376])\n",
      "Loss: 4.662571\n",
      "loss 4.6626\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4154])\n",
      "Loss: 4.290815\n",
      "loss 4.2908\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4884])\n",
      "Loss: 4.885437\n",
      "loss 4.8854\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5571])\n",
      "Loss: 5.052258\n",
      "loss 5.0523\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6208])\n",
      "Loss: 5.201222\n",
      "loss 5.2012\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6787])\n",
      "Loss: 4.927219\n",
      "loss 4.9272\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7310])\n",
      "Loss: 5.865819\n",
      "loss 5.8658\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7772])\n",
      "Loss: 5.678732\n",
      "loss 5.6787\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8150])\n",
      "Loss: 6.153400\n",
      "loss 6.1534\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8444])\n",
      "Loss: 5.433794\n",
      "loss 5.4338\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8632])\n",
      "Loss: 5.801589\n",
      "loss 5.8016\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8714])\n",
      "Loss: 5.718126\n",
      "loss 5.7181\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8683])\n",
      "Loss: 5.164052\n",
      "loss 5.1641\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8523])\n",
      "Loss: 5.482176\n",
      "loss 5.4822\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8240])\n",
      "Loss: 5.255441\n",
      "loss 5.2554\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7823])\n",
      "Loss: 5.212409\n",
      "loss 5.2124\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7238])\n",
      "Loss: 4.163231\n",
      "loss 4.1632\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6493])\n",
      "Loss: 5.003548\n",
      "loss 5.0035\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5617])\n",
      "Loss: 5.123967\n",
      "loss 5.1240\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4609])\n",
      "Loss: 3.622650\n",
      "loss 3.6226\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3485])\n",
      "Loss: 3.546131\n",
      "loss 3.5461\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2290])\n",
      "Loss: 3.105585\n",
      "loss 3.1056\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1037])\n",
      "Loss: 3.455036\n",
      "loss 3.4550\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9741])\n",
      "Loss: 2.954477\n",
      "loss 2.9545\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8414])\n",
      "Loss: 2.442646\n",
      "loss 2.4426\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7079])\n",
      "Loss: 2.421689\n",
      "loss 2.4217\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5752])\n",
      "Loss: 2.186792\n",
      "loss 2.1868\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4437])\n",
      "Loss: 1.889597\n",
      "loss 1.8896\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3141])\n",
      "Loss: 1.961083\n",
      "loss 1.9611\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1870])\n",
      "Loss: 1.869530\n",
      "loss 1.8695\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0619])\n",
      "Loss: 1.855685\n",
      "loss 1.8557\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3951])\n",
      "Loss: 2.112839\n",
      "loss 2.1128\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1961])\n",
      "Loss: 1.837033\n",
      "loss 1.8370\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0255])\n",
      "Loss: 1.952886\n",
      "loss 1.9529\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8777])\n",
      "Loss: 2.023674\n",
      "loss 2.0237\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7483])\n",
      "Loss: 1.866113\n",
      "loss 1.8661\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6348])\n",
      "Loss: 1.874478\n",
      "loss 1.8745\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5356])\n",
      "Loss: 1.830750\n",
      "loss 1.8308\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4520])\n",
      "Loss: 1.925924\n",
      "loss 1.9259\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.8165])\n",
      "Loss: 3.091741\n",
      "loss 3.0917\n",
      "tensor(1.00000e-04 *\n",
      "       [ 6.7566])\n",
      "Loss: 6.147725\n",
      "loss 6.1477\n",
      "tensor(1.00000e-04 *\n",
      "       [ 9.4021])\n",
      "Loss: 1.867869\n",
      "loss 1.8679\n",
      "tensor(1.00000e-05 *\n",
      "       [-3.6979])\n",
      "Loss: 2.435795\n",
      "loss 2.4358\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0285])\n",
      "Loss: 1.953294\n",
      "loss 1.9533\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0521])\n",
      "Loss: 1.973382\n",
      "loss 1.9734\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0885])\n",
      "Loss: 1.879311\n",
      "loss 1.8793\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1193])\n",
      "Loss: 1.979788\n",
      "loss 1.9798\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1507])\n",
      "Loss: 2.913683\n",
      "loss 2.9137\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2019])\n",
      "Loss: 1.689828\n",
      "loss 1.6898\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.2654])\n",
      "Loss: 1.813402\n",
      "loss 1.8134\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3242])\n",
      "Loss: 3.327382\n",
      "loss 3.3274\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3676])\n",
      "Loss: 2.355258\n",
      "loss 2.3553\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0418])\n",
      "Loss: 2.403265\n",
      "loss 2.4033\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1455])\n",
      "Loss: 1.975655\n",
      "loss 1.9757\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2463])\n",
      "Loss: 1.781879\n",
      "loss 1.7819\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3458])\n",
      "Loss: 2.216667\n",
      "loss 2.2167\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4440])\n",
      "Loss: 4.244291\n",
      "loss 4.2443\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5430])\n",
      "Loss: 2.259940\n",
      "loss 2.2599\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6423])\n",
      "Loss: 2.164230\n",
      "loss 2.1642\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7392])\n",
      "Loss: 2.765363\n",
      "loss 2.7654\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8340])\n",
      "Loss: 2.876267\n",
      "loss 2.8763\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9274])\n",
      "Loss: 2.460097\n",
      "loss 2.4601\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0171])\n",
      "Loss: 3.026832\n",
      "loss 3.0268\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1043])\n",
      "Loss: 3.436928\n",
      "loss 3.4369\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1884])\n",
      "Loss: 3.505333\n",
      "loss 3.5053\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2685])\n",
      "Loss: 3.098454\n",
      "loss 3.0985\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3448])\n",
      "Loss: 3.375234\n",
      "loss 3.3752\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4168])\n",
      "Loss: 3.713110\n",
      "loss 3.7131\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4845])\n",
      "Loss: 3.579566\n",
      "loss 3.5796\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5481])\n",
      "Loss: 7.231097\n",
      "loss 7.2311\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6072])\n",
      "Loss: 3.885952\n",
      "loss 3.8860\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6625])\n",
      "Loss: 4.760366\n",
      "loss 4.7604\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7131])\n",
      "Loss: 4.507727\n",
      "loss 4.5077\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7561])\n",
      "Loss: 6.595604\n",
      "loss 6.5956\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7925])\n",
      "Loss: 4.915726\n",
      "loss 4.9157\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8169])\n",
      "Loss: 4.033308\n",
      "loss 4.0333\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8306])\n",
      "Loss: 3.864410\n",
      "loss 3.8644\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8354])\n",
      "Loss: 5.082402\n",
      "loss 5.0824\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8301])\n",
      "Loss: 3.999261\n",
      "loss 3.9993\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8124])\n",
      "Loss: 4.626626\n",
      "loss 4.6266\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7815])\n",
      "Loss: 4.348325\n",
      "loss 4.3483\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7368])\n",
      "Loss: 4.413389\n",
      "loss 4.4134\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6792])\n",
      "Loss: 3.666548\n",
      "loss 3.6665\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6076])\n",
      "Loss: 3.105142\n",
      "loss 3.1051\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5238])\n",
      "Loss: 3.551361\n",
      "loss 3.5514\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4298])\n",
      "Loss: 2.802082\n",
      "loss 2.8021\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3254])\n",
      "Loss: 4.190026\n",
      "loss 4.1900\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2137])\n",
      "Loss: 2.809665\n",
      "loss 2.8097\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0963])\n",
      "Loss: 2.584571\n",
      "loss 2.5846\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9746])\n",
      "Loss: 2.320079\n",
      "loss 2.3201\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8494])\n",
      "Loss: 2.587556\n",
      "loss 2.5876\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7232])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.464392\n",
      "loss 2.4644\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5971])\n",
      "Loss: 2.106827\n",
      "loss 2.1068\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4705])\n",
      "Loss: 2.114560\n",
      "loss 2.1146\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3448])\n",
      "Loss: 2.460776\n",
      "loss 2.4608\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2198])\n",
      "Loss: 2.427544\n",
      "loss 2.4275\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0972])\n",
      "Loss: 1.931466\n",
      "loss 1.9315\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7424])\n",
      "Loss: 1.995169\n",
      "loss 1.9952\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5351])\n",
      "Loss: 2.150693\n",
      "loss 2.1507\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3611])\n",
      "Loss: 1.931295\n",
      "loss 1.9313\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1977])\n",
      "Loss: 2.613325\n",
      "loss 2.6133\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0685])\n",
      "Loss: 3.311715\n",
      "loss 3.3117\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9835])\n",
      "Loss: 1.785921\n",
      "loss 1.7859\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9281])\n",
      "Loss: 2.518485\n",
      "loss 2.5185\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8716])\n",
      "Loss: 2.126100\n",
      "loss 2.1261\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.4290])\n",
      "Loss: 2.111485\n",
      "loss 2.1115\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.2693])\n",
      "Loss: 4.824605\n",
      "loss 4.8246\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0079])\n",
      "Loss: 2.905813\n",
      "loss 2.9058\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.7568])\n",
      "Loss: 2.694502\n",
      "loss 2.6945\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.7594])\n",
      "Loss: 2.749528\n",
      "loss 2.7495\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3746])\n",
      "Loss: 1.902119\n",
      "loss 1.9021\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7062])\n",
      "Loss: 1.950437\n",
      "loss 1.9504\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9645])\n",
      "Loss: 1.888573\n",
      "loss 1.8886\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1859])\n",
      "Loss: 1.908055\n",
      "loss 1.9081\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3799])\n",
      "Loss: 1.849236\n",
      "loss 1.8492\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5513])\n",
      "Loss: 2.102754\n",
      "loss 2.1028\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6973])\n",
      "Loss: 2.024388\n",
      "loss 2.0244\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0832])\n",
      "Loss: 2.102101\n",
      "loss 2.1021\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1942])\n",
      "Loss: 2.392038\n",
      "loss 2.3920\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3028])\n",
      "Loss: 2.041439\n",
      "loss 2.0414\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4088])\n",
      "Loss: 2.042843\n",
      "loss 2.0428\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5125])\n",
      "Loss: 2.322216\n",
      "loss 2.3222\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6141])\n",
      "Loss: 2.395291\n",
      "loss 2.3953\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7129])\n",
      "Loss: 3.068291\n",
      "loss 3.0683\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8095])\n",
      "Loss: 2.720485\n",
      "loss 2.7205\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9037])\n",
      "Loss: 2.715362\n",
      "loss 2.7154\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9954])\n",
      "Loss: 2.735821\n",
      "loss 2.7358\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0850])\n",
      "Loss: 3.438022\n",
      "loss 3.4380\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1716])\n",
      "Loss: 3.001443\n",
      "loss 3.0014\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2558])\n",
      "Loss: 3.250731\n",
      "loss 3.2507\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3361])\n",
      "Loss: 4.732605\n",
      "loss 4.7326\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4125])\n",
      "Loss: 4.259717\n",
      "loss 4.2597\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4841])\n",
      "Loss: 4.239681\n",
      "loss 4.2397\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5505])\n",
      "Loss: 3.873649\n",
      "loss 3.8736\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6119])\n",
      "Loss: 3.974349\n",
      "loss 3.9743\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6683])\n",
      "Loss: 3.579333\n",
      "loss 3.5793\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7196])\n",
      "Loss: 4.463268\n",
      "loss 4.4633\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7651])\n",
      "Loss: 5.440635\n",
      "loss 5.4406\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8052])\n",
      "Loss: 5.049909\n",
      "loss 5.0499\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8355])\n",
      "Loss: 4.814960\n",
      "loss 4.8150\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8562])\n",
      "Loss: 5.275672\n",
      "loss 5.2757\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8672])\n",
      "Loss: 4.646336\n",
      "loss 4.6463\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8657])\n",
      "Loss: 3.947985\n",
      "loss 3.9480\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8539])\n",
      "Loss: 5.742098\n",
      "loss 5.7421\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8305])\n",
      "Loss: 3.664349\n",
      "loss 3.6643\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7965])\n",
      "Loss: 4.363891\n",
      "loss 4.3639\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7503])\n",
      "Loss: 3.865179\n",
      "loss 3.8652\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6873])\n",
      "Loss: 4.167226\n",
      "loss 4.1672\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6109])\n",
      "Loss: 3.494057\n",
      "loss 3.4941\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5194])\n",
      "Loss: 3.589933\n",
      "loss 3.5899\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4166])\n",
      "Loss: 3.812366\n",
      "loss 3.8124\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3053])\n",
      "Loss: 3.112810\n",
      "loss 3.1128\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1842])\n",
      "Loss: 2.493868\n",
      "loss 2.4939\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0581])\n",
      "Loss: 2.821163\n",
      "loss 2.8212\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9299])\n",
      "Loss: 3.551101\n",
      "loss 3.5511\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8025])\n",
      "Loss: 2.296520\n",
      "loss 2.2965\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6746])\n",
      "Loss: 2.740501\n",
      "loss 2.7405\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5474])\n",
      "Loss: 2.067315\n",
      "loss 2.0673\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4216])\n",
      "Loss: 2.028684\n",
      "loss 2.0287\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2982])\n",
      "Loss: 1.867476\n",
      "loss 1.8675\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1759])\n",
      "Loss: 1.828046\n",
      "loss 1.8280\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0570])\n",
      "Loss: 2.184400\n",
      "loss 2.1844\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3888])\n",
      "Loss: 1.865939\n",
      "loss 1.8659\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2192])\n",
      "Loss: 1.880278\n",
      "loss 1.8803\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0688])\n",
      "Loss: 1.874342\n",
      "loss 1.8743\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9337])\n",
      "Loss: 1.944414\n",
      "loss 1.9444\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8131])\n",
      "Loss: 1.956336\n",
      "loss 1.9563\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7361])\n",
      "Loss: 2.283078\n",
      "loss 2.2831\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6744])\n",
      "Loss: 2.610338\n",
      "loss 2.6103\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6170])\n",
      "Loss: 1.978798\n",
      "loss 1.9788\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.9744])\n",
      "Loss: 2.490549\n",
      "loss 2.4905\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.0583])\n",
      "Loss: 2.584735\n",
      "loss 2.5847\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.0366])\n",
      "Loss: 2.738430\n",
      "loss 2.7384\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.5428])\n",
      "Loss: 2.340292\n",
      "loss 2.3403\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8510])\n",
      "Loss: 1.990997\n",
      "loss 1.9910\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9896])\n",
      "Loss: 1.881985\n",
      "loss 1.8820\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1407])\n",
      "Loss: 1.932269\n",
      "loss 1.9323\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2775])\n",
      "Loss: 2.271065\n",
      "loss 2.2711\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3967])\n",
      "Loss: 1.640221\n",
      "loss 1.6402\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5210])\n",
      "Loss: 1.814353\n",
      "loss 1.8144\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6310])\n",
      "Loss: 2.132728\n",
      "loss 2.1327\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7225])\n",
      "Loss: 2.052772\n",
      "loss 2.0528\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0790])\n",
      "Loss: 1.999353\n",
      "loss 1.9994\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1840])\n",
      "Loss: 2.209346\n",
      "loss 2.2093\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2870])\n",
      "Loss: 2.166299\n",
      "loss 2.1663\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3874])\n",
      "Loss: 2.341755\n",
      "loss 2.3418\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4860])\n",
      "Loss: 1.956597\n",
      "loss 1.9566\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5829])\n",
      "Loss: 2.252172\n",
      "loss 2.2522\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6780])\n",
      "Loss: 2.901067\n",
      "loss 2.9011\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7706])\n",
      "Loss: 2.704401\n",
      "loss 2.7044\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8586])\n",
      "Loss: 2.454089\n",
      "loss 2.4541\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9431])\n",
      "Loss: 2.609528\n",
      "loss 2.6095\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0245])\n",
      "Loss: 3.067074\n",
      "loss 3.0671\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1020])\n",
      "Loss: 3.409754\n",
      "loss 3.4098\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1747])\n",
      "Loss: 3.010303\n",
      "loss 3.0103\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2417])\n",
      "Loss: 3.879467\n",
      "loss 3.8795\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3046])\n",
      "Loss: 3.054232\n",
      "loss 3.0542\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3601])\n",
      "Loss: 3.401625\n",
      "loss 3.4016\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4092])\n",
      "Loss: 3.523955\n",
      "loss 3.5240\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4516])\n",
      "Loss: 3.854081\n",
      "loss 3.8541\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4851])\n",
      "Loss: 3.991205\n",
      "loss 3.9912\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5076])\n",
      "Loss: 3.780125\n",
      "loss 3.7801\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5189])\n",
      "Loss: 3.700296\n",
      "loss 3.7003\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5170])\n",
      "Loss: 3.262064\n",
      "loss 3.2621\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5031])\n",
      "Loss: 3.783503\n",
      "loss 3.7835\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4791])\n",
      "Loss: 2.986801\n",
      "loss 2.9868\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4420])\n",
      "Loss: 3.453806\n",
      "loss 3.4538\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3916])\n",
      "Loss: 3.217291\n",
      "loss 3.2173\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3247])\n",
      "Loss: 3.604754\n",
      "loss 3.6048\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2449])\n",
      "Loss: 2.813973\n",
      "loss 2.8140\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1486])\n",
      "Loss: 2.176894\n",
      "loss 2.1769\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0412])\n",
      "Loss: 2.723931\n",
      "loss 2.7239\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9284])\n",
      "Loss: 2.314760\n",
      "loss 2.3148\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8065])\n",
      "Loss: 2.385662\n",
      "loss 2.3857\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6805])\n",
      "Loss: 2.027896\n",
      "loss 2.0279\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5520])\n",
      "Loss: 1.855590\n",
      "loss 1.8556\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4241])\n",
      "Loss: 2.504305\n",
      "loss 2.5043\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2968])\n",
      "Loss: 1.843282\n",
      "loss 1.8433\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1709])\n",
      "Loss: 2.080873\n",
      "loss 2.0809\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0466])\n",
      "Loss: 3.982892\n",
      "loss 3.9829\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.2379])\n",
      "Loss: 1.889829\n",
      "loss 1.8898\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.0875])\n",
      "Loss: 1.896845\n",
      "loss 1.8968\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9409])\n",
      "Loss: 2.045226\n",
      "loss 2.0452\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8059])\n",
      "Loss: 1.845112\n",
      "loss 1.8451\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6976])\n",
      "Loss: 1.964121\n",
      "loss 1.9641\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5990])\n",
      "Loss: 1.877460\n",
      "loss 1.8775\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5257])\n",
      "Loss: 2.243452\n",
      "loss 2.2435\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4714])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.086820\n",
      "loss 2.0868\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.4826])\n",
      "Loss: 2.923357\n",
      "loss 2.9234\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.5034])\n",
      "Loss: 3.556650\n",
      "loss 3.5567\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.2065])\n",
      "Loss: 3.933072\n",
      "loss 3.9331\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2043])\n",
      "Loss: 1.759226\n",
      "loss 1.7592\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5870])\n",
      "Loss: 1.793403\n",
      "loss 1.7934\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8999])\n",
      "Loss: 1.784362\n",
      "loss 1.7844\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1708])\n",
      "Loss: 1.832796\n",
      "loss 1.8328\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4045])\n",
      "Loss: 1.910892\n",
      "loss 1.9109\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.6092])\n",
      "Loss: 1.926653\n",
      "loss 1.9267\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7878])\n",
      "Loss: 1.949395\n",
      "loss 1.9494\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.9419])\n",
      "Loss: 1.872688\n",
      "loss 1.8727\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1066])\n",
      "Loss: 2.788858\n",
      "loss 2.7889\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2173])\n",
      "Loss: 2.207045\n",
      "loss 2.2070\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3300])\n",
      "Loss: 2.099492\n",
      "loss 2.0995\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4391])\n",
      "Loss: 2.149911\n",
      "loss 2.1499\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5450])\n",
      "Loss: 2.229682\n",
      "loss 2.2297\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6481])\n",
      "Loss: 2.032236\n",
      "loss 2.0322\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7481])\n",
      "Loss: 2.697959\n",
      "loss 2.6980\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8457])\n",
      "Loss: 2.352327\n",
      "loss 2.3523\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9396])\n",
      "Loss: 2.542723\n",
      "loss 2.5427\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0306])\n",
      "Loss: 2.967984\n",
      "loss 2.9680\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1166])\n",
      "Loss: 3.223394\n",
      "loss 3.2234\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1983])\n",
      "Loss: 2.859409\n",
      "loss 2.8594\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2736])\n",
      "Loss: 3.678382\n",
      "loss 3.6784\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3440])\n",
      "Loss: 3.055389\n",
      "loss 3.0554\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4082])\n",
      "Loss: 3.912311\n",
      "loss 3.9123\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4673])\n",
      "Loss: 3.794261\n",
      "loss 3.7943\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5178])\n",
      "Loss: 3.834924\n",
      "loss 3.8349\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5602])\n",
      "Loss: 3.705963\n",
      "loss 3.7060\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5938])\n",
      "Loss: 3.879236\n",
      "loss 3.8792\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6183])\n",
      "Loss: 3.786500\n",
      "loss 3.7865\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6331])\n",
      "Loss: 4.055140\n",
      "loss 4.0551\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6370])\n",
      "Loss: 3.840451\n",
      "loss 3.8405\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6277])\n",
      "Loss: 3.587634\n",
      "loss 3.5876\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6011])\n",
      "Loss: 3.727904\n",
      "loss 3.7279\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5588])\n",
      "Loss: 3.377540\n",
      "loss 3.3775\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4988])\n",
      "Loss: 3.161985\n",
      "loss 3.1620\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4230])\n",
      "Loss: 3.371874\n",
      "loss 3.3719\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3278])\n",
      "Loss: 2.795168\n",
      "loss 2.7952\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2200])\n",
      "Loss: 2.930239\n",
      "loss 2.9302\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1034])\n",
      "Loss: 2.522267\n",
      "loss 2.5223\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9798])\n",
      "Loss: 2.462753\n",
      "loss 2.4628\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8526])\n",
      "Loss: 2.255443\n",
      "loss 2.2554\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7232])\n",
      "Loss: 2.294427\n",
      "loss 2.2944\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5930])\n",
      "Loss: 2.307979\n",
      "loss 2.3080\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4646])\n",
      "Loss: 1.898199\n",
      "loss 1.8982\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3388])\n",
      "Loss: 2.018492\n",
      "loss 2.0185\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2144])\n",
      "Loss: 1.924430\n",
      "loss 1.9244\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0891])\n",
      "Loss: 1.981203\n",
      "loss 1.9812\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6649])\n",
      "Loss: 1.859193\n",
      "loss 1.8592\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4828])\n",
      "Loss: 2.968826\n",
      "loss 2.9688\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3225])\n",
      "Loss: 3.736506\n",
      "loss 3.7365\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2079])\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.8268704414367676\n",
      "Loss: 2.7285714149475098\n",
      "Loss: 2.289259910583496\n",
      "Loss: 2.271031141281128\n",
      "Loss: 2.20009708404541\n",
      "Loss: 2.045694351196289\n",
      "Loss: 1.9569151401519775\n",
      "Loss: 1.9066400527954102\n",
      "Loss: 1.8437570333480835\n",
      "Loss: 1.8244924545288086\n",
      "Loss: 1.5523459911346436\n",
      "Loss: 1.493617296218872\n",
      "Loss: 1.4734643697738647\n",
      "Loss: 1.4040019512176514\n",
      "Loss: 1.2545500993728638\n",
      "Loss: 1.2252566814422607\n",
      "Loss: 1.1988110542297363\n",
      "Loss: 1.1603686809539795\n",
      "Loss: 1.0522253513336182\n",
      "Loss: 1.0674927234649658\n",
      "Loss: 1.1157876253128052\n",
      "Loss: 0.9356104135513306\n",
      "Loss: 1.149595022201538\n",
      "Loss: 0.9950889348983765\n",
      "Loss: 0.8677853941917419\n",
      "Loss: 1.0581778287887573\n",
      "Loss: 0.886738121509552\n",
      "Loss: 0.7940847277641296\n",
      "Loss: 0.8008765578269958\n",
      "Loss: 0.8940781950950623\n",
      "Loss: 0.563798725605011\n",
      "Loss: 0.6920072436332703\n",
      "Loss: 0.8638289570808411\n",
      "Loss: 0.4535703659057617\n",
      "Loss: 0.9806511402130127\n",
      "Loss: 0.7706287503242493\n",
      "Loss: 0.855924129486084\n",
      "Loss: 0.8537286520004272\n",
      "Loss: 0.9185181260108948\n",
      "Loss: 0.5246160626411438\n",
      "Loss: 0.693431556224823\n",
      "Loss: 0.6233241558074951\n",
      "Loss: 0.9098241925239563\n",
      "Loss: 0.6677922606468201\n",
      "Loss: 0.7086930274963379\n",
      "Loss: 0.4602228105068207\n",
      "Loss: 0.7210118174552917\n",
      "Loss: 0.5201213359832764\n",
      "Loss: 0.7750748991966248\n",
      "Loss: 0.5702149271965027\n",
      "Loss: 0.8938213586807251\n",
      "Loss: 0.5993800759315491\n",
      "Loss: 0.5511675477027893\n",
      "Loss: 0.7822446227073669\n",
      "Loss: 0.6509726643562317\n",
      "Loss: 0.6547588109970093\n",
      "Loss: 0.8151517510414124\n",
      "Loss: 0.8324121832847595\n",
      "Loss: 0.5763236880302429\n",
      "Loss: 0.5537047982215881\n",
      "Loss: 0.7426064014434814\n",
      "Loss: 0.9309027791023254\n",
      "Loss: 0.7014952898025513\n",
      "Loss: 1.026444911956787\n",
      "Loss: 0.9896385669708252\n",
      "Loss: 0.6845539808273315\n",
      "Loss: 0.7052661776542664\n",
      "Loss: 0.7488518357276917\n",
      "Loss: 0.5220500230789185\n",
      "Loss: 0.3925919234752655\n",
      "Loss: 0.44295042753219604\n",
      "Loss: 0.7924319505691528\n",
      "Loss: 0.8517633080482483\n",
      "Loss: 0.5767934918403625\n",
      "Loss: 0.7855997681617737\n",
      "Loss: 0.6985209584236145\n",
      "Loss: 1.1090222597122192\n",
      "Loss: 0.6099109053611755\n",
      "Loss: 0.4266444742679596\n",
      "Loss: 0.49177172780036926\n",
      "Loss: 0.9608611464500427\n",
      "Loss: 0.8972565531730652\n",
      "Loss: 0.6924885511398315\n",
      "Loss: 0.5931373238563538\n",
      "Loss: 0.6276944875717163\n",
      "Loss: 0.515484631061554\n",
      "Loss: 0.8354217410087585\n",
      "Loss: 0.5010345578193665\n",
      "Loss: 0.7157689929008484\n",
      "Loss: 0.6963730454444885\n",
      "Loss: 0.4852728247642517\n",
      "Loss: 0.5829402208328247\n",
      "Loss: 0.5689002275466919\n",
      "Loss: 0.7938761115074158\n",
      "Loss: 0.5338709950447083\n",
      "Loss: 0.522861897945404\n",
      "Loss: 0.3315768539905548\n",
      "Loss: 0.7007299065589905\n",
      "Loss: 0.517586350440979\n",
      "Loss: 0.9612346887588501\n",
      "Loss: 0.6699265241622925\n",
      "Loss: 0.46109169721603394\n",
      "Loss: 0.5555634498596191\n",
      "Loss: 0.5147398114204407\n",
      "Loss: 0.6290324926376343\n",
      "Loss: 0.4117487072944641\n",
      "Loss: 0.604621171951294\n",
      "Loss: 0.4543677568435669\n",
      "Loss: 0.7622026205062866\n",
      "Loss: 0.5069926977157593\n",
      "Loss: 0.5410445928573608\n",
      "Loss: 0.6943756937980652\n",
      "Loss: 0.5105015635490417\n",
      "Loss: 0.5102148056030273\n",
      "Loss: 0.5004056692123413\n",
      "Loss: 0.4602140486240387\n",
      "Loss: 0.5079277157783508\n",
      "Loss: 0.8302347660064697\n",
      "Loss: 0.630818784236908\n",
      "Loss: 0.5466413497924805\n",
      "Loss: 0.6061133146286011\n",
      "Loss: 0.3273656964302063\n",
      "Loss: 0.5894258618354797\n",
      "Loss: 0.2853078246116638\n",
      "Loss: 0.7046576142311096\n",
      "Loss: 0.653439998626709\n",
      "Loss: 0.4462164044380188\n",
      "Loss: 0.4035573899745941\n",
      "Loss: 0.5139361023902893\n",
      "Loss: 0.220862478017807\n",
      "Loss: 0.5272828340530396\n",
      "Loss: 0.5220127105712891\n",
      "Loss: 0.632683277130127\n",
      "Loss: 0.3095960319042206\n",
      "Loss: 0.4700898826122284\n",
      "Loss: 0.46073219180107117\n",
      "Loss: 0.48940131068229675\n",
      "Loss: 0.3680412173271179\n",
      "Loss: 0.6610591411590576\n",
      "Loss: 0.6855260729789734\n",
      "Loss: 0.6226629018783569\n",
      "Loss: 0.4426116645336151\n",
      "Loss: 0.6082232594490051\n",
      "Loss: 0.6631662845611572\n",
      "Loss: 0.5707111954689026\n",
      "Loss: 0.40644511580467224\n",
      "Loss: 0.5316007137298584\n",
      "Loss: 0.5037873983383179\n",
      "Loss: 0.5136178731918335\n",
      "Loss: 0.6446705460548401\n",
      "Loss: 0.41355228424072266\n",
      "Loss: 0.47688916325569153\n",
      "Loss: 0.6814756989479065\n",
      "Loss: 0.422492116689682\n",
      "Loss: 0.6781734228134155\n",
      "Loss: 0.4035264253616333\n",
      "Loss: 0.6626248359680176\n",
      "Loss: 0.4403608739376068\n",
      "Loss: 0.3370770215988159\n",
      "Loss: 0.7379875183105469\n",
      "Loss: 0.4407710134983063\n",
      "Loss: 0.5245612859725952\n",
      "Loss: 0.5790132880210876\n",
      "Loss: 0.4555608034133911\n",
      "Loss: 0.3000392019748688\n",
      "Loss: 0.3928617835044861\n",
      "Loss: 0.5111456513404846\n",
      "Loss: 0.3450430631637573\n",
      "Loss: 0.5152300000190735\n",
      "Loss: 0.5624929666519165\n",
      "Loss: 0.5460996627807617\n",
      "Loss: 0.40787413716316223\n",
      "Loss: 0.6095544695854187\n",
      "Loss: 0.47354161739349365\n",
      "Loss: 0.5062823295593262\n",
      "Loss: 0.4686928689479828\n",
      "Loss: 0.4339519143104553\n",
      "Loss: 0.4458712041378021\n",
      "Loss: 0.3373465836048126\n",
      "Loss: 0.41347193717956543\n",
      "Loss: 0.5509779453277588\n",
      "Loss: 0.23359142243862152\n",
      "Loss: 0.6642550826072693\n",
      "Loss: 0.5890916585922241\n",
      "Loss: 0.28382325172424316\n",
      "Loss: 0.36184462904930115\n",
      "Loss: 0.6134248971939087\n",
      "Loss: 0.48593270778656006\n",
      "Loss: 0.7739272117614746\n",
      "Loss: 0.36094456911087036\n",
      "Loss: 0.6224782466888428\n",
      "Loss: 0.40261173248291016\n",
      "Loss: 0.3470136225223541\n",
      "Loss: 0.4201328158378601\n",
      "Loss: 0.5963545441627502\n",
      "Loss: 0.35519900918006897\n",
      "Loss: 0.5821698307991028\n",
      "Loss: 0.4760313034057617\n",
      "Loss: 0.35489243268966675\n",
      "Loss: 0.3332943022251129\n",
      "Loss: 0.43500176072120667\n",
      "Loss: 0.5509073138237\n",
      "Loss: 0.31604158878326416\n",
      "Loss: 0.33963775634765625\n",
      "Loss: 0.6776637434959412\n",
      "Loss: 0.527444064617157\n",
      "Loss: 0.2651779055595398\n",
      "Loss: 0.556480884552002\n",
      "Loss: 0.3209173083305359\n",
      "Loss: 0.5822687149047852\n",
      "Loss: 0.41936981678009033\n",
      "Loss: 0.3425793945789337\n",
      "Loss: 0.4008038341999054\n",
      "Loss: 0.46316927671432495\n",
      "Loss: 0.5621505975723267\n",
      "Loss: 0.43168535828590393\n",
      "Loss: 0.290035605430603\n",
      "Loss: 0.5173320770263672\n",
      "Loss: 0.39727330207824707\n",
      "Loss: 0.7035868763923645\n",
      "Loss: 0.5119448304176331\n",
      "Loss: 0.4258573055267334\n",
      "Loss: 0.4846433997154236\n",
      "Loss: 0.6602756381034851\n",
      "Loss: 0.4879384934902191\n",
      "Loss: 0.4889649450778961\n",
      "Loss: 0.27223408222198486\n",
      "Loss: 0.5132256150245667\n",
      "Loss: 0.3637648820877075\n",
      "Loss: 0.3480847179889679\n",
      "Loss: 0.5559399724006653\n",
      "Loss: 0.4157484769821167\n",
      "Loss: 0.5064781308174133\n",
      "Loss: 0.6629538536071777\n",
      "Loss: 0.32494670152664185\n",
      "Loss: 0.6650556921958923\n",
      "Loss: 0.32571738958358765\n",
      "Loss: 0.5212994813919067\n",
      "Loss: 0.38104480504989624\n",
      "Loss: 0.432529091835022\n",
      "Loss: 0.6762896776199341\n",
      "Loss: 0.6272403597831726\n",
      "Loss: 0.43699848651885986\n",
      "Loss: 0.4193158745765686\n",
      "Loss: 0.35156285762786865\n",
      "Loss: 0.23917771875858307\n",
      "Loss: 0.44955337047576904\n",
      "Loss: 0.5602311491966248\n",
      "Loss: 0.4148620665073395\n",
      "Loss: 0.3851999044418335\n",
      "Loss: 0.6558510065078735\n",
      "Loss: 0.4176062345504761\n",
      "Loss: 0.3264029026031494\n",
      "Loss: 0.2663118541240692\n",
      "Loss: 0.7788742184638977\n",
      "Loss: 0.47577187418937683\n",
      "Loss: 0.5198990106582642\n",
      "Loss: 0.3389027416706085\n",
      "Loss: 0.3751872777938843\n",
      "Loss: 0.4407028555870056\n",
      "Loss: 0.36425384879112244\n",
      "Loss: 0.575973629951477\n",
      "Loss: 0.480204701423645\n",
      "Loss: 0.1705932766199112\n",
      "Loss: 0.7690532803535461\n",
      "Loss: 0.4371262788772583\n",
      "Loss: 0.39403587579727173\n",
      "Loss: 0.3658250570297241\n",
      "Loss: 0.379911869764328\n",
      "Loss: 0.3565920293331146\n",
      "Loss: 0.21022476255893707\n",
      "Loss: 0.3426927626132965\n",
      "Loss: 0.2927516996860504\n",
      "Loss: 0.43185150623321533\n",
      "Loss: 0.7297479510307312\n",
      "Loss: 0.386101633310318\n",
      "Loss: 0.40475451946258545\n",
      "Loss: 0.44047728180885315\n",
      "Loss: 0.3261023461818695\n",
      "Loss: 0.5146797895431519\n",
      "Loss: 0.42695367336273193\n",
      "Loss: 0.47117432951927185\n",
      "Loss: 0.5259628891944885\n",
      "Loss: 0.3137573003768921\n",
      "Loss: 0.4583764672279358\n",
      "Loss: 0.581698477268219\n",
      "Loss: 0.38874009251594543\n",
      "Loss: 0.28687554597854614\n",
      "Loss: 0.5658443570137024\n",
      "Loss: 0.2958012819290161\n",
      "Loss: 0.5853860378265381\n",
      "Loss: 0.454496294260025\n",
      "Loss: 0.3268589675426483\n",
      "Loss: 0.5054172873497009\n",
      "Loss: 0.3103293180465698\n",
      "Loss: 0.4274301528930664\n",
      "Loss: 0.4583113193511963\n",
      "Loss: 0.5827463269233704\n",
      "Loss: 0.464794397354126\n",
      "Loss: 0.35336780548095703\n",
      "Loss: 0.3082122802734375\n",
      "Loss: 0.25274530053138733\n",
      "Loss: 0.33683276176452637\n",
      "Loss: 0.43775513768196106\n",
      "Loss: 0.37797218561172485\n",
      "Loss: 0.6106452941894531\n",
      "Loss: 0.4653424322605133\n",
      "Loss: 0.5015596151351929\n",
      "Loss: 0.464051216840744\n",
      "Loss: 0.2788836359977722\n",
      "Loss: 0.324863076210022\n",
      "Loss: 0.5338000655174255\n",
      "Loss: 0.5589936375617981\n",
      "Loss: 0.7324172258377075\n",
      "Loss: 0.47566941380500793\n",
      "Loss: 0.588752269744873\n",
      "Loss: 0.45509204268455505\n",
      "Loss: 0.2571176290512085\n",
      "Loss: 0.5294003486633301\n",
      "Loss: 0.2725122570991516\n",
      "Loss: 0.19845417141914368\n",
      "Loss: 0.41041821241378784\n",
      "Loss: 0.5984894633293152\n",
      "Loss: 0.35793402791023254\n",
      "Loss: 0.7470488548278809\n",
      "Loss: 0.30976417660713196\n",
      "Loss: 0.3765689730644226\n",
      "Loss: 0.4377851188182831\n",
      "Loss: 0.27791446447372437\n",
      "Loss: 0.48252442479133606\n",
      "Loss: 0.2815602421760559\n",
      "Loss: 0.45667994022369385\n",
      "Loss: 0.5877588391304016\n",
      "Loss: 0.3559221625328064\n",
      "Loss: 0.3226439356803894\n",
      "Loss: 0.5283555388450623\n",
      "Loss: 0.32189297676086426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.563618540763855\n",
      "Loss: 0.5111581087112427\n",
      "Loss: 0.36514556407928467\n",
      "Loss: 0.45664486289024353\n",
      "Loss: 0.3715624213218689\n",
      "Loss: 0.5820598006248474\n",
      "Loss: 0.3891925811767578\n",
      "Loss: 0.7250534296035767\n",
      "Loss: 0.27294039726257324\n",
      "Loss: 0.3358617424964905\n",
      "Loss: 0.4220786988735199\n",
      "Loss: 0.6052753329277039\n",
      "Loss: 0.8506615161895752\n",
      "Loss: 0.6801207065582275\n",
      "Loss: 0.5049019455909729\n",
      "Loss: 0.31883618235588074\n",
      "Loss: 0.37427768111228943\n",
      "Loss: 0.3218585252761841\n",
      "Loss: 0.5204296708106995\n",
      "Loss: 0.22815518081188202\n",
      "Loss: 0.5129340887069702\n",
      "Loss: 0.39079907536506653\n",
      "Loss: 0.2627788782119751\n",
      "Loss: 0.2520519196987152\n",
      "Loss: 0.23376873135566711\n",
      "Loss: 0.4659309685230255\n",
      "Loss: 0.5273849368095398\n",
      "Loss: 0.48139050602912903\n",
      "Loss: 0.31470754742622375\n",
      "Loss: 0.3491021990776062\n",
      "Loss: 0.5018615126609802\n",
      "Loss: 0.24332115054130554\n",
      "Loss: 0.49822482466697693\n",
      "Loss: 0.3501926064491272\n",
      "Loss: 0.4038342237472534\n",
      "Loss: 0.49790531396865845\n",
      "Loss: 0.6591052412986755\n",
      "Loss: 0.5976378917694092\n",
      "Loss: 0.5134736895561218\n",
      "Loss: 0.2911728024482727\n",
      "Loss: 0.7659839391708374\n",
      "Loss: 0.47913265228271484\n",
      "Loss: 0.4788520038127899\n",
      "Loss: 0.404119074344635\n",
      "Loss: 0.6876100897789001\n",
      "Loss: 0.5375731587409973\n",
      "Loss: 0.5261980295181274\n",
      "Loss: 0.371092289686203\n",
      "Loss: 0.7272440195083618\n",
      "Loss: 0.63410484790802\n",
      "Loss: 0.3805043399333954\n",
      "Loss: 0.375753253698349\n",
      "Loss: 0.4576834738254547\n",
      "Loss: 0.3502000868320465\n",
      "Loss: 0.5035855770111084\n",
      "Loss: 0.3576285243034363\n",
      "Loss: 0.3737734854221344\n",
      "Loss: 0.6611928343772888\n",
      "Loss: 0.31855103373527527\n",
      "Loss: 0.38986101746559143\n",
      "Loss: 0.4754767119884491\n",
      "Loss: 0.298458069562912\n",
      "Loss: 0.6445478200912476\n",
      "Loss: 0.5078971982002258\n",
      "Loss: 0.6039398312568665\n",
      "Loss: 0.488328218460083\n",
      "Loss: 0.35021111369132996\n",
      "Loss: 0.28739333152770996\n",
      "Loss: 0.27082717418670654\n",
      "Loss: 0.6304298043251038\n",
      "Loss: 0.5461423993110657\n",
      "Loss: 0.4203183352947235\n",
      "Loss: 0.39061060547828674\n",
      "Loss: 0.26402726769447327\n",
      "Loss: 0.3604545295238495\n",
      "Loss: 0.4873143434524536\n",
      "Loss: 0.40456536412239075\n",
      "Loss: 0.40246376395225525\n",
      "Loss: 0.5202978253364563\n",
      "Loss: 0.2795114517211914\n",
      "Loss: 0.47758904099464417\n",
      "Loss: 0.4638949930667877\n",
      "Loss: 0.2956402897834778\n",
      "Loss: 0.6076560616493225\n",
      "Loss: 0.6627346277236938\n",
      "Loss: 0.39825308322906494\n",
      "Loss: 0.30197057127952576\n",
      "Loss: 0.27487263083457947\n",
      "Loss: 0.6901791095733643\n",
      "Loss: 0.24419240653514862\n",
      "Loss: 0.49304354190826416\n",
      "Loss: 0.5703350305557251\n",
      "Loss: 0.3169631361961365\n",
      "Loss: 0.7870416045188904\n",
      "Loss: 0.42648619413375854\n",
      "Loss: 0.7112293839454651\n",
      "Loss: 0.32083502411842346\n",
      "Loss: 0.402365118265152\n",
      "Loss: 0.5212119221687317\n",
      "Loss: 0.4426797032356262\n",
      "Loss: 0.3562711775302887\n",
      "Loss: 0.4773198366165161\n",
      "Loss: 0.28448328375816345\n",
      "Loss: 0.8241384029388428\n",
      "Loss: 0.3077535033226013\n",
      "Loss: 0.33838504552841187\n",
      "Loss: 0.8048654794692993\n",
      "Loss: 0.2793505787849426\n",
      "Loss: 0.5205234289169312\n",
      "Loss: 0.6791040897369385\n",
      "Loss: 0.43597984313964844\n",
      "Loss: 0.29521939158439636\n",
      "Loss: 0.34556400775909424\n",
      "Loss: 0.5338584184646606\n",
      "Loss: 0.659247875213623\n",
      "Loss: 0.2924928069114685\n",
      "Loss: 0.6539412140846252\n",
      "Loss: 0.30148428678512573\n",
      "Loss: 0.4627296030521393\n",
      "Loss: 0.38542747497558594\n",
      "Loss: 0.5484554171562195\n",
      "Loss: 0.37392517924308777\n",
      "Loss: 0.5838325023651123\n",
      "Loss: 0.40977194905281067\n",
      "Loss: 0.4264295697212219\n",
      "Loss: 0.39422667026519775\n",
      "Loss: 0.3904668390750885\n",
      "Loss: 0.692312479019165\n",
      "Loss: 0.6914656758308411\n",
      "Loss: 0.4735942780971527\n",
      "Loss: 0.5849688649177551\n",
      "Loss: 0.5230314135551453\n",
      "Loss: 0.6569890379905701\n",
      "Loss: 0.3596445918083191\n",
      "Loss: 0.5719095468521118\n",
      "Loss: 0.44344887137413025\n",
      "Loss: 0.2626059651374817\n",
      "Loss: 0.6863491535186768\n",
      "Loss: 0.3321501612663269\n",
      "Loss: 0.5600516200065613\n",
      "Loss: 0.8035033345222473\n",
      "Loss: 0.38777050375938416\n",
      "Loss: 0.4128214120864868\n",
      "Loss: 0.4070849120616913\n",
      "Loss: 0.6394978165626526\n",
      "Loss: 0.6827226281166077\n",
      "Loss: 0.5240834951400757\n",
      "Loss: 0.39003437757492065\n",
      "Loss: 0.5558003783226013\n",
      "Loss: 0.33243680000305176\n",
      "Loss: 0.5212699174880981\n",
      "Loss: 0.5858243107795715\n",
      "Loss: 0.2381519377231598\n",
      "Loss: 0.5399771332740784\n",
      "Loss: 0.4159449636936188\n",
      "Loss: 0.37324604392051697\n",
      "Loss: 0.3249644339084625\n",
      "Loss: 0.3030511140823364\n",
      "Loss: 0.43353748321533203\n",
      "Loss: 0.28459712862968445\n",
      "Loss: 0.2785043716430664\n",
      "Loss: 0.3611451983451843\n",
      "Loss: 0.5812318921089172\n",
      "Loss: 0.40265780687332153\n",
      "Loss: 0.2588941156864166\n",
      "Loss: 0.39571160078048706\n",
      "Loss: 0.3912501931190491\n",
      "Loss: 0.47331103682518005\n",
      "Loss: 0.3357907831668854\n",
      "Loss: 0.28727957606315613\n",
      "Loss: 0.42356541752815247\n",
      "Loss: 0.3336896002292633\n",
      "Loss: 0.7242122888565063\n",
      "Loss: 0.5439851880073547\n",
      "Loss: 0.8285448551177979\n",
      "Loss: 0.9079920053482056\n",
      "Loss: 0.3781103789806366\n",
      "Loss: 0.3699275255203247\n",
      "Loss: 0.31653034687042236\n",
      "Loss: 0.6272127628326416\n",
      "Loss: 0.12932434678077698\n",
      "Loss: 0.34521278738975525\n",
      "Loss: 0.3940148651599884\n",
      "Loss: 0.4212331473827362\n",
      "Loss: 0.4312910735607147\n",
      "Loss: 0.5801954865455627\n",
      "Loss: 1.025148868560791\n",
      "Loss: 0.5785686373710632\n",
      "Loss: 0.3353825509548187\n",
      "Loss: 0.46787986159324646\n",
      "Loss: 0.4393676817417145\n",
      "Loss: 0.347383052110672\n",
      "Loss: 0.5893007516860962\n",
      "Loss: 0.3485175371170044\n",
      "Loss: 0.4088495373725891\n",
      "Loss: 0.3853636085987091\n",
      "Loss: 0.3470275104045868\n",
      "Loss: 0.4731697142124176\n",
      "Loss: 0.8306799530982971\n",
      "Loss: 0.547270655632019\n",
      "Loss: 0.46324488520622253\n",
      "Loss: 0.34927958250045776\n",
      "Loss: 0.5155799388885498\n",
      "Loss: 0.3313893973827362\n",
      "Loss: 0.3711455464363098\n",
      "Loss: 0.4387716054916382\n",
      "Loss: 0.5280826687812805\n",
      "Loss: 0.5209189057350159\n",
      "Loss: 0.583734929561615\n",
      "Loss: 0.5172989964485168\n",
      "Loss: 0.6780651211738586\n",
      "Loss: 0.3391484022140503\n",
      "Loss: 0.45682278275489807\n",
      "Loss: 0.538408637046814\n",
      "Loss: 0.3385186791419983\n",
      "Loss: 0.6156297922134399\n",
      "Loss: 0.4733361303806305\n",
      "Loss: 0.3993856906890869\n",
      "Loss: 0.278679758310318\n",
      "Loss: 0.5002280473709106\n",
      "Loss: 0.6740127801895142\n",
      "Loss: 0.2525623142719269\n",
      "Loss: 0.6443734169006348\n",
      "Loss: 0.6065722703933716\n",
      "Loss: 0.36635878682136536\n",
      "Loss: 0.5162913799285889\n",
      "Loss: 0.5036328434944153\n",
      "Loss: 0.7715233564376831\n",
      "Loss: 0.33348843455314636\n",
      "Loss: 0.36289066076278687\n",
      "Loss: 0.44213107228279114\n",
      "Loss: 0.48683783411979675\n",
      "Loss: 0.46406230330467224\n",
      "Loss: 0.4581737220287323\n",
      "Loss: 0.6640055179595947\n",
      "Loss: 0.364592045545578\n",
      "Loss: 0.39251500368118286\n",
      "Loss: 0.30045798420906067\n",
      "Loss: 0.34439337253570557\n",
      "Loss: 0.2893798053264618\n",
      "Loss: 0.19340407848358154\n",
      "Loss: 0.4479158818721771\n",
      "Loss: 0.4030481278896332\n",
      "Loss: 0.38516026735305786\n",
      "Loss: 0.32437801361083984\n",
      "Loss: 0.5819297432899475\n",
      "Loss: 0.25128984451293945\n",
      "Loss: 0.5931104421615601\n",
      "Loss: 0.6342899799346924\n",
      "Loss: 0.45997878909111023\n",
      "Loss: 0.43027418851852417\n",
      "Loss: 0.5391866564750671\n",
      "Loss: 0.3334451913833618\n",
      "Loss: 0.30619016289711\n",
      "Loss: 0.47433075308799744\n",
      "Loss: 0.3552566468715668\n",
      "Loss: 0.4733935296535492\n",
      "Loss: 0.5108640193939209\n",
      "Loss: 0.36388739943504333\n",
      "Loss: 0.2689220607280731\n",
      "Loss: 0.2976963520050049\n",
      "Loss: 0.4099748730659485\n",
      "Loss: 0.2821563482284546\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7761785984039307\n",
      "Loss: 2.6229982376098633\n",
      "Loss: 2.3505163192749023\n",
      "Loss: 2.4562995433807373\n",
      "Loss: 2.2963004112243652\n",
      "Loss: 2.2877907752990723\n",
      "Loss: 2.326465368270874\n",
      "Loss: 2.2519524097442627\n",
      "Loss: 2.257910966873169\n",
      "Loss: 2.294045925140381\n",
      "Loss: 2.233964204788208\n",
      "Loss: 2.2303404808044434\n",
      "Loss: 2.2559640407562256\n",
      "Loss: 2.2133121490478516\n",
      "Loss: 2.2026357650756836\n",
      "Loss: 2.190990686416626\n",
      "Loss: 2.220290422439575\n",
      "Loss: 2.1194369792938232\n",
      "Loss: 2.217381238937378\n",
      "Loss: 2.2835404872894287\n",
      "Loss: 2.217271566390991\n",
      "Loss: 2.1934566497802734\n",
      "Loss: 2.1742794513702393\n",
      "Loss: 2.1087615489959717\n",
      "Loss: 2.1302084922790527\n",
      "Loss: 2.1062982082366943\n",
      "Loss: 2.1814534664154053\n",
      "Loss: 2.2340080738067627\n",
      "Loss: 2.0547263622283936\n",
      "Loss: 2.057990074157715\n",
      "Loss: 2.0572774410247803\n",
      "Loss: 1.9716852903366089\n",
      "Loss: 2.0525991916656494\n",
      "Loss: 2.092095136642456\n",
      "Loss: 2.1406333446502686\n",
      "Loss: 1.951910376548767\n",
      "Loss: 1.848122000694275\n",
      "Loss: 2.0032799243927\n",
      "Loss: 2.099607229232788\n",
      "Loss: 2.067258358001709\n",
      "Loss: 1.9320701360702515\n",
      "Loss: 1.9726645946502686\n",
      "Loss: 1.9771926403045654\n",
      "Loss: 1.8770033121109009\n",
      "Loss: 1.8907755613327026\n",
      "Loss: 1.9728175401687622\n",
      "Loss: 2.000509262084961\n",
      "Loss: 1.7934232950210571\n",
      "Loss: 1.9496251344680786\n",
      "Loss: 1.9357458353042603\n",
      "Loss: 1.9000872373580933\n",
      "Loss: 1.8720113039016724\n",
      "Loss: 1.960324764251709\n",
      "Loss: 1.9707434177398682\n",
      "Loss: 1.7768954038619995\n",
      "Loss: 2.065124750137329\n",
      "Loss: 1.8660792112350464\n",
      "Loss: 1.915037989616394\n",
      "Loss: 1.6766194105148315\n",
      "Loss: 1.8622937202453613\n",
      "Loss: 1.881842851638794\n",
      "Loss: 1.9258832931518555\n",
      "Loss: 1.802130937576294\n",
      "Loss: 1.8607308864593506\n",
      "Loss: 1.9005357027053833\n",
      "Loss: 1.8307286500930786\n",
      "Loss: 1.7792598009109497\n",
      "Loss: 1.7618829011917114\n",
      "Loss: 1.736527442932129\n",
      "Loss: 1.816770076751709\n",
      "Loss: 1.8389784097671509\n",
      "Loss: 1.7220661640167236\n",
      "Loss: 1.8376452922821045\n",
      "Loss: 1.707107663154602\n",
      "Loss: 1.663920283317566\n",
      "Loss: 1.6257520914077759\n",
      "Loss: 1.7988008260726929\n",
      "Loss: 1.8563679456710815\n",
      "Loss: 1.7532262802124023\n",
      "Loss: 1.543245553970337\n",
      "Loss: 1.8294804096221924\n",
      "Loss: 1.7639998197555542\n",
      "Loss: 1.6721739768981934\n",
      "Loss: 1.8296372890472412\n",
      "Loss: 1.7845969200134277\n",
      "Loss: 1.8127943277359009\n",
      "Loss: 1.8519117832183838\n",
      "Loss: 1.8449656963348389\n",
      "Loss: 1.8906545639038086\n",
      "Loss: 1.6486183404922485\n",
      "Loss: 1.809994101524353\n",
      "Loss: 1.7419756650924683\n",
      "Loss: 1.8006377220153809\n",
      "Loss: 1.7438676357269287\n",
      "Loss: 1.5806028842926025\n",
      "Loss: 1.583214521408081\n",
      "Loss: 1.7524930238723755\n",
      "Loss: 1.7737038135528564\n",
      "Loss: 1.6557321548461914\n",
      "Loss: 1.7771484851837158\n",
      "Loss: 1.8024941682815552\n",
      "Loss: 1.6213306188583374\n",
      "Loss: 1.4715304374694824\n",
      "Loss: 1.5456717014312744\n",
      "Loss: 1.4665478467941284\n",
      "Loss: 1.6817634105682373\n",
      "Loss: 1.5167316198349\n",
      "Loss: 1.6538474559783936\n",
      "Loss: 1.7058523893356323\n",
      "Loss: 1.4588689804077148\n",
      "Loss: 1.7701730728149414\n",
      "Loss: 1.6554874181747437\n",
      "Loss: 1.6338406801223755\n",
      "Loss: 1.5570696592330933\n",
      "Loss: 1.8326280117034912\n",
      "Loss: 1.587528109550476\n",
      "Loss: 1.6124500036239624\n",
      "Loss: 1.6094468832015991\n",
      "Loss: 1.5516315698623657\n",
      "Loss: 1.6840689182281494\n",
      "Loss: 1.679428219795227\n",
      "Loss: 1.7079211473464966\n",
      "Loss: 1.5571653842926025\n",
      "Loss: 1.6354900598526\n",
      "Loss: 1.7003772258758545\n",
      "Loss: 1.512791633605957\n",
      "Loss: 1.6826539039611816\n",
      "Loss: 1.8110897541046143\n",
      "Loss: 1.7202889919281006\n",
      "Loss: 1.5797975063323975\n",
      "Loss: 1.723537564277649\n",
      "Loss: 1.636834740638733\n",
      "Loss: 1.6808491945266724\n",
      "Loss: 1.7392839193344116\n",
      "Loss: 1.4444971084594727\n",
      "Loss: 1.7004094123840332\n",
      "Loss: 1.6908364295959473\n",
      "Loss: 1.572078824043274\n",
      "Loss: 1.626466989517212\n",
      "Loss: 1.6525421142578125\n",
      "Loss: 1.6239421367645264\n",
      "Loss: 1.462048053741455\n",
      "Loss: 1.6433939933776855\n",
      "Loss: 1.8903087377548218\n",
      "Loss: 1.5401743650436401\n",
      "Loss: 1.6263736486434937\n",
      "Loss: 1.528706669807434\n",
      "Loss: 1.6734346151351929\n",
      "Loss: 1.5962586402893066\n",
      "Loss: 1.595068097114563\n",
      "Loss: 1.52561616897583\n",
      "Loss: 1.819730520248413\n",
      "Loss: 1.6733849048614502\n",
      "Loss: 1.817211627960205\n",
      "Loss: 1.4844944477081299\n",
      "Loss: 1.751998782157898\n",
      "Loss: 1.6968834400177002\n",
      "Loss: 1.518936038017273\n",
      "Loss: 1.5036094188690186\n",
      "Loss: 1.4877314567565918\n",
      "Loss: 1.8428317308425903\n",
      "Loss: 1.6342304944992065\n",
      "Loss: 1.594912052154541\n",
      "Loss: 1.5100491046905518\n",
      "Loss: 1.6239559650421143\n",
      "Loss: 1.5958971977233887\n",
      "Loss: 1.7587298154830933\n",
      "Loss: 1.6967289447784424\n",
      "Loss: 1.6746734380722046\n",
      "Loss: 1.7997050285339355\n",
      "Loss: 1.5136351585388184\n",
      "Loss: 1.7643330097198486\n",
      "Loss: 1.8106151819229126\n",
      "Loss: 1.6323562860488892\n",
      "Loss: 1.5722110271453857\n",
      "Loss: 1.5515551567077637\n",
      "Loss: 1.416326880455017\n",
      "Loss: 1.7000635862350464\n",
      "Loss: 1.4369713068008423\n",
      "Loss: 1.6291558742523193\n",
      "Loss: 1.541536569595337\n",
      "Loss: 1.607566475868225\n",
      "Loss: 1.584220290184021\n",
      "Loss: 1.3659470081329346\n",
      "Loss: 1.6521391868591309\n",
      "Loss: 1.6615324020385742\n",
      "Loss: 1.4404128789901733\n",
      "Loss: 1.5067371129989624\n",
      "Loss: 1.2626413106918335\n",
      "Loss: 1.5025233030319214\n",
      "Loss: 1.6143734455108643\n",
      "Loss: 1.5208206176757812\n",
      "Loss: 1.735862135887146\n",
      "Loss: 1.576749563217163\n",
      "Loss: 1.3646061420440674\n",
      "Loss: 1.5571105480194092\n",
      "Loss: 1.5071966648101807\n",
      "Loss: 1.7666889429092407\n",
      "Loss: 1.507231593132019\n",
      "Loss: 1.6360491514205933\n",
      "Loss: 1.505568504333496\n",
      "Loss: 1.383665919303894\n",
      "Loss: 1.736325740814209\n",
      "Loss: 1.4471265077590942\n",
      "Loss: 1.593106985092163\n",
      "Loss: 1.4998126029968262\n",
      "Loss: 1.2213186025619507\n",
      "Loss: 1.2973114252090454\n",
      "Loss: 1.5741902589797974\n",
      "Loss: 1.6257116794586182\n",
      "Loss: 1.4233134984970093\n",
      "Loss: 1.399164080619812\n",
      "Loss: 1.462852120399475\n",
      "Loss: 1.6052166223526\n",
      "Loss: 1.4667857885360718\n",
      "Loss: 1.5968642234802246\n",
      "Loss: 1.7493380308151245\n",
      "Loss: 1.5773197412490845\n",
      "Loss: 1.5953483581542969\n",
      "Loss: 1.6044787168502808\n",
      "Loss: 1.5084611177444458\n",
      "Loss: 1.5916444063186646\n",
      "Loss: 1.7250856161117554\n",
      "Loss: 1.5554431676864624\n",
      "Loss: 1.4115393161773682\n",
      "Loss: 1.4591134786605835\n",
      "Loss: 1.8305299282073975\n",
      "Loss: 1.395514726638794\n",
      "Loss: 1.4815808534622192\n",
      "Loss: 1.538163661956787\n",
      "Loss: 1.5059020519256592\n",
      "Loss: 1.5055125951766968\n",
      "Loss: 1.4645609855651855\n",
      "Loss: 1.8675035238265991\n",
      "Loss: 1.5466854572296143\n",
      "Loss: 1.5062354803085327\n",
      "Loss: 1.5456379652023315\n",
      "Loss: 1.553948163986206\n",
      "Loss: 1.5146870613098145\n",
      "Loss: 1.7647161483764648\n",
      "Loss: 1.5082603693008423\n",
      "Loss: 1.4551100730895996\n",
      "Loss: 1.5703791379928589\n",
      "Loss: 1.4716547727584839\n",
      "Loss: 1.2912379503250122\n",
      "Loss: 1.6472060680389404\n",
      "Loss: 1.600956678390503\n",
      "Loss: 1.5671576261520386\n",
      "Loss: 1.5071845054626465\n",
      "Loss: 1.5075640678405762\n",
      "Loss: 1.5636727809906006\n",
      "Loss: 1.3572016954421997\n",
      "Loss: 1.5184375047683716\n",
      "Loss: 1.4689732789993286\n",
      "Loss: 1.5819975137710571\n",
      "Loss: 1.4406338930130005\n",
      "Loss: 1.8974151611328125\n",
      "Loss: 1.719369649887085\n",
      "Loss: 1.6633858680725098\n",
      "Loss: 1.4323545694351196\n",
      "Loss: 1.3943198919296265\n",
      "Loss: 1.4427450895309448\n",
      "Loss: 1.615565299987793\n",
      "Loss: 1.714080810546875\n",
      "Loss: 1.5154027938842773\n",
      "Loss: 1.5838967561721802\n",
      "Loss: 1.4493129253387451\n",
      "Loss: 1.5035319328308105\n",
      "Loss: 1.820945143699646\n",
      "Loss: 1.4723750352859497\n",
      "Loss: 1.5922569036483765\n",
      "Loss: 1.6271014213562012\n",
      "Loss: 1.4847397804260254\n",
      "Loss: 1.6104986667633057\n",
      "Loss: 1.5725834369659424\n",
      "Loss: 1.4891072511672974\n",
      "Loss: 1.4101393222808838\n",
      "Loss: 1.5976330041885376\n",
      "Loss: 1.5365347862243652\n",
      "Loss: 1.5974735021591187\n",
      "Loss: 1.4917323589324951\n",
      "Loss: 1.5239332914352417\n",
      "Loss: 1.572494387626648\n",
      "Loss: 1.593377709388733\n",
      "Loss: 1.5873990058898926\n",
      "Loss: 1.6570435762405396\n",
      "Loss: 1.4243273735046387\n",
      "Loss: 1.5628372430801392\n",
      "Loss: 1.4515984058380127\n",
      "Loss: 1.7201132774353027\n",
      "Loss: 1.8243991136550903\n",
      "Loss: 1.537463665008545\n",
      "Loss: 1.415650486946106\n",
      "Loss: 1.5504472255706787\n",
      "Loss: 1.52260160446167\n",
      "Loss: 1.3236714601516724\n",
      "Loss: 1.5097945928573608\n",
      "Loss: 1.8192799091339111\n",
      "Loss: 1.772128701210022\n",
      "Loss: 1.408054232597351\n",
      "Loss: 1.6835083961486816\n",
      "Loss: 1.3749375343322754\n",
      "Loss: 1.66214919090271\n",
      "Loss: 1.5398566722869873\n",
      "Loss: 1.4899840354919434\n",
      "Loss: 1.9051909446716309\n",
      "Loss: 1.5038502216339111\n",
      "Loss: 1.4286137819290161\n",
      "Loss: 1.4966223239898682\n",
      "Loss: 1.3705494403839111\n",
      "Loss: 1.329818606376648\n",
      "Loss: 1.5943913459777832\n",
      "Loss: 1.6672313213348389\n",
      "Loss: 1.4939039945602417\n",
      "Loss: 1.4371376037597656\n",
      "Loss: 1.7317304611206055\n",
      "Loss: 1.6196781396865845\n",
      "Loss: 1.6257492303848267\n",
      "Loss: 1.4937913417816162\n",
      "Loss: 1.5238195657730103\n",
      "Loss: 1.597273826599121\n",
      "Loss: 1.5098016262054443\n",
      "Loss: 1.5811847448349\n",
      "Loss: 1.5827651023864746\n",
      "Loss: 1.4454160928726196\n",
      "Loss: 1.5823006629943848\n",
      "Loss: 1.386690616607666\n",
      "Loss: 1.7494556903839111\n",
      "Loss: 1.490159034729004\n",
      "Loss: 1.2917611598968506\n",
      "Loss: 1.586527705192566\n",
      "Loss: 1.5947983264923096\n",
      "Loss: 1.5692976713180542\n",
      "Loss: 1.5671240091323853\n",
      "Loss: 1.5097029209136963\n",
      "Loss: 1.4475985765457153\n",
      "Loss: 1.5411763191223145\n",
      "Loss: 1.598391056060791\n",
      "Loss: 1.5139411687850952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5343044996261597\n",
      "Loss: 1.470085859298706\n",
      "Loss: 1.7409405708312988\n",
      "Loss: 1.3208531141281128\n",
      "Loss: 1.6532695293426514\n",
      "Loss: 1.701149344444275\n",
      "Loss: 1.5775469541549683\n",
      "Loss: 1.6016736030578613\n",
      "Loss: 1.3746775388717651\n",
      "Loss: 1.5244183540344238\n",
      "Loss: 1.4841586351394653\n",
      "Loss: 1.537546992301941\n",
      "Loss: 1.5829201936721802\n",
      "Loss: 1.5313972234725952\n",
      "Loss: 1.5151599645614624\n",
      "Loss: 1.5521986484527588\n",
      "Loss: 1.5350052118301392\n",
      "Loss: 1.6204383373260498\n",
      "Loss: 1.681959867477417\n",
      "Loss: 1.3975515365600586\n",
      "Loss: 1.558091402053833\n",
      "Loss: 1.5642389059066772\n",
      "Loss: 1.472490668296814\n",
      "Loss: 1.7206252813339233\n",
      "Loss: 1.522516131401062\n",
      "Loss: 1.4833563566207886\n",
      "Loss: 1.6760822534561157\n",
      "Loss: 1.4523215293884277\n",
      "Loss: 1.5783722400665283\n",
      "Loss: 1.555077075958252\n",
      "Loss: 1.6597400903701782\n",
      "Loss: 1.6604149341583252\n",
      "Loss: 1.4987215995788574\n",
      "Loss: 1.5114774703979492\n",
      "Loss: 1.4944146871566772\n",
      "Loss: 1.5135207176208496\n",
      "Loss: 1.6269962787628174\n",
      "Loss: 1.5217090845108032\n",
      "Loss: 1.389207363128662\n",
      "Loss: 1.621984601020813\n",
      "Loss: 1.5753686428070068\n",
      "Loss: 1.5409507751464844\n",
      "Loss: 1.5931193828582764\n",
      "Loss: 1.6199302673339844\n",
      "Loss: 1.629512071609497\n",
      "Loss: 1.827525019645691\n",
      "Loss: 1.6532903909683228\n",
      "Loss: 1.3504116535186768\n",
      "Loss: 1.5963494777679443\n",
      "Loss: 1.5406008958816528\n",
      "Loss: 1.638831377029419\n",
      "Loss: 1.528141736984253\n",
      "Loss: 1.7446500062942505\n",
      "Loss: 1.510393500328064\n",
      "Loss: 1.4084275960922241\n",
      "Loss: 1.6090604066848755\n",
      "Loss: 1.5933597087860107\n",
      "Loss: 1.6655994653701782\n",
      "Loss: 1.5397627353668213\n",
      "Loss: 1.458688497543335\n",
      "Loss: 1.5335770845413208\n",
      "Loss: 1.5929964780807495\n",
      "Loss: 1.705870509147644\n",
      "Loss: 1.5552974939346313\n",
      "Loss: 1.5136550664901733\n",
      "Loss: 1.5646765232086182\n",
      "Loss: 1.392611026763916\n",
      "Loss: 1.5995692014694214\n",
      "Loss: 1.4531805515289307\n",
      "Loss: 1.6103997230529785\n",
      "Loss: 1.6716846227645874\n",
      "Loss: 1.549573540687561\n",
      "Loss: 1.4017820358276367\n",
      "Loss: 1.6390647888183594\n",
      "Loss: 1.5101181268692017\n",
      "Loss: 1.5625238418579102\n",
      "Loss: 1.4571006298065186\n",
      "Loss: 1.574476957321167\n",
      "Loss: 1.424648404121399\n",
      "Loss: 1.5549055337905884\n",
      "Loss: 1.4935333728790283\n",
      "Loss: 1.6002296209335327\n",
      "Loss: 1.5620155334472656\n",
      "Loss: 1.6560494899749756\n",
      "Loss: 1.5104341506958008\n",
      "Loss: 1.5055127143859863\n",
      "Loss: 1.7962449789047241\n",
      "Loss: 1.4280176162719727\n",
      "Loss: 1.4739768505096436\n",
      "Loss: 1.3707084655761719\n",
      "Loss: 2.106766700744629\n",
      "Loss: 1.5505783557891846\n",
      "Loss: 1.3097163438796997\n",
      "Loss: 1.777113914489746\n",
      "Loss: 1.3507236242294312\n",
      "Loss: 1.7479238510131836\n",
      "Loss: 1.7770371437072754\n",
      "Loss: 1.390498399734497\n",
      "Loss: 1.6129238605499268\n",
      "Loss: 1.6359107494354248\n",
      "Loss: 1.4551031589508057\n",
      "Loss: 1.5794916152954102\n",
      "Loss: 1.7027055025100708\n",
      "Loss: 1.384650707244873\n",
      "Loss: 1.6227943897247314\n",
      "Loss: 1.5602810382843018\n",
      "Loss: 1.638441562652588\n",
      "Loss: 1.2709004878997803\n",
      "Loss: 1.4861770868301392\n",
      "Loss: 1.663499355316162\n",
      "Loss: 1.4804283380508423\n",
      "Loss: 1.6152435541152954\n",
      "Loss: 1.5918046236038208\n",
      "Loss: 1.4579856395721436\n",
      "Loss: 1.5907222032546997\n",
      "Loss: 1.621343731880188\n",
      "Loss: 1.5673729181289673\n",
      "Loss: 1.4909920692443848\n",
      "Loss: 1.528672218322754\n",
      "Loss: 1.4115391969680786\n",
      "Loss: 1.4243489503860474\n",
      "Loss: 1.6038563251495361\n",
      "Loss: 1.6333248615264893\n",
      "Loss: 1.6235246658325195\n",
      "Loss: 1.6829451322555542\n",
      "Loss: 1.5916616916656494\n",
      "Loss: 1.4439927339553833\n",
      "Loss: 1.6825995445251465\n",
      "Loss: 1.5691903829574585\n",
      "Loss: 1.4316097497940063\n",
      "Loss: 1.7475290298461914\n",
      "Loss: 1.6994175910949707\n",
      "Loss: 1.5749497413635254\n",
      "Loss: 1.659165859222412\n",
      "Loss: 1.6758341789245605\n",
      "Loss: 1.6737756729125977\n",
      "Loss: 1.5939319133758545\n",
      "Loss: 1.6920057535171509\n",
      "Loss: 1.597353219985962\n",
      "Loss: 1.4669060707092285\n",
      "Loss: 1.5401506423950195\n",
      "Loss: 1.5392425060272217\n",
      "Loss: 1.3521902561187744\n",
      "Loss: 1.6511791944503784\n",
      "Loss: 1.64912748336792\n",
      "Loss: 1.5819220542907715\n",
      "Loss: 1.5033568143844604\n",
      "Loss: 1.6022647619247437\n",
      "Loss: 1.4391857385635376\n",
      "Loss: 1.6254489421844482\n",
      "Loss: 1.596197247505188\n",
      "Loss: 1.5687733888626099\n",
      "Loss: 1.5790891647338867\n",
      "Loss: 1.5681740045547485\n",
      "Loss: 1.590164303779602\n",
      "Loss: 1.3778759241104126\n",
      "Loss: 1.3419548273086548\n",
      "Loss: 1.647099256515503\n",
      "Loss: 1.4481297731399536\n",
      "Loss: 1.4846818447113037\n",
      "Loss: 1.5988149642944336\n",
      "Loss: 1.5195213556289673\n",
      "Loss: 1.633169412612915\n",
      "Loss: 1.6118196249008179\n",
      "Loss: 1.428260326385498\n",
      "Loss: 1.4640355110168457\n",
      "Loss: 1.4841848611831665\n",
      "Loss: 1.9670422077178955\n",
      "Loss: 1.4622858762741089\n",
      "Loss: 1.6675320863723755\n",
      "Loss: 1.4284231662750244\n",
      "Loss: 1.4797112941741943\n",
      "Loss: 1.4810982942581177\n",
      "Loss: 1.4590855836868286\n",
      "Loss: 1.4949618577957153\n",
      "Loss: 1.3983267545700073\n",
      "Loss: 1.6099759340286255\n",
      "Loss: 1.6294236183166504\n",
      "Loss: 1.4829295873641968\n",
      "Loss: 1.5615054368972778\n",
      "Loss: 1.537269115447998\n",
      "Loss: 1.400254249572754\n",
      "Loss: 1.5005472898483276\n",
      "Loss: 1.7669275999069214\n",
      "Loss: 1.38670015335083\n",
      "Loss: 1.4989023208618164\n",
      "Loss: 1.8539766073226929\n",
      "Loss: 1.599676251411438\n",
      "Loss: 1.6344985961914062\n",
      "Loss: 1.7219427824020386\n",
      "Loss: 1.515519618988037\n",
      "Loss: 1.5778532028198242\n",
      "Loss: 1.7104159593582153\n",
      "Loss: 1.6265486478805542\n",
      "Loss: 1.5569829940795898\n",
      "Loss: 1.3978215456008911\n",
      "Loss: 1.5287452936172485\n",
      "Loss: 1.5393568277359009\n",
      "Loss: 1.6861801147460938\n",
      "Loss: 1.5201188325881958\n",
      "Loss: 1.5393409729003906\n",
      "Loss: 1.460595965385437\n",
      "Loss: 1.4774037599563599\n",
      "Loss: 1.3996373414993286\n",
      "Loss: 1.6843987703323364\n",
      "Loss: 1.4568686485290527\n",
      "Loss: 1.6146607398986816\n",
      "Loss: 1.4948500394821167\n",
      "Loss: 1.5927820205688477\n",
      "Loss: 1.8413398265838623\n",
      "Loss: 1.5195305347442627\n",
      "Loss: 1.5434958934783936\n",
      "Loss: 1.437991738319397\n",
      "Loss: 1.5307523012161255\n",
      "Loss: 1.4471113681793213\n",
      "Loss: 1.3936467170715332\n",
      "Loss: 1.5155798196792603\n",
      "Loss: 1.5646121501922607\n",
      "Loss: 1.6177020072937012\n",
      "Loss: 1.3290575742721558\n",
      "Loss: 1.4489772319793701\n",
      "Loss: 1.3382295370101929\n",
      "Loss: 1.6492629051208496\n",
      "Loss: 1.4417616128921509\n",
      "Loss: 1.578925609588623\n",
      "Loss: 1.6021395921707153\n",
      "Loss: 1.573384404182434\n",
      "Loss: 1.614251971244812\n",
      "Loss: 1.5287230014801025\n",
      "Loss: 1.605767011642456\n",
      "Loss: 1.5922834873199463\n",
      "Loss: 1.5023020505905151\n",
      "Loss: 1.5897899866104126\n",
      "Loss: 1.6713165044784546\n",
      "Loss: 1.4963496923446655\n",
      "Loss: 1.5939724445343018\n",
      "Loss: 1.537688136100769\n",
      "Loss: 1.5991227626800537\n",
      "Loss: 1.5547136068344116\n",
      "Loss: 1.5811593532562256\n",
      "Loss: 1.7833503484725952\n",
      "Loss: 1.4555660486221313\n",
      "Loss: 1.591063380241394\n",
      "Loss: 1.5137027502059937\n",
      "Loss: 1.367160439491272\n",
      "Loss: 1.2959015369415283\n",
      "Loss: 1.5772099494934082\n",
      "Loss: 1.6178503036499023\n",
      "Loss: 1.6101313829421997\n",
      "Loss: 1.5224809646606445\n",
      "Loss: 1.5360145568847656\n",
      "Loss: 1.4580786228179932\n",
      "Loss: 1.8025439977645874\n",
      "Loss: 1.4006925821304321\n",
      "Loss: 1.5741125345230103\n",
      "Loss: 1.3538310527801514\n",
      "Loss: 1.4688515663146973\n",
      "Loss: 1.4943941831588745\n",
      "Loss: 1.4495532512664795\n",
      "Loss: 1.6634716987609863\n",
      "Loss: 1.4460095167160034\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5307977199554443\n",
      "tensor([-0.1000])\n",
      "Loss: 2.622321367263794\n",
      "tensor([-0.1032])\n",
      "Loss: 2.4067304134368896\n",
      "tensor([-0.1000])\n",
      "Loss: 2.3277883529663086\n",
      "tensor([-0.1010])\n",
      "Loss: 2.3197245597839355\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.8731])\n",
      "Loss: 2.3492014408111572\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.5878])\n",
      "Loss: 2.348719358444214\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.3105])\n",
      "Loss: 2.166813373565674\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.0342])\n",
      "Loss: 2.177947521209717\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.7885])\n",
      "Loss: 2.269373893737793\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.5675])\n",
      "Loss: 2.2585909366607666\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.3655])\n",
      "Loss: 2.2392072677612305\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.1608])\n",
      "Loss: 2.0897912979125977\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.9748])\n",
      "Loss: 2.103684425354004\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.8039])\n",
      "Loss: 2.1360888481140137\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.6418])\n",
      "Loss: 2.218292236328125\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.4742])\n",
      "Loss: 2.0900697708129883\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.3223])\n",
      "Loss: 2.0272181034088135\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.1597])\n",
      "Loss: 1.9743233919143677\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.9955])\n",
      "Loss: 2.034900426864624\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.8335])\n",
      "Loss: 1.9810638427734375\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.6720])\n",
      "Loss: 1.8910317420959473\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.5074])\n",
      "Loss: 2.0135374069213867\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.3489])\n",
      "Loss: 1.8457380533218384\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.2013])\n",
      "Loss: 1.9447760581970215\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.0541])\n",
      "Loss: 2.004739761352539\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.9112])\n",
      "Loss: 1.9750561714172363\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.7759])\n",
      "Loss: 1.7867560386657715\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.6323])\n",
      "Loss: 1.8644882440567017\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.4786])\n",
      "Loss: 1.8320738077163696\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.3230])\n",
      "Loss: 1.9755641222000122\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.1712])\n",
      "Loss: 1.9309157133102417\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.0213])\n",
      "Loss: 2.0530545711517334\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.8765])\n",
      "Loss: 1.9800833463668823\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.7381])\n",
      "Loss: 1.7047858238220215\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.6065])\n",
      "Loss: 1.8452731370925903\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.4772])\n",
      "Loss: 1.6533136367797852\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.3459])\n",
      "Loss: 1.840152621269226\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.2153])\n",
      "Loss: 1.6760417222976685\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.0823])\n",
      "Loss: 1.7451521158218384\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.9460])\n",
      "Loss: 1.693416953086853\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.8077])\n",
      "Loss: 1.494986891746521\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.6735])\n",
      "Loss: 1.753210186958313\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.5419])\n",
      "Loss: 1.5180003643035889\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.4091])\n",
      "Loss: 1.6721594333648682\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.2776])\n",
      "Loss: 1.6411923170089722\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.1510])\n",
      "Loss: 1.5832370519638062\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.0256])\n",
      "Loss: 1.3768246173858643\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.9031])\n",
      "Loss: 1.5290902853012085\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7868])\n",
      "Loss: 1.4184075593948364\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.6690])\n",
      "Loss: 1.4902430772781372\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5591])\n",
      "Loss: 1.5443451404571533\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4461])\n",
      "Loss: 1.4915306568145752\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3382])\n",
      "Loss: 1.6413071155548096\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2291])\n",
      "Loss: 1.8974907398223877\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1191])\n",
      "Loss: 1.4150580167770386\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0113])\n",
      "Loss: 1.7076815366744995\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9067])\n",
      "Loss: 1.4174978733062744\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8048])\n",
      "Loss: 1.6221107244491577\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7094])\n",
      "Loss: 1.5703818798065186\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6189])\n",
      "Loss: 1.4224915504455566\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5225])\n",
      "Loss: 1.5231355428695679\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4201])\n",
      "Loss: 1.4247832298278809\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3251])\n",
      "Loss: 1.457269549369812\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2319])\n",
      "Loss: 1.56058669090271\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1304])\n",
      "Loss: 1.552995204925537\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0347])\n",
      "Loss: 1.368260383605957\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4128])\n",
      "Loss: 1.256722092628479\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5868])\n",
      "Loss: 1.2906670570373535\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.8292])\n",
      "Loss: 1.3668408393859863\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0963])\n",
      "Loss: 1.4586971998214722\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3485])\n",
      "Loss: 1.4023845195770264\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.6399])\n",
      "Loss: 1.3735829591751099\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9899])\n",
      "Loss: 1.4420684576034546\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3318])\n",
      "Loss: 1.365787386894226\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7070])\n",
      "Loss: 1.4473881721496582\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1334])\n",
      "Loss: 1.3877562284469604\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7545])\n",
      "Loss: 1.3599128723144531\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6056])\n",
      "Loss: 1.1702831983566284\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9311])\n",
      "Loss: 1.2519381046295166\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4322])\n",
      "Loss: 1.211711049079895\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0738])\n",
      "Loss: 1.4901217222213745\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8529])\n",
      "Loss: 1.2833759784698486\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7528])\n",
      "Loss: 1.2920864820480347\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.8025])\n",
      "Loss: 1.3263392448425293\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.6408])\n",
      "Loss: 1.359778642654419\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2994])\n",
      "Loss: 1.2932250499725342\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6389])\n",
      "Loss: 1.511567234992981\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7135])\n",
      "Loss: 1.364518642425537\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4052])\n",
      "Loss: 1.2104536294937134\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6198])\n",
      "Loss: 1.315014362335205\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3356])\n",
      "Loss: 1.1731579303741455\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7586])\n",
      "Loss: 1.1367048025131226\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0943])\n",
      "Loss: 1.366518259048462\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.8151])\n",
      "Loss: 1.3551809787750244\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7049])\n",
      "Loss: 1.2994863986968994\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4558])\n",
      "Loss: 1.1023433208465576\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1431])\n",
      "Loss: 1.1936382055282593\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9705])\n",
      "Loss: 1.1231491565704346\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1592])\n",
      "Loss: 1.2393217086791992\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4201])\n",
      "Loss: 1.2117081880569458\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6886])\n",
      "Loss: 1.3527462482452393\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6531])\n",
      "Loss: 1.3715028762817383\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1624])\n",
      "Loss: 1.2735058069229126\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9627])\n",
      "Loss: 1.201246738433838\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8921])\n",
      "Loss: 1.0848021507263184\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5495])\n",
      "Loss: 1.3113157749176025\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4255])\n",
      "Loss: 1.3299429416656494\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2860])\n",
      "Loss: 1.1593976020812988\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0879])\n",
      "Loss: 1.3905035257339478\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2639])\n",
      "Loss: 1.1199114322662354\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4911])\n",
      "Loss: 1.322480320930481\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7686])\n",
      "Loss: 1.0331621170043945\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0524])\n",
      "Loss: 1.4592065811157227\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5058])\n",
      "Loss: 1.0068243741989136\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3288])\n",
      "Loss: 1.199082612991333\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1952])\n",
      "Loss: 1.2374951839447021\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0877])\n",
      "Loss: 1.2298836708068848\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9497])\n",
      "Loss: 1.3555693626403809\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7753])\n",
      "Loss: 1.0008853673934937\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8732])\n",
      "Loss: 1.2735644578933716\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1780])\n",
      "Loss: 1.2872799634933472\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9508])\n",
      "Loss: 1.0307483673095703\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1362])\n",
      "Loss: 1.179654598236084\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9154])\n",
      "Loss: 1.1918562650680542\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0703])\n",
      "Loss: 1.020478367805481\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.6450])\n",
      "Loss: 0.9619939923286438\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8444])\n",
      "Loss: 1.3502577543258667\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7543])\n",
      "Loss: 0.9492108821868896\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3431])\n",
      "Loss: 1.1832506656646729\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9901])\n",
      "Loss: 0.950868546962738\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4152])\n",
      "Loss: 1.1121532917022705\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.2974])\n",
      "Loss: 1.2214783430099487\n",
      "tensor(1.00000e-04 *\n",
      "       [ 8.7198])\n",
      "Loss: 1.2478255033493042\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.8776])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.611220359802246\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3215])\n",
      "Loss: 1.068949580192566\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3813])\n",
      "Loss: 1.3698468208312988\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0570])\n",
      "Loss: 1.163787841796875\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6090])\n",
      "Loss: 1.500837802886963\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0828])\n",
      "Loss: 1.6064335107803345\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8043])\n",
      "Loss: 1.3276734352111816\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7705])\n",
      "Loss: 1.047562599182129\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1946])\n",
      "Loss: 1.214798092842102\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.3250])\n",
      "Loss: 1.2055529356002808\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.2009])\n",
      "Loss: 1.3080590963363647\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.6587])\n",
      "Loss: 1.1697635650634766\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.6362])\n",
      "Loss: 1.2595913410186768\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.3768])\n",
      "Loss: 1.1572777032852173\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.0096])\n",
      "Loss: 1.2675567865371704\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2080])\n",
      "Loss: 1.2254329919815063\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1495])\n",
      "Loss: 1.3725627660751343\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7331])\n",
      "Loss: 1.120773434638977\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9218])\n",
      "Loss: 1.2252458333969116\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5801])\n",
      "Loss: 1.6200495958328247\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4445])\n",
      "Loss: 1.3140650987625122\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3701])\n",
      "Loss: 1.3267643451690674\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7289])\n",
      "Loss: 1.2706319093704224\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0374])\n",
      "Loss: 1.051564335823059\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.3463])\n",
      "Loss: 1.457568645477295\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.5573])\n",
      "Loss: 1.2562308311462402\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.8343])\n",
      "Loss: 1.3485426902770996\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.6151])\n",
      "Loss: 1.0725089311599731\n",
      "tensor(1.00000e-03 *\n",
      "       [ 3.1053])\n",
      "Loss: 1.0665115118026733\n",
      "tensor(1.00000e-03 *\n",
      "       [ 3.1685])\n",
      "Loss: 1.3139419555664062\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.7796])\n",
      "Loss: 1.3406050205230713\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.0603])\n",
      "Loss: 1.0600510835647583\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0777])\n",
      "Loss: 1.3136450052261353\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.6148])\n",
      "Loss: 1.2196691036224365\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2179])\n",
      "Loss: 1.2919573783874512\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4834])\n",
      "Loss: 1.368374228477478\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5558])\n",
      "Loss: 1.4815007448196411\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2746])\n",
      "Loss: 1.4628273248672485\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7518])\n",
      "Loss: 1.287846326828003\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7590])\n",
      "Loss: 1.075969934463501\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2497])\n",
      "Loss: 0.9943026900291443\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3107])\n",
      "Loss: 1.1624101400375366\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0506])\n",
      "Loss: 1.2693703174591064\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.2167])\n",
      "Loss: 1.22963285446167\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.8414])\n",
      "Loss: 1.3169512748718262\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.5088])\n",
      "Loss: 0.9649247527122498\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.3931])\n",
      "Loss: 1.2786734104156494\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.8974])\n",
      "Loss: 1.187228798866272\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.7308])\n",
      "Loss: 1.2447947263717651\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.9802])\n",
      "Loss: 1.3932331800460815\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.4993])\n",
      "Loss: 0.9770522117614746\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.2057])\n",
      "Loss: 1.4443731307983398\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4779])\n",
      "Loss: 1.3751388788223267\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6512])\n",
      "Loss: 0.9802268147468567\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5951])\n",
      "Loss: 1.1568313837051392\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4303])\n",
      "Loss: 1.3004013299942017\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9211])\n",
      "Loss: 1.4408378601074219\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6236])\n",
      "Loss: 1.1063059568405151\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7766])\n",
      "Loss: 0.9489553570747375\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7631])\n",
      "Loss: 0.9966602921485901\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3824])\n",
      "Loss: 1.17925226688385\n",
      "tensor(1.00000e-05 *\n",
      "       [-8.5785])\n",
      "Loss: 1.1313432455062866\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0927])\n",
      "Loss: 1.2893136739730835\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.3690])\n",
      "Loss: 1.1760187149047852\n",
      "tensor(1.00000e-03 *\n",
      "       [ 3.0340])\n",
      "Loss: 1.3020319938659668\n",
      "tensor(1.00000e-03 *\n",
      "       [ 3.2611])\n",
      "Loss: 1.0214039087295532\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.5634])\n",
      "Loss: 1.2938313484191895\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.6195])\n",
      "Loss: 1.3502086400985718\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.4602])\n",
      "Loss: 1.343705177307129\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0280])\n",
      "Loss: 1.500480055809021\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4679])\n",
      "Loss: 1.1011180877685547\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8377])\n",
      "Loss: 1.0973182916641235\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8696])\n",
      "Loss: 1.4097627401351929\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7117])\n",
      "Loss: 1.2391117811203003\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2374])\n",
      "Loss: 1.1054930686950684\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3789])\n",
      "Loss: 1.516899824142456\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2147])\n",
      "Loss: 1.18687105178833\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.6000])\n",
      "Loss: 0.9172744154930115\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4933])\n",
      "Loss: 0.9447095990180969\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2587])\n",
      "Loss: 1.328000783920288\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7833])\n",
      "Loss: 1.0790445804595947\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.6169])\n",
      "Loss: 1.2275792360305786\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.7122])\n",
      "Loss: 1.4836844205856323\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0586])\n",
      "Loss: 1.0342531204223633\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.6195])\n",
      "Loss: 1.0372437238693237\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.0271])\n",
      "Loss: 1.3204426765441895\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.4242])\n",
      "Loss: 1.3062958717346191\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.3854])\n",
      "Loss: 1.0846291780471802\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.7544])\n",
      "Loss: 1.1400798559188843\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.2677])\n",
      "Loss: 1.0360759496688843\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.4006])\n",
      "Loss: 1.0765475034713745\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2394])\n",
      "Loss: 1.1635160446166992\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5641])\n",
      "Loss: 1.0953147411346436\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7654])\n",
      "Loss: 1.1273882389068604\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.6359])\n",
      "Loss: 1.1159740686416626\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0200])\n",
      "Loss: 1.2796865701675415\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0428])\n",
      "Loss: 1.1586858034133911\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3961])\n",
      "Loss: 1.266058087348938\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0953])\n",
      "Loss: 1.031570553779602\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5158])\n",
      "Loss: 1.2200907468795776\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.8756])\n",
      "Loss: 1.0191013813018799\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.6804])\n",
      "Loss: 1.1819744110107422\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.9500])\n",
      "Loss: 1.2092612981796265\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.9599])\n",
      "Loss: 1.479443073272705\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.7334])\n",
      "Loss: 1.150904655456543\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.0902])\n",
      "Loss: 1.1190085411071777\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0593])\n",
      "Loss: 1.3522833585739136\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.3898])\n",
      "Loss: 1.0975817441940308\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.3579])\n",
      "Loss: 1.5516787767410278\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.1591])\n",
      "Loss: 1.329158902168274\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4396])\n",
      "Loss: 1.418321132659912\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3273])\n",
      "Loss: 1.088884949684143\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2545])\n",
      "Loss: 1.206701397895813\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0635])\n",
      "Loss: 1.0615146160125732\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8895])\n",
      "Loss: 1.122676134109497\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.6435])\n",
      "Loss: 1.0304187536239624\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2799])\n",
      "Loss: 1.2391581535339355\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4774])\n",
      "Loss: 1.0713139772415161\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1301])\n",
      "Loss: 0.9759982824325562\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2200])\n",
      "Loss: 1.169317603111267\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2474])\n",
      "Loss: 0.9727141857147217\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9904])\n",
      "Loss: 1.1299494504928589\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7045])\n",
      "Loss: 0.977495014667511\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.4998])\n",
      "Loss: 0.904954195022583\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.1593])\n",
      "Loss: 1.1579097509384155\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.9563])\n",
      "Loss: 1.2920536994934082\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.1564])\n",
      "Loss: 1.1336723566055298\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.5310])\n",
      "Loss: 1.0398825407028198\n",
      "tensor(1.00000e-04 *\n",
      "       [ 8.4026])\n",
      "Loss: 1.1733636856079102\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.5226])\n",
      "Loss: 1.114740252494812\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5548])\n",
      "Loss: 1.1058589220046997\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7063])\n",
      "Loss: 1.3917737007141113\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7416])\n",
      "Loss: 1.1169345378875732\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5975])\n",
      "Loss: 1.1815558671951294\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4496])\n",
      "Loss: 1.1845563650131226\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9517])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2005417346954346\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3113])\n",
      "Loss: 1.2915048599243164\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4505])\n",
      "Loss: 1.1674519777297974\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1824])\n",
      "Loss: 1.0041338205337524\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5025])\n",
      "Loss: 1.2410303354263306\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5173])\n",
      "Loss: 1.1797305345535278\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4474])\n",
      "Loss: 1.3148618936538696\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3589])\n",
      "Loss: 1.0487557649612427\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3066])\n",
      "Loss: 1.1620583534240723\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.9474])\n",
      "Loss: 1.1833609342575073\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.1992])\n",
      "Loss: 1.3943617343902588\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.4471])\n",
      "Loss: 0.9734368324279785\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.9497])\n",
      "Loss: 1.1953359842300415\n",
      "tensor(1.00000e-04 *\n",
      "       [ 9.7303])\n",
      "Loss: 1.2459336519241333\n",
      "tensor(1.00000e-05 *\n",
      "       [ 8.0679])\n",
      "Loss: 1.14519202709198\n",
      "tensor(1.00000e-05 *\n",
      "       [-8.8484])\n",
      "Loss: 1.3013402223587036\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.8379])\n",
      "Loss: 1.2326427698135376\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.5095])\n",
      "Loss: 1.185225486755371\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6078])\n",
      "Loss: 1.3782588243484497\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3568])\n",
      "Loss: 1.1366240978240967\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9555])\n",
      "Loss: 1.0712131261825562\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3296])\n",
      "Loss: 1.2551459074020386\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3045])\n",
      "Loss: 1.1084277629852295\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2488])\n",
      "Loss: 1.09987473487854\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0926])\n",
      "Loss: 1.2987627983093262\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8105])\n",
      "Loss: 1.286655068397522\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7913])\n",
      "Loss: 1.1736412048339844\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5166])\n",
      "Loss: 1.2126708030700684\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2465])\n",
      "Loss: 1.2248544692993164\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1383])\n",
      "Loss: 1.154227375984192\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8299])\n",
      "Loss: 1.1580103635787964\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4156])\n",
      "Loss: 1.2139465808868408\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6613])\n",
      "Loss: 1.0789610147476196\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1787])\n",
      "Loss: 0.8879572153091431\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8766])\n",
      "Loss: 1.1638740301132202\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.4160])\n",
      "Loss: 1.1216849088668823\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.5383])\n",
      "Loss: 1.252472162246704\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.3570])\n",
      "Loss: 1.2187138795852661\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.6140])\n",
      "Loss: 1.1594263315200806\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9456])\n",
      "Loss: 1.16384756565094\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3698])\n",
      "Loss: 1.310121774673462\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5975])\n",
      "Loss: 1.4439377784729004\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4085])\n",
      "Loss: 1.1030327081680298\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5754])\n",
      "Loss: 1.129677653312683\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.4041])\n",
      "Loss: 1.2343803644180298\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.6198])\n",
      "Loss: 1.512083888053894\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.5239])\n",
      "Loss: 1.208141565322876\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.2853])\n",
      "Loss: 1.015101432800293\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7115])\n",
      "Loss: 1.178231120109558\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4148])\n",
      "Loss: 1.1881804466247559\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6401])\n",
      "Loss: 1.288750171661377\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5239])\n",
      "Loss: 1.4259483814239502\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8987])\n",
      "Loss: 1.2548104524612427\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7807])\n",
      "Loss: 1.0260608196258545\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8802])\n",
      "Loss: 1.0782482624053955\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6951])\n",
      "Loss: 1.1692267656326294\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2589])\n",
      "Loss: 0.9209554195404053\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.7035])\n",
      "Loss: 1.0193548202514648\n",
      "tensor(1.00000e-04 *\n",
      "       [ 6.8999])\n",
      "Loss: 1.1051018238067627\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.6289])\n",
      "Loss: 0.9836729168891907\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.4014])\n",
      "Loss: 1.3116836547851562\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.6274])\n",
      "Loss: 1.3213915824890137\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.9204])\n",
      "Loss: 1.0790159702301025\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9505])\n",
      "Loss: 1.2771331071853638\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9294])\n",
      "Loss: 1.1729367971420288\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9616])\n",
      "Loss: 1.2343509197235107\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5977])\n",
      "Loss: 1.1708221435546875\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9257])\n",
      "Loss: 1.0458500385284424\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9955])\n",
      "Loss: 1.0793192386627197\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8629])\n",
      "Loss: 1.1128684282302856\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1247])\n",
      "Loss: 1.2001539468765259\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8885])\n",
      "Loss: 1.002297282218933\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6775])\n",
      "Loss: 1.0422427654266357\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.8218])\n",
      "Loss: 1.1492738723754883\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.4092])\n",
      "Loss: 1.3083044290542603\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.6204])\n",
      "Loss: 1.2464120388031006\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.6084])\n",
      "Loss: 1.1536749601364136\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1005])\n",
      "Loss: 1.3662099838256836\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7037])\n",
      "Loss: 1.1611264944076538\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3156])\n",
      "Loss: 1.1335415840148926\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5957])\n",
      "Loss: 1.3777540922164917\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6815])\n",
      "Loss: 1.281803846359253\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5310])\n",
      "Loss: 1.4963974952697754\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2160])\n",
      "Loss: 1.4587640762329102\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4818])\n",
      "Loss: 1.2339268922805786\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3771])\n",
      "Loss: 1.2866820096969604\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.9031])\n",
      "Loss: 1.2607834339141846\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0598])\n",
      "Loss: 1.1395777463912964\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8082])\n",
      "Loss: 1.0858240127563477\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4723])\n",
      "Loss: 1.0722627639770508\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0529])\n",
      "Loss: 1.3770602941513062\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7618])\n",
      "Loss: 1.3692691326141357\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.5286])\n",
      "Loss: 1.2747642993927002\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.5207])\n",
      "Loss: 1.459678053855896\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.2614])\n",
      "Loss: 1.3779281377792358\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.2988])\n",
      "Loss: 0.9835483431816101\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0598])\n",
      "Loss: 1.4511741399765015\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.1674])\n",
      "Loss: 1.4190092086791992\n",
      "tensor(1.00000e-06 *\n",
      "       [-1.9795])\n",
      "Loss: 1.24844229221344\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2521])\n",
      "Loss: 1.070881724357605\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7007])\n",
      "Loss: 1.1533564329147339\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0295])\n",
      "Loss: 1.3931567668914795\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3041])\n",
      "Loss: 1.1067461967468262\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5794])\n",
      "Loss: 1.0845524072647095\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5487])\n",
      "Loss: 1.3073936700820923\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3080])\n",
      "Loss: 1.222888469696045\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7658])\n",
      "Loss: 1.183457374572754\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.9164])\n",
      "Loss: 1.0655118227005005\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8054])\n",
      "Loss: 1.0771845579147339\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4275])\n",
      "Loss: 1.2159610986709595\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7137])\n",
      "Loss: 1.5392974615097046\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.8242])\n",
      "Loss: 1.4120168685913086\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8729])\n",
      "Loss: 1.4626961946487427\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9330])\n",
      "Loss: 1.3273898363113403\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8333])\n",
      "Loss: 0.9874668717384338\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8382])\n",
      "Loss: 1.5252362489700317\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8735])\n",
      "Loss: 0.9426025152206421\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1527])\n",
      "Loss: 1.491216778755188\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.0921])\n",
      "Loss: 1.0474791526794434\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.1418])\n",
      "Loss: 0.8747737407684326\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.9575])\n",
      "Loss: 1.1583616733551025\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.2322])\n",
      "Loss: 1.1663671731948853\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.1109])\n",
      "Loss: 1.1022099256515503\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7042])\n",
      "Loss: 1.1904726028442383\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8119])\n",
      "Loss: 1.4581952095031738\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1608])\n",
      "Loss: 1.3654087781906128\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4064])\n",
      "Loss: 1.141476035118103\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1584])\n",
      "Loss: 1.324074149131775\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4712])\n",
      "Loss: 1.3596230745315552\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2368])\n",
      "Loss: 1.177210807800293\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4759])\n",
      "Loss: 1.1336995363235474\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2756])\n",
      "Loss: 1.2201852798461914\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8339])\n",
      "Loss: 1.2271174192428589\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7236])\n",
      "Loss: 1.043127417564392\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1818])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2758526802062988\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.6351])\n",
      "Loss: 1.3784838914871216\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.9785])\n",
      "Loss: 1.4601374864578247\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.6726])\n",
      "Loss: 1.3185627460479736\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0712])\n",
      "Loss: 1.1767839193344116\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2713])\n",
      "Loss: 1.2558155059814453\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3839])\n",
      "Loss: 1.50919508934021\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2278])\n",
      "Loss: 1.5236337184906006\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0987])\n",
      "Loss: 1.2066893577575684\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5352])\n",
      "Loss: 1.2618050575256348\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.8705])\n",
      "Loss: 1.1846778392791748\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1731])\n",
      "Loss: 1.3614072799682617\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7145])\n",
      "Loss: 1.188896656036377\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1742])\n",
      "Loss: 1.2866969108581543\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7476])\n",
      "Loss: 1.3850998878479004\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.2913])\n",
      "Loss: 1.2575024366378784\n",
      "tensor(1.00000e-04 *\n",
      "       [ 4.4827])\n",
      "Loss: 1.376704454421997\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.1328])\n",
      "Loss: 1.115574598312378\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2753])\n",
      "Loss: 1.3411822319030762\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4987])\n",
      "Loss: 1.028604507446289\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6359])\n",
      "Loss: 1.213713526725769\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5015])\n",
      "Loss: 1.2786781787872314\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8234])\n",
      "Loss: 1.1714874505996704\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5746])\n",
      "Loss: 1.0884430408477783\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8627])\n",
      "Loss: 1.2443767786026\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9835])\n",
      "Loss: 1.0712565183639526\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0826])\n",
      "Loss: 1.0369354486465454\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2452])\n",
      "Loss: 1.6255416870117188\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3243])\n",
      "Loss: 1.1652544736862183\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2272])\n",
      "Loss: 1.2891006469726562\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1984])\n",
      "Loss: 1.3557941913604736\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2390])\n",
      "Loss: 1.0350186824798584\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7940])\n",
      "Loss: 1.405246615409851\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4973])\n",
      "Loss: 1.1545689105987549\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5866])\n",
      "Loss: 1.0074851512908936\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4162])\n",
      "Loss: 1.202305793762207\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9513])\n",
      "Loss: 0.9985321164131165\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1437])\n",
      "Loss: 1.2725749015808105\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8361])\n",
      "Loss: 1.1769999265670776\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5496])\n",
      "Loss: 1.272742509841919\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9132])\n",
      "Loss: 1.2188870906829834\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0485])\n",
      "Loss: 1.2040190696716309\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1125])\n",
      "Loss: 1.3943307399749756\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4803])\n",
      "Loss: 1.557337760925293\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2210])\n",
      "Loss: 1.2469481229782104\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3867])\n",
      "Loss: 1.0250190496444702\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3402])\n",
      "Loss: 1.051547646522522\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9046])\n",
      "Loss: 1.243836522102356\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1209])\n",
      "Loss: 1.0038114786148071\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7371])\n",
      "Loss: 1.4757298231124878\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5701])\n",
      "Loss: 1.270613670349121\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9048])\n",
      "Loss: 1.0032973289489746\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3281])\n",
      "Loss: 1.1631180047988892\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5696])\n",
      "Loss: 1.3903487920761108\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8265])\n",
      "Loss: 0.9319853782653809\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.7872])\n",
      "Loss: 1.261741280555725\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.2888])\n",
      "Loss: 1.3898615837097168\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.3348])\n",
      "Loss: 1.5601016283035278\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4257])\n",
      "Loss: 1.4109699726104736\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4777])\n",
      "Loss: 1.146242380142212\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4773])\n",
      "Loss: 1.1675792932510376\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0670])\n",
      "Loss: 1.459080457687378\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3431])\n",
      "Loss: 1.4454818964004517\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3133])\n",
      "Loss: 1.4769939184188843\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6758])\n",
      "Loss: 1.3123594522476196\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.5647])\n",
      "Loss: 1.2441037893295288\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7118])\n",
      "Loss: 1.1936726570129395\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5856])\n",
      "Loss: 1.0917057991027832\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2063])\n",
      "Loss: 1.33563232421875\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8601])\n",
      "Loss: 1.430977463722229\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5623])\n",
      "Loss: 1.1723830699920654\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4153])\n",
      "Loss: 1.1580766439437866\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3393])\n",
      "Loss: 1.2399239540100098\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.6617])\n",
      "Loss: 1.325455665588379\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.4003])\n",
      "Loss: 1.3823132514953613\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.1853])\n",
      "Loss: 1.1414422988891602\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1905])\n",
      "Loss: 2.1092636585235596\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4743])\n",
      "Loss: 0.9795600771903992\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9733])\n",
      "Loss: 1.5495415925979614\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2536])\n",
      "Loss: 1.232400894165039\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4284])\n",
      "Loss: 1.0794304609298706\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4398])\n",
      "Loss: 1.3306667804718018\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3533])\n",
      "Loss: 1.3481019735336304\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1004])\n",
      "Loss: 1.4508095979690552\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6102])\n",
      "Loss: 1.4875456094741821\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7953])\n",
      "Loss: 1.7726378440856934\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.5436])\n",
      "Loss: 1.435125708580017\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1379])\n",
      "Loss: 1.2903084754943848\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3921])\n",
      "Loss: 1.0375778675079346\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.2880])\n",
      "Loss: 1.2348580360412598\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9996])\n",
      "Loss: 0.982859194278717\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6035])\n",
      "Loss: 1.2691571712493896\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3682])\n",
      "Loss: 1.1774201393127441\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2392])\n",
      "Loss: 1.7654640674591064\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2482])\n",
      "Loss: 1.4107273817062378\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.0660])\n",
      "Loss: 1.4406017065048218\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.8252])\n",
      "Loss: 1.2643226385116577\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.9420])\n",
      "Loss: 1.1940126419067383\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5856])\n",
      "Loss: 1.120582103729248\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1867])\n",
      "Loss: 1.2489616870880127\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9508])\n",
      "Loss: 1.3905543088912964\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8758])\n",
      "Loss: 1.601912021636963\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6756])\n",
      "Loss: 1.0618891716003418\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5590])\n",
      "Loss: 1.137445092201233\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3452])\n",
      "Loss: 1.1336467266082764\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9374])\n",
      "Loss: 1.2884135246276855\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3956])\n",
      "Loss: 1.2055513858795166\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4392])\n",
      "Loss: 1.324101448059082\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1859])\n",
      "Loss: 1.3733090162277222\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4794])\n",
      "Loss: 1.1153885126113892\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2997])\n",
      "Loss: 1.3674067258834839\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0608])\n",
      "Loss: 1.3196606636047363\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6868])\n",
      "Loss: 1.1816318035125732\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5037])\n",
      "Loss: 1.0882203578948975\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.4402])\n",
      "Loss: 1.2830685377120972\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.2719])\n",
      "Loss: 1.3913397789001465\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2372])\n",
      "Loss: 1.1538593769073486\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8334])\n",
      "Loss: 1.1104280948638916\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3990])\n",
      "Loss: 1.2124005556106567\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7305])\n",
      "Loss: 1.2304482460021973\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0098])\n",
      "Loss: 1.2377092838287354\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.9576])\n",
      "Loss: 1.1295506954193115\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7085])\n",
      "Loss: 1.3022756576538086\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.2561])\n",
      "Loss: 1.4583145380020142\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3957])\n",
      "Loss: 1.207370638847351\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1070])\n",
      "Loss: 1.26779043674469\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4537])\n",
      "Loss: 1.4980981349945068\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.2441])\n",
      "Loss: 1.2157095670700073\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7881])\n",
      "Loss: 1.1319804191589355\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3061])\n",
      "Loss: 1.0517692565917969\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7033])\n",
      "Loss: 1.549872875213623\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2569])\n",
      "Loss: 1.1756646633148193\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.2238])\n",
      "Loss: 1.1103636026382446\n",
      "tensor(1.00000e-04 *\n",
      "       [ 7.8930])\n",
      "Loss: 1.1834336519241333\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.5760])\n",
      "Loss: 1.2195258140563965\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4068])\n",
      "Loss: 1.3057200908660889\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5550])\n",
      "Loss: 1.2826876640319824\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6903])\n",
      "Loss: 1.0854638814926147\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9262])\n",
      "Loss: 1.1451449394226074\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9980])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3048418760299683\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0060])\n",
      "Loss: 1.1744650602340698\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7949])\n",
      "Loss: 1.5569287538528442\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5059])\n",
      "Loss: 1.0844371318817139\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.9414])\n",
      "Loss: 1.3887611627578735\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1041])\n",
      "Loss: 1.3032950162887573\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1204])\n",
      "Loss: 1.5156968832015991\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7626])\n",
      "Loss: 1.2857452630996704\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1870])\n",
      "Loss: 1.4450812339782715\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3384])\n",
      "Loss: 1.0133841037750244\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3220])\n",
      "Loss: 1.5994927883148193\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2030])\n",
      "Loss: 1.1177027225494385\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0868])\n",
      "Loss: 1.1413968801498413\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9405])\n",
      "Loss: 1.2888699769973755\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9067])\n",
      "Loss: 1.020195722579956\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.8387])\n",
      "Loss: 1.0561456680297852\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.5323])\n",
      "Loss: 1.1646665334701538\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.5063])\n",
      "Loss: 1.163540244102478\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.4885])\n",
      "Loss: 1.0277693271636963\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.4337])\n",
      "Loss: 1.5223997831344604\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9146])\n",
      "Loss: 1.2557452917099\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3859])\n",
      "Loss: 1.2955219745635986\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8334])\n",
      "Loss: 1.2647815942764282\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0798])\n",
      "Loss: 1.1040900945663452\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.2214])\n",
      "Loss: 1.3962188959121704\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1484])\n",
      "Loss: 1.4097073078155518\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8459])\n",
      "Loss: 1.1936930418014526\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3912])\n",
      "Loss: 1.420655369758606\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7218])\n",
      "Loss: 0.9880724549293518\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.5649])\n",
      "Loss: 1.3343329429626465\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.0569])\n",
      "Loss: 1.1998826265335083\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2169])\n",
      "Loss: 0.9405947327613831\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0888])\n",
      "Loss: 1.0112205743789673\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8381])\n",
      "Loss: 1.2571327686309814\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4570])\n",
      "Loss: 1.0153712034225464\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1323])\n",
      "Loss: 1.175100564956665\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8069])\n",
      "Loss: 1.6119647026062012\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.8732])\n",
      "Loss: 0.9556637406349182\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.2030])\n",
      "Loss: 1.6078723669052124\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.4506])\n",
      "Loss: 1.0729197263717651\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.2021])\n",
      "Loss: 1.021224856376648\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2330])\n",
      "Loss: 1.067552089691162\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8716])\n",
      "Loss: 1.2530441284179688\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3846])\n",
      "Loss: 0.992125928401947\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7420])\n",
      "Loss: 1.3624142408370972\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.9909])\n",
      "Loss: 1.0702104568481445\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1624])\n",
      "Loss: 1.3388372659683228\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0191])\n",
      "Loss: 1.1730990409851074\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1075])\n",
      "Loss: 1.451034426689148\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1831])\n",
      "Loss: 1.4054979085922241\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2408])\n",
      "Loss: 1.4365627765655518\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2751])\n",
      "Loss: 1.5254380702972412\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2826])\n",
      "Loss: 1.381832480430603\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2645])\n",
      "Loss: 1.4426078796386719\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2178])\n",
      "Loss: 1.4339499473571777\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1377])\n",
      "Loss: 1.1607917547225952\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0330])\n",
      "Loss: 1.1742801666259766\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1457])\n",
      "Loss: 1.235689401626587\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.8310])\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    print(a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

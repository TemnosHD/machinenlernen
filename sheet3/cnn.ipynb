{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3650574684143066\n",
      "Loss: 2.4331960678100586\n",
      "Loss: 2.265251398086548\n",
      "Loss: 2.1515004634857178\n",
      "Loss: 2.122255563735962\n",
      "Loss: 2.0780086517333984\n",
      "Loss: 1.9510952234268188\n",
      "Loss: 1.7534270286560059\n",
      "Loss: 1.7162812948226929\n",
      "Loss: 1.6557480096817017\n",
      "Loss: 1.5992196798324585\n",
      "Loss: 1.4189345836639404\n",
      "Loss: 1.5806589126586914\n",
      "Loss: 1.5623953342437744\n",
      "Loss: 1.2064529657363892\n",
      "Loss: 1.1244980096817017\n",
      "Loss: 1.1663738489151\n",
      "Loss: 1.2691872119903564\n",
      "Loss: 1.3251729011535645\n",
      "Loss: 0.9300680756568909\n",
      "Loss: 0.9207238554954529\n",
      "Loss: 0.9960853457450867\n",
      "Loss: 0.8971188068389893\n",
      "Loss: 0.9928513169288635\n",
      "Loss: 0.9217621088027954\n",
      "Loss: 1.1288933753967285\n",
      "Loss: 0.9542943835258484\n",
      "Loss: 0.7582323551177979\n",
      "Loss: 1.0451061725616455\n",
      "Loss: 1.059401512145996\n",
      "Loss: 0.8125346302986145\n",
      "Loss: 0.8085698485374451\n",
      "Loss: 0.8077687621116638\n",
      "Loss: 0.7307479381561279\n",
      "Loss: 0.9974827766418457\n",
      "Loss: 0.8421211242675781\n",
      "Loss: 1.032852053642273\n",
      "Loss: 0.8811557292938232\n",
      "Loss: 1.1567177772521973\n",
      "Loss: 1.06706702709198\n",
      "Loss: 0.8559035658836365\n",
      "Loss: 1.0791888236999512\n",
      "Loss: 0.8036449551582336\n",
      "Loss: 0.6844608187675476\n",
      "Loss: 0.9883559942245483\n",
      "Loss: 1.1506540775299072\n",
      "Loss: 1.0397189855575562\n",
      "Loss: 0.6055870056152344\n",
      "Loss: 0.7405663132667542\n",
      "Loss: 0.9825933575630188\n",
      "Loss: 0.7988519072532654\n",
      "Loss: 0.9820525646209717\n",
      "Loss: 0.8285650610923767\n",
      "Loss: 1.0368586778640747\n",
      "Loss: 1.0683485269546509\n",
      "Loss: 1.154941201210022\n",
      "Loss: 0.9600487351417542\n",
      "Loss: 0.6293689608573914\n",
      "Loss: 0.564500093460083\n",
      "Loss: 0.8129035234451294\n",
      "Loss: 0.6928654313087463\n",
      "Loss: 0.6508433818817139\n",
      "Loss: 0.7288529872894287\n",
      "Loss: 0.6771632432937622\n",
      "Loss: 0.6286119818687439\n",
      "Loss: 0.9556434154510498\n",
      "Loss: 0.8439106941223145\n",
      "Loss: 0.7661811113357544\n",
      "Loss: 0.5859904885292053\n",
      "Loss: 0.6611115336418152\n",
      "Loss: 0.7714277505874634\n",
      "Loss: 0.632220447063446\n",
      "Loss: 0.7729300856590271\n",
      "Loss: 0.6190295219421387\n",
      "Loss: 0.8990380764007568\n",
      "Loss: 0.6540015339851379\n",
      "Loss: 0.6201560497283936\n",
      "Loss: 0.7160980105400085\n",
      "Loss: 0.6847814917564392\n",
      "Loss: 0.5895481705665588\n",
      "Loss: 0.6408910155296326\n",
      "Loss: 0.6109510660171509\n",
      "Loss: 0.6835634708404541\n",
      "Loss: 0.692812979221344\n",
      "Loss: 0.5034008622169495\n",
      "Loss: 0.5447413921356201\n",
      "Loss: 0.7732231020927429\n",
      "Loss: 0.791365385055542\n",
      "Loss: 0.7652235627174377\n",
      "Loss: 0.8104759454727173\n",
      "Loss: 0.6577518582344055\n",
      "Loss: 0.6512376666069031\n",
      "Loss: 0.6451898217201233\n",
      "Loss: 0.684539258480072\n",
      "Loss: 0.5042752623558044\n",
      "Loss: 0.7702072858810425\n",
      "Loss: 0.5851987600326538\n",
      "Loss: 0.5781981945037842\n",
      "Loss: 0.6168696880340576\n",
      "Loss: 0.7753382325172424\n",
      "Loss: 0.4587059020996094\n",
      "Loss: 0.6291112899780273\n",
      "Loss: 0.533973753452301\n",
      "Loss: 0.7789945006370544\n",
      "Loss: 0.6855491399765015\n",
      "Loss: 0.6686776876449585\n",
      "Loss: 0.5873626470565796\n",
      "Loss: 0.33462753891944885\n",
      "Loss: 0.6334778070449829\n",
      "Loss: 0.6278411746025085\n",
      "Loss: 0.6651527285575867\n",
      "Loss: 0.5597331523895264\n",
      "Loss: 0.45526495575904846\n",
      "Loss: 0.5389357209205627\n",
      "Loss: 0.5272625088691711\n",
      "Loss: 0.4685162603855133\n",
      "Loss: 0.7639849781990051\n",
      "Loss: 0.6656653881072998\n",
      "Loss: 0.6456759572029114\n",
      "Loss: 0.4453032612800598\n",
      "Loss: 0.43424704670906067\n",
      "Loss: 0.5852906107902527\n",
      "Loss: 0.47679123282432556\n",
      "Loss: 0.7452694177627563\n",
      "Loss: 0.7225393056869507\n",
      "Loss: 0.7032426595687866\n",
      "Loss: 0.4811435341835022\n",
      "Loss: 0.6189815998077393\n",
      "Loss: 0.7163064479827881\n",
      "Loss: 0.6587616205215454\n",
      "Loss: 0.5449330806732178\n",
      "Loss: 0.6396982073783875\n",
      "Loss: 0.4499250650405884\n",
      "Loss: 0.6364572048187256\n",
      "Loss: 0.37947237491607666\n",
      "Loss: 0.4630916714668274\n",
      "Loss: 0.5744653344154358\n",
      "Loss: 0.6387542486190796\n",
      "Loss: 0.5287184715270996\n",
      "Loss: 0.6385812759399414\n",
      "Loss: 0.5127446055412292\n",
      "Loss: 0.5618216395378113\n",
      "Loss: 0.7225258350372314\n",
      "Loss: 0.45201846957206726\n",
      "Loss: 0.721106767654419\n",
      "Loss: 0.6852401494979858\n",
      "Loss: 0.37845003604888916\n",
      "Loss: 0.5617449283599854\n",
      "Loss: 0.36711472272872925\n",
      "Loss: 0.6054775714874268\n",
      "Loss: 0.47314757108688354\n",
      "Loss: 0.46259385347366333\n",
      "Loss: 0.412568062543869\n",
      "Loss: 0.3601912260055542\n",
      "Loss: 0.7341428399085999\n",
      "Loss: 0.553292989730835\n",
      "Loss: 0.34219038486480713\n",
      "Loss: 0.6733389496803284\n",
      "Loss: 0.6352821588516235\n",
      "Loss: 0.5677583813667297\n",
      "Loss: 0.3381854295730591\n",
      "Loss: 0.44535884261131287\n",
      "Loss: 0.6133540868759155\n",
      "Loss: 0.5945792198181152\n",
      "Loss: 0.6529228091239929\n",
      "Loss: 0.6352332830429077\n",
      "Loss: 0.5425738096237183\n",
      "Loss: 0.4083259105682373\n",
      "Loss: 0.509526252746582\n",
      "Loss: 0.5691691040992737\n",
      "Loss: 0.22077573835849762\n",
      "Loss: 0.9056119322776794\n",
      "Loss: 0.537041187286377\n",
      "Loss: 0.3875855505466461\n",
      "Loss: 0.6468822956085205\n",
      "Loss: 0.4897061288356781\n",
      "Loss: 0.6064438819885254\n",
      "Loss: 0.347486674785614\n",
      "Loss: 0.3994227945804596\n",
      "Loss: 0.47996777296066284\n",
      "Loss: 0.25438565015792847\n",
      "Loss: 0.6974520087242126\n",
      "Loss: 0.6072283387184143\n",
      "Loss: 0.5223544239997864\n",
      "Loss: 0.4216822385787964\n",
      "Loss: 0.4069603979587555\n",
      "Loss: 0.48031002283096313\n",
      "Loss: 0.4957202076911926\n",
      "Loss: 0.5229082703590393\n",
      "Loss: 0.4940219223499298\n",
      "Loss: 0.69359290599823\n",
      "Loss: 0.26708799600601196\n",
      "Loss: 0.42114636301994324\n",
      "Loss: 0.42234596610069275\n",
      "Loss: 0.6391850709915161\n",
      "Loss: 0.44184571504592896\n",
      "Loss: 0.6529497504234314\n",
      "Loss: 0.4917890429496765\n",
      "Loss: 0.4363159239292145\n",
      "Loss: 0.570524275302887\n",
      "Loss: 0.5075458288192749\n",
      "Loss: 0.47657641768455505\n",
      "Loss: 0.5528730154037476\n",
      "Loss: 0.28546473383903503\n",
      "Loss: 0.29306769371032715\n",
      "Loss: 0.6023327708244324\n",
      "Loss: 0.4849349856376648\n",
      "Loss: 0.7168842554092407\n",
      "Loss: 0.5415575504302979\n",
      "Loss: 0.6548671722412109\n",
      "Loss: 0.2951251268386841\n",
      "Loss: 0.18846163153648376\n",
      "Loss: 0.4648306369781494\n",
      "Loss: 0.3907495141029358\n",
      "Loss: 0.5814256072044373\n",
      "Loss: 0.3388889729976654\n",
      "Loss: 0.47552451491355896\n",
      "Loss: 1.1211563348770142\n",
      "Loss: 0.48804762959480286\n",
      "Loss: 0.5485422611236572\n",
      "Loss: 0.6147031188011169\n",
      "Loss: 0.5625352263450623\n",
      "Loss: 0.5146719217300415\n",
      "Loss: 0.42221206426620483\n",
      "Loss: 0.3585098683834076\n",
      "Loss: 0.6060096025466919\n",
      "Loss: 0.23304355144500732\n",
      "Loss: 0.5501836538314819\n",
      "Loss: 0.5149670243263245\n",
      "Loss: 0.40778055787086487\n",
      "Loss: 0.590813159942627\n",
      "Loss: 0.3336839973926544\n",
      "Loss: 0.4750367999076843\n",
      "Loss: 0.719468891620636\n",
      "Loss: 0.4458407461643219\n",
      "Loss: 0.6704366207122803\n",
      "Loss: 0.4697405993938446\n",
      "Loss: 0.41843825578689575\n",
      "Loss: 0.4971904754638672\n",
      "Loss: 0.30161431431770325\n",
      "Loss: 0.4082678258419037\n",
      "Loss: 0.47146713733673096\n",
      "Loss: 0.3059348165988922\n",
      "Loss: 0.32713109254837036\n",
      "Loss: 0.45287859439849854\n",
      "Loss: 0.4473413825035095\n",
      "Loss: 0.5287097692489624\n",
      "Loss: 0.4739595651626587\n",
      "Loss: 0.3712783753871918\n",
      "Loss: 0.4780649244785309\n",
      "Loss: 0.2829945385456085\n",
      "Loss: 0.2540278732776642\n",
      "Loss: 0.3392035961151123\n",
      "Loss: 0.39597782492637634\n",
      "Loss: 0.31442761421203613\n",
      "Loss: 0.2663291096687317\n",
      "Loss: 0.3821934461593628\n",
      "Loss: 0.3601236045360565\n",
      "Loss: 0.5823522806167603\n",
      "Loss: 0.39646485447883606\n",
      "Loss: 0.504083514213562\n",
      "Loss: 0.37505611777305603\n",
      "Loss: 0.5243967175483704\n",
      "Loss: 0.3995676040649414\n",
      "Loss: 0.5160486698150635\n",
      "Loss: 0.389894038438797\n",
      "Loss: 0.5172825455665588\n",
      "Loss: 0.3714984655380249\n",
      "Loss: 0.5091200470924377\n",
      "Loss: 0.37327101826667786\n",
      "Loss: 0.4434426426887512\n",
      "Loss: 0.7227074503898621\n",
      "Loss: 0.499289870262146\n",
      "Loss: 0.7013443112373352\n",
      "Loss: 0.3415789306163788\n",
      "Loss: 0.638639509677887\n",
      "Loss: 0.6647087335586548\n",
      "Loss: 0.670081615447998\n",
      "Loss: 0.4969065487384796\n",
      "Loss: 0.7542670369148254\n",
      "Loss: 0.35829639434814453\n",
      "Loss: 0.43808040022850037\n",
      "Loss: 0.3624533712863922\n",
      "Loss: 0.3744394779205322\n",
      "Loss: 0.49683624505996704\n",
      "Loss: 0.5152036547660828\n",
      "Loss: 0.5353504419326782\n",
      "Loss: 0.623234748840332\n",
      "Loss: 0.4849754571914673\n",
      "Loss: 0.4262237250804901\n",
      "Loss: 0.4175715744495392\n",
      "Loss: 0.36343488097190857\n",
      "Loss: 0.6033088564872742\n",
      "Loss: 0.6853214502334595\n",
      "Loss: 0.6239199638366699\n",
      "Loss: 0.3933919072151184\n",
      "Loss: 0.2755245864391327\n",
      "Loss: 0.4803830087184906\n",
      "Loss: 0.3743283450603485\n",
      "Loss: 0.4452108144760132\n",
      "Loss: 0.4996219277381897\n",
      "Loss: 0.5498732924461365\n",
      "Loss: 0.5256993770599365\n",
      "Loss: 0.4069792926311493\n",
      "Loss: 0.5765613913536072\n",
      "Loss: 0.36130088567733765\n",
      "Loss: 0.4936230480670929\n",
      "Loss: 0.6213726997375488\n",
      "Loss: 0.5142908692359924\n",
      "Loss: 0.6712819933891296\n",
      "Loss: 0.3642176687717438\n",
      "Loss: 0.5157058238983154\n",
      "Loss: 0.3666820526123047\n",
      "Loss: 0.43993499875068665\n",
      "Loss: 0.42840179800987244\n",
      "Loss: 0.51747065782547\n",
      "Loss: 0.2857414782047272\n",
      "Loss: 0.4117826521396637\n",
      "Loss: 0.3992210626602173\n",
      "Loss: 0.6473966836929321\n",
      "Loss: 0.3458259701728821\n",
      "Loss: 0.3499215245246887\n",
      "Loss: 0.3012178838253021\n",
      "Loss: 0.293722927570343\n",
      "Loss: 0.4627702236175537\n",
      "Loss: 0.3555641174316406\n",
      "Loss: 0.38779008388519287\n",
      "Loss: 0.19039921462535858\n",
      "Loss: 0.3846890330314636\n",
      "Loss: 0.40622252225875854\n",
      "Loss: 0.30134662985801697\n",
      "Loss: 0.5024200081825256\n",
      "Loss: 0.5311226844787598\n",
      "Loss: 0.6692376136779785\n",
      "Loss: 0.29277074337005615\n",
      "Loss: 0.2722848057746887\n",
      "Loss: 0.2279292643070221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5127851366996765\n",
      "Loss: 0.5655394196510315\n",
      "Loss: 0.34945568442344666\n",
      "Loss: 0.38680610060691833\n",
      "Loss: 0.3107341229915619\n",
      "Loss: 0.7074310183525085\n",
      "Loss: 0.3330828547477722\n",
      "Loss: 0.676852822303772\n",
      "Loss: 0.49547135829925537\n",
      "Loss: 0.6491640210151672\n",
      "Loss: 0.4973950982093811\n",
      "Loss: 0.4866896867752075\n",
      "Loss: 0.5007063150405884\n",
      "Loss: 0.3366341292858124\n",
      "Loss: 0.8163063526153564\n",
      "Loss: 0.664827287197113\n",
      "Loss: 0.38241255283355713\n",
      "Loss: 0.43316036462783813\n",
      "Loss: 0.8723129034042358\n",
      "Loss: 0.8584028482437134\n",
      "Loss: 0.639012336730957\n",
      "Loss: 0.7487651109695435\n",
      "Loss: 0.7680003643035889\n",
      "Loss: 0.779984712600708\n",
      "Loss: 0.4549174904823303\n",
      "Loss: 0.5873608589172363\n",
      "Loss: 0.28681111335754395\n",
      "Loss: 0.3197105824947357\n",
      "Loss: 0.3368431031703949\n",
      "Loss: 0.533292293548584\n",
      "Loss: 0.5929557085037231\n",
      "Loss: 0.4101201593875885\n",
      "Loss: 0.4817691743373871\n",
      "Loss: 0.25115570425987244\n",
      "Loss: 0.45354145765304565\n",
      "Loss: 0.5120304226875305\n",
      "Loss: 0.3162699341773987\n",
      "Loss: 0.42274218797683716\n",
      "Loss: 0.5852655172348022\n",
      "Loss: 0.5165359973907471\n",
      "Loss: 0.3206394612789154\n",
      "Loss: 0.5534145832061768\n",
      "Loss: 0.6419504284858704\n",
      "Loss: 0.47534045577049255\n",
      "Loss: 0.4604980945587158\n",
      "Loss: 0.5247328281402588\n",
      "Loss: 0.5915282964706421\n",
      "Loss: 0.5447443127632141\n",
      "Loss: 0.6337142586708069\n",
      "Loss: 0.6616935133934021\n",
      "Loss: 0.36085402965545654\n",
      "Loss: 0.46135643124580383\n",
      "Loss: 0.49538353085517883\n",
      "Loss: 0.33782854676246643\n",
      "Loss: 0.4512167274951935\n",
      "Loss: 0.5049334168434143\n",
      "Loss: 0.3554404377937317\n",
      "Loss: 0.29196086525917053\n",
      "Loss: 0.6224295496940613\n",
      "Loss: 0.41949915885925293\n",
      "Loss: 0.3771205544471741\n",
      "Loss: 0.42212650179862976\n",
      "Loss: 0.38212573528289795\n",
      "Loss: 0.4919183850288391\n",
      "Loss: 0.4852111041545868\n",
      "Loss: 0.35674378275871277\n",
      "Loss: 0.38559815287590027\n",
      "Loss: 0.5891015529632568\n",
      "Loss: 0.43118804693222046\n",
      "Loss: 0.4804955720901489\n",
      "Loss: 0.7766368985176086\n",
      "Loss: 0.41090676188468933\n",
      "Loss: 0.5626564025878906\n",
      "Loss: 0.4092235267162323\n",
      "Loss: 0.3006955683231354\n",
      "Loss: 0.5894263386726379\n",
      "Loss: 0.5752329230308533\n",
      "Loss: 0.46727830171585083\n",
      "Loss: 0.6244351863861084\n",
      "Loss: 0.3917973041534424\n",
      "Loss: 0.4301028549671173\n",
      "Loss: 0.6796953082084656\n",
      "Loss: 0.25053903460502625\n",
      "Loss: 0.2772796154022217\n",
      "Loss: 0.5091433525085449\n",
      "Loss: 0.36742374300956726\n",
      "Loss: 0.5138311386108398\n",
      "Loss: 0.7410765886306763\n",
      "Loss: 0.7004266977310181\n",
      "Loss: 0.40223896503448486\n",
      "Loss: 0.3367856740951538\n",
      "Loss: 0.6097464561462402\n",
      "Loss: 0.4221581220626831\n",
      "Loss: 0.33417820930480957\n",
      "Loss: 0.3148976266384125\n",
      "Loss: 0.6042286157608032\n",
      "Loss: 0.6066534519195557\n",
      "Loss: 0.28491854667663574\n",
      "Loss: 0.5514196157455444\n",
      "Loss: 0.46771249175071716\n",
      "Loss: 0.5538370013237\n",
      "Loss: 0.48993366956710815\n",
      "Loss: 0.2602291405200958\n",
      "Loss: 0.5345669388771057\n",
      "Loss: 0.5451562404632568\n",
      "Loss: 0.390532523393631\n",
      "Loss: 0.6315757036209106\n",
      "Loss: 0.4599429965019226\n",
      "Loss: 0.3483264446258545\n",
      "Loss: 0.44473227858543396\n",
      "Loss: 0.33266207575798035\n",
      "Loss: 0.4077163338661194\n",
      "Loss: 0.4727291166782379\n",
      "Loss: 0.3664938807487488\n",
      "Loss: 0.5129612684249878\n",
      "Loss: 0.27089327573776245\n",
      "Loss: 0.36887723207473755\n",
      "Loss: 0.5765495896339417\n",
      "Loss: 0.3422292470932007\n",
      "Loss: 0.5155162215232849\n",
      "Loss: 0.5142764449119568\n",
      "Loss: 0.40369048714637756\n",
      "Loss: 0.4797927737236023\n",
      "Loss: 0.3323989510536194\n",
      "Loss: 0.3206058442592621\n",
      "Loss: 0.42555567622184753\n",
      "Loss: 0.4186936616897583\n",
      "Loss: 0.5940144658088684\n",
      "Loss: 0.3479961156845093\n",
      "Loss: 0.45766255259513855\n",
      "Loss: 0.5772507786750793\n",
      "Loss: 0.5475040078163147\n",
      "Loss: 0.24843564629554749\n",
      "Loss: 0.6562725305557251\n",
      "Loss: 0.563555121421814\n",
      "Loss: 0.35747063159942627\n",
      "Loss: 0.4235590100288391\n",
      "Loss: 0.40465328097343445\n",
      "Loss: 0.3204059302806854\n",
      "Loss: 0.48152831196784973\n",
      "Loss: 0.4527026414871216\n",
      "Loss: 0.5519655346870422\n",
      "Loss: 0.624286413192749\n",
      "Loss: 0.3011092245578766\n",
      "Loss: 0.5267050266265869\n",
      "Loss: 0.6247853636741638\n",
      "Loss: 0.49066784977912903\n",
      "Loss: 0.8511314988136292\n",
      "Loss: 0.6174619793891907\n",
      "Loss: 0.3294063210487366\n",
      "Loss: 0.27064287662506104\n",
      "Loss: 0.2617306709289551\n",
      "Loss: 0.5097780823707581\n",
      "Loss: 0.31029069423675537\n",
      "Loss: 0.31919944286346436\n",
      "Loss: 0.3240244686603546\n",
      "Loss: 0.41227665543556213\n",
      "Loss: 0.2692432105541229\n",
      "Loss: 0.4175680875778198\n",
      "Loss: 0.3521176278591156\n",
      "Loss: 0.4349735975265503\n",
      "Loss: 0.41332173347473145\n",
      "Loss: 0.5656170845031738\n",
      "Loss: 0.40436074137687683\n",
      "Loss: 0.6273259520530701\n",
      "Loss: 0.5046689510345459\n",
      "Loss: 0.6561998724937439\n",
      "Loss: 0.3590623438358307\n",
      "Loss: 0.48644179105758667\n",
      "Loss: 0.4392489194869995\n",
      "Loss: 0.4171540439128876\n",
      "Loss: 0.4496398866176605\n",
      "Loss: 0.3343767821788788\n",
      "Loss: 0.6074291467666626\n",
      "Loss: 0.4407690763473511\n",
      "Loss: 0.30286335945129395\n",
      "Loss: 0.8047800660133362\n",
      "Loss: 0.47833651304244995\n",
      "Loss: 0.3231460154056549\n",
      "Loss: 0.43450236320495605\n",
      "Loss: 0.46201837062835693\n",
      "Loss: 0.5528594851493835\n",
      "Loss: 0.22315986454486847\n",
      "Loss: 0.4840676188468933\n",
      "Loss: 0.7568973302841187\n",
      "Loss: 0.47001010179519653\n",
      "Loss: 0.40325015783309937\n",
      "Loss: 0.7828872799873352\n",
      "Loss: 0.786093533039093\n",
      "Loss: 0.34034788608551025\n",
      "Loss: 0.6181748509407043\n",
      "Loss: 0.39463549852371216\n",
      "Loss: 0.35728490352630615\n",
      "Loss: 0.4416646957397461\n",
      "Loss: 0.4110030233860016\n",
      "Loss: 0.31634634733200073\n",
      "Loss: 0.28162992000579834\n",
      "Loss: 0.5872890949249268\n",
      "Loss: 0.5958981513977051\n",
      "Loss: 0.4230750799179077\n",
      "Loss: 0.43159958720207214\n",
      "Loss: 0.38264521956443787\n",
      "Loss: 0.4128115475177765\n",
      "Loss: 0.17790551483631134\n",
      "Loss: 0.36861762404441833\n",
      "Loss: 0.35932457447052\n",
      "Loss: 0.7262014746665955\n",
      "Loss: 0.6555560827255249\n",
      "Loss: 0.5893847346305847\n",
      "Loss: 0.8175399303436279\n",
      "Loss: 0.4939069449901581\n",
      "Loss: 0.41707366704940796\n",
      "Loss: 0.3979678452014923\n",
      "Loss: 0.46222835779190063\n",
      "Loss: 0.6369987726211548\n",
      "Loss: 0.6383286118507385\n",
      "Loss: 0.3133610188961029\n",
      "Loss: 0.3389568030834198\n",
      "Loss: 0.5108773708343506\n",
      "Loss: 0.4366106688976288\n",
      "Loss: 0.38590967655181885\n",
      "Loss: 0.46587231755256653\n",
      "Loss: 0.6324616074562073\n",
      "Loss: 0.2886861562728882\n",
      "Loss: 0.24748088419437408\n",
      "Loss: 0.24724259972572327\n",
      "Loss: 0.4162887930870056\n",
      "Loss: 0.5034471154212952\n",
      "Loss: 0.503518283367157\n",
      "Loss: 0.5020281076431274\n",
      "Loss: 0.3540687561035156\n",
      "Loss: 0.36178871989250183\n",
      "Loss: 0.29333630204200745\n",
      "Loss: 0.3986876308917999\n",
      "Loss: 0.3953399956226349\n",
      "Loss: 0.3443760573863983\n",
      "Loss: 0.4521641135215759\n",
      "Loss: 0.4735855758190155\n",
      "Loss: 0.32675567269325256\n",
      "Loss: 0.35927045345306396\n",
      "Loss: 0.39387673139572144\n",
      "Loss: 0.2506846487522125\n",
      "Loss: 0.6316465735435486\n",
      "Loss: 0.4687082767486572\n",
      "Loss: 0.5147429704666138\n",
      "Loss: 0.37622061371803284\n",
      "Loss: 0.634743332862854\n",
      "Loss: 0.30583998560905457\n",
      "Loss: 0.562686026096344\n",
      "Loss: 0.48055610060691833\n",
      "Loss: 0.5809935927391052\n",
      "Loss: 0.37032073736190796\n",
      "Loss: 0.53501296043396\n",
      "Loss: 0.5394855737686157\n",
      "Loss: 0.2410304993391037\n",
      "Loss: 0.4839496910572052\n",
      "Loss: 0.9208887219429016\n",
      "Loss: 0.5016876459121704\n",
      "Loss: 0.3468455970287323\n",
      "Loss: 0.5018131136894226\n",
      "Loss: 0.33203375339508057\n",
      "Loss: 0.44290706515312195\n",
      "Loss: 0.5873661637306213\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7761785984039307\n",
      "Loss: 2.6229982376098633\n",
      "Loss: 2.3505163192749023\n",
      "Loss: 2.4562995433807373\n",
      "Loss: 2.2963004112243652\n",
      "Loss: 2.2877907752990723\n",
      "Loss: 2.326465368270874\n",
      "Loss: 2.2519524097442627\n",
      "Loss: 2.257910966873169\n",
      "Loss: 2.294045925140381\n",
      "Loss: 2.233964204788208\n",
      "Loss: 2.2303404808044434\n",
      "Loss: 2.2559640407562256\n",
      "Loss: 2.2133121490478516\n",
      "Loss: 2.2026357650756836\n",
      "Loss: 2.190990686416626\n",
      "Loss: 2.220290422439575\n",
      "Loss: 2.1194369792938232\n",
      "Loss: 2.217381238937378\n",
      "Loss: 2.2835404872894287\n",
      "Loss: 2.217271566390991\n",
      "Loss: 2.1934566497802734\n",
      "Loss: 2.1742794513702393\n",
      "Loss: 2.1087615489959717\n",
      "Loss: 2.1302084922790527\n",
      "Loss: 2.1062982082366943\n",
      "Loss: 2.1814534664154053\n",
      "Loss: 2.2340080738067627\n",
      "Loss: 2.0547263622283936\n",
      "Loss: 2.057990074157715\n",
      "Loss: 2.0572774410247803\n",
      "Loss: 1.9716852903366089\n",
      "Loss: 2.0525991916656494\n",
      "Loss: 2.092095136642456\n",
      "Loss: 2.1406333446502686\n",
      "Loss: 1.951910376548767\n",
      "Loss: 1.848122000694275\n",
      "Loss: 2.0032799243927\n",
      "Loss: 2.099607229232788\n",
      "Loss: 2.067258358001709\n",
      "Loss: 1.9320701360702515\n",
      "Loss: 1.9726645946502686\n",
      "Loss: 1.9771926403045654\n",
      "Loss: 1.8770033121109009\n",
      "Loss: 1.8907755613327026\n",
      "Loss: 1.9728175401687622\n",
      "Loss: 2.000509262084961\n",
      "Loss: 1.7934232950210571\n",
      "Loss: 1.9496251344680786\n",
      "Loss: 1.9357458353042603\n",
      "Loss: 1.9000872373580933\n",
      "Loss: 1.8720113039016724\n",
      "Loss: 1.960324764251709\n",
      "Loss: 1.9707434177398682\n",
      "Loss: 1.7768954038619995\n",
      "Loss: 2.065124750137329\n",
      "Loss: 1.8660792112350464\n",
      "Loss: 1.915037989616394\n",
      "Loss: 1.6766194105148315\n",
      "Loss: 1.8622937202453613\n",
      "Loss: 1.881842851638794\n",
      "Loss: 1.9258832931518555\n",
      "Loss: 1.802130937576294\n",
      "Loss: 1.8607308864593506\n",
      "Loss: 1.9005357027053833\n",
      "Loss: 1.8307286500930786\n",
      "Loss: 1.7792598009109497\n",
      "Loss: 1.7618829011917114\n",
      "Loss: 1.736527442932129\n",
      "Loss: 1.816770076751709\n",
      "Loss: 1.8389784097671509\n",
      "Loss: 1.7220661640167236\n",
      "Loss: 1.8376452922821045\n",
      "Loss: 1.707107663154602\n",
      "Loss: 1.663920283317566\n",
      "Loss: 1.6257520914077759\n",
      "Loss: 1.7988008260726929\n",
      "Loss: 1.8563679456710815\n",
      "Loss: 1.7532262802124023\n",
      "Loss: 1.543245553970337\n",
      "Loss: 1.8294804096221924\n",
      "Loss: 1.7639998197555542\n",
      "Loss: 1.6721739768981934\n",
      "Loss: 1.8296372890472412\n",
      "Loss: 1.7845969200134277\n",
      "Loss: 1.8127943277359009\n",
      "Loss: 1.8519117832183838\n",
      "Loss: 1.8449656963348389\n",
      "Loss: 1.8906545639038086\n",
      "Loss: 1.6486183404922485\n",
      "Loss: 1.809994101524353\n",
      "Loss: 1.7419756650924683\n",
      "Loss: 1.8006377220153809\n",
      "Loss: 1.7438676357269287\n",
      "Loss: 1.5806028842926025\n",
      "Loss: 1.583214521408081\n",
      "Loss: 1.7524930238723755\n",
      "Loss: 1.7737038135528564\n",
      "Loss: 1.6557321548461914\n",
      "Loss: 1.7771484851837158\n",
      "Loss: 1.8024941682815552\n",
      "Loss: 1.6213306188583374\n",
      "Loss: 1.4715304374694824\n",
      "Loss: 1.5456717014312744\n",
      "Loss: 1.4665478467941284\n",
      "Loss: 1.6817634105682373\n",
      "Loss: 1.5167316198349\n",
      "Loss: 1.6538474559783936\n",
      "Loss: 1.7058523893356323\n",
      "Loss: 1.4588689804077148\n",
      "Loss: 1.7701730728149414\n",
      "Loss: 1.6554874181747437\n",
      "Loss: 1.6338406801223755\n",
      "Loss: 1.5570696592330933\n",
      "Loss: 1.8326280117034912\n",
      "Loss: 1.587528109550476\n",
      "Loss: 1.6124500036239624\n",
      "Loss: 1.6094468832015991\n",
      "Loss: 1.5516315698623657\n",
      "Loss: 1.6840689182281494\n",
      "Loss: 1.679428219795227\n",
      "Loss: 1.7079211473464966\n",
      "Loss: 1.5571653842926025\n",
      "Loss: 1.6354900598526\n",
      "Loss: 1.7003772258758545\n",
      "Loss: 1.512791633605957\n",
      "Loss: 1.6826539039611816\n",
      "Loss: 1.8110897541046143\n",
      "Loss: 1.7202889919281006\n",
      "Loss: 1.5797975063323975\n",
      "Loss: 1.723537564277649\n",
      "Loss: 1.636834740638733\n",
      "Loss: 1.6808491945266724\n",
      "Loss: 1.7392839193344116\n",
      "Loss: 1.4444971084594727\n",
      "Loss: 1.7004094123840332\n",
      "Loss: 1.6908364295959473\n",
      "Loss: 1.572078824043274\n",
      "Loss: 1.626466989517212\n",
      "Loss: 1.6525421142578125\n",
      "Loss: 1.6239421367645264\n",
      "Loss: 1.462048053741455\n",
      "Loss: 1.6433939933776855\n",
      "Loss: 1.8903087377548218\n",
      "Loss: 1.5401743650436401\n",
      "Loss: 1.6263736486434937\n",
      "Loss: 1.528706669807434\n",
      "Loss: 1.6734346151351929\n",
      "Loss: 1.5962586402893066\n",
      "Loss: 1.595068097114563\n",
      "Loss: 1.52561616897583\n",
      "Loss: 1.819730520248413\n",
      "Loss: 1.6733849048614502\n",
      "Loss: 1.817211627960205\n",
      "Loss: 1.4844944477081299\n",
      "Loss: 1.751998782157898\n",
      "Loss: 1.6968834400177002\n",
      "Loss: 1.518936038017273\n",
      "Loss: 1.5036094188690186\n",
      "Loss: 1.4877314567565918\n",
      "Loss: 1.8428317308425903\n",
      "Loss: 1.6342304944992065\n",
      "Loss: 1.594912052154541\n",
      "Loss: 1.5100491046905518\n",
      "Loss: 1.6239559650421143\n",
      "Loss: 1.5958971977233887\n",
      "Loss: 1.7587298154830933\n",
      "Loss: 1.6967289447784424\n",
      "Loss: 1.6746734380722046\n",
      "Loss: 1.7997050285339355\n",
      "Loss: 1.5136351585388184\n",
      "Loss: 1.7643330097198486\n",
      "Loss: 1.8106151819229126\n",
      "Loss: 1.6323562860488892\n",
      "Loss: 1.5722110271453857\n",
      "Loss: 1.5515551567077637\n",
      "Loss: 1.416326880455017\n",
      "Loss: 1.7000635862350464\n",
      "Loss: 1.4369713068008423\n",
      "Loss: 1.6291558742523193\n",
      "Loss: 1.541536569595337\n",
      "Loss: 1.607566475868225\n",
      "Loss: 1.584220290184021\n",
      "Loss: 1.3659470081329346\n",
      "Loss: 1.6521391868591309\n",
      "Loss: 1.6615324020385742\n",
      "Loss: 1.4404128789901733\n",
      "Loss: 1.5067371129989624\n",
      "Loss: 1.2626413106918335\n",
      "Loss: 1.5025233030319214\n",
      "Loss: 1.6143734455108643\n",
      "Loss: 1.5208206176757812\n",
      "Loss: 1.735862135887146\n",
      "Loss: 1.576749563217163\n",
      "Loss: 1.3646061420440674\n",
      "Loss: 1.5571105480194092\n",
      "Loss: 1.5071966648101807\n",
      "Loss: 1.7666889429092407\n",
      "Loss: 1.507231593132019\n",
      "Loss: 1.6360491514205933\n",
      "Loss: 1.505568504333496\n",
      "Loss: 1.383665919303894\n",
      "Loss: 1.736325740814209\n",
      "Loss: 1.4471265077590942\n",
      "Loss: 1.593106985092163\n",
      "Loss: 1.4998126029968262\n",
      "Loss: 1.2213186025619507\n",
      "Loss: 1.2973114252090454\n",
      "Loss: 1.5741902589797974\n",
      "Loss: 1.6257116794586182\n",
      "Loss: 1.4233134984970093\n",
      "Loss: 1.399164080619812\n",
      "Loss: 1.462852120399475\n",
      "Loss: 1.6052166223526\n",
      "Loss: 1.4667857885360718\n",
      "Loss: 1.5968642234802246\n",
      "Loss: 1.7493380308151245\n",
      "Loss: 1.5773197412490845\n",
      "Loss: 1.5953483581542969\n",
      "Loss: 1.6044787168502808\n",
      "Loss: 1.5084611177444458\n",
      "Loss: 1.5916444063186646\n",
      "Loss: 1.7250856161117554\n",
      "Loss: 1.5554431676864624\n",
      "Loss: 1.4115393161773682\n",
      "Loss: 1.4591134786605835\n",
      "Loss: 1.8305299282073975\n",
      "Loss: 1.395514726638794\n",
      "Loss: 1.4815808534622192\n",
      "Loss: 1.538163661956787\n",
      "Loss: 1.5059020519256592\n",
      "Loss: 1.5055125951766968\n",
      "Loss: 1.4645609855651855\n",
      "Loss: 1.8675035238265991\n",
      "Loss: 1.5466854572296143\n",
      "Loss: 1.5062354803085327\n",
      "Loss: 1.5456379652023315\n",
      "Loss: 1.553948163986206\n",
      "Loss: 1.5146870613098145\n",
      "Loss: 1.7647161483764648\n",
      "Loss: 1.5082603693008423\n",
      "Loss: 1.4551100730895996\n",
      "Loss: 1.5703791379928589\n",
      "Loss: 1.4716547727584839\n",
      "Loss: 1.2912379503250122\n",
      "Loss: 1.6472060680389404\n",
      "Loss: 1.600956678390503\n",
      "Loss: 1.5671576261520386\n",
      "Loss: 1.5071845054626465\n",
      "Loss: 1.5075640678405762\n",
      "Loss: 1.5636727809906006\n",
      "Loss: 1.3572016954421997\n",
      "Loss: 1.5184375047683716\n",
      "Loss: 1.4689732789993286\n",
      "Loss: 1.5819975137710571\n",
      "Loss: 1.4406338930130005\n",
      "Loss: 1.8974151611328125\n",
      "Loss: 1.719369649887085\n",
      "Loss: 1.6633858680725098\n",
      "Loss: 1.4323545694351196\n",
      "Loss: 1.3943198919296265\n",
      "Loss: 1.4427450895309448\n",
      "Loss: 1.615565299987793\n",
      "Loss: 1.714080810546875\n",
      "Loss: 1.5154027938842773\n",
      "Loss: 1.5838967561721802\n",
      "Loss: 1.4493129253387451\n",
      "Loss: 1.5035319328308105\n",
      "Loss: 1.820945143699646\n",
      "Loss: 1.4723750352859497\n",
      "Loss: 1.5922569036483765\n",
      "Loss: 1.6271014213562012\n",
      "Loss: 1.4847397804260254\n",
      "Loss: 1.6104986667633057\n",
      "Loss: 1.5725834369659424\n",
      "Loss: 1.4891072511672974\n",
      "Loss: 1.4101393222808838\n",
      "Loss: 1.5976330041885376\n",
      "Loss: 1.5365347862243652\n",
      "Loss: 1.5974735021591187\n",
      "Loss: 1.4917323589324951\n",
      "Loss: 1.5239332914352417\n",
      "Loss: 1.572494387626648\n",
      "Loss: 1.593377709388733\n",
      "Loss: 1.5873990058898926\n",
      "Loss: 1.6570435762405396\n",
      "Loss: 1.4243273735046387\n",
      "Loss: 1.5628372430801392\n",
      "Loss: 1.4515984058380127\n",
      "Loss: 1.7201132774353027\n",
      "Loss: 1.8243991136550903\n",
      "Loss: 1.537463665008545\n",
      "Loss: 1.415650486946106\n",
      "Loss: 1.5504472255706787\n",
      "Loss: 1.52260160446167\n",
      "Loss: 1.3236714601516724\n",
      "Loss: 1.5097945928573608\n",
      "Loss: 1.8192799091339111\n",
      "Loss: 1.772128701210022\n",
      "Loss: 1.408054232597351\n",
      "Loss: 1.6835083961486816\n",
      "Loss: 1.3749375343322754\n",
      "Loss: 1.66214919090271\n",
      "Loss: 1.5398566722869873\n",
      "Loss: 1.4899840354919434\n",
      "Loss: 1.9051909446716309\n",
      "Loss: 1.5038502216339111\n",
      "Loss: 1.4286137819290161\n",
      "Loss: 1.4966223239898682\n",
      "Loss: 1.3705494403839111\n",
      "Loss: 1.329818606376648\n",
      "Loss: 1.5943913459777832\n",
      "Loss: 1.6672313213348389\n",
      "Loss: 1.4939039945602417\n",
      "Loss: 1.4371376037597656\n",
      "Loss: 1.7317304611206055\n",
      "Loss: 1.6196781396865845\n",
      "Loss: 1.6257492303848267\n",
      "Loss: 1.4937913417816162\n",
      "Loss: 1.5238195657730103\n",
      "Loss: 1.597273826599121\n",
      "Loss: 1.5098016262054443\n",
      "Loss: 1.5811847448349\n",
      "Loss: 1.5827651023864746\n",
      "Loss: 1.4454160928726196\n",
      "Loss: 1.5823006629943848\n",
      "Loss: 1.386690616607666\n",
      "Loss: 1.7494556903839111\n",
      "Loss: 1.490159034729004\n",
      "Loss: 1.2917611598968506\n",
      "Loss: 1.586527705192566\n",
      "Loss: 1.5947983264923096\n",
      "Loss: 1.5692976713180542\n",
      "Loss: 1.5671240091323853\n",
      "Loss: 1.5097029209136963\n",
      "Loss: 1.4475985765457153\n",
      "Loss: 1.5411763191223145\n",
      "Loss: 1.598391056060791\n",
      "Loss: 1.5139411687850952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5343044996261597\n",
      "Loss: 1.470085859298706\n",
      "Loss: 1.7409405708312988\n",
      "Loss: 1.3208531141281128\n",
      "Loss: 1.6532695293426514\n",
      "Loss: 1.701149344444275\n",
      "Loss: 1.5775469541549683\n",
      "Loss: 1.6016736030578613\n",
      "Loss: 1.3746775388717651\n",
      "Loss: 1.5244183540344238\n",
      "Loss: 1.4841586351394653\n",
      "Loss: 1.537546992301941\n",
      "Loss: 1.5829201936721802\n",
      "Loss: 1.5313972234725952\n",
      "Loss: 1.5151599645614624\n",
      "Loss: 1.5521986484527588\n",
      "Loss: 1.5350052118301392\n",
      "Loss: 1.6204383373260498\n",
      "Loss: 1.681959867477417\n",
      "Loss: 1.3975515365600586\n",
      "Loss: 1.558091402053833\n",
      "Loss: 1.5642389059066772\n",
      "Loss: 1.472490668296814\n",
      "Loss: 1.7206252813339233\n",
      "Loss: 1.522516131401062\n",
      "Loss: 1.4833563566207886\n",
      "Loss: 1.6760822534561157\n",
      "Loss: 1.4523215293884277\n",
      "Loss: 1.5783722400665283\n",
      "Loss: 1.555077075958252\n",
      "Loss: 1.6597400903701782\n",
      "Loss: 1.6604149341583252\n",
      "Loss: 1.4987215995788574\n",
      "Loss: 1.5114774703979492\n",
      "Loss: 1.4944146871566772\n",
      "Loss: 1.5135207176208496\n",
      "Loss: 1.6269962787628174\n",
      "Loss: 1.5217090845108032\n",
      "Loss: 1.389207363128662\n",
      "Loss: 1.621984601020813\n",
      "Loss: 1.5753686428070068\n",
      "Loss: 1.5409507751464844\n",
      "Loss: 1.5931193828582764\n",
      "Loss: 1.6199302673339844\n",
      "Loss: 1.629512071609497\n",
      "Loss: 1.827525019645691\n",
      "Loss: 1.6532903909683228\n",
      "Loss: 1.3504116535186768\n",
      "Loss: 1.5963494777679443\n",
      "Loss: 1.5406008958816528\n",
      "Loss: 1.638831377029419\n",
      "Loss: 1.528141736984253\n",
      "Loss: 1.7446500062942505\n",
      "Loss: 1.510393500328064\n",
      "Loss: 1.4084275960922241\n",
      "Loss: 1.6090604066848755\n",
      "Loss: 1.5933597087860107\n",
      "Loss: 1.6655994653701782\n",
      "Loss: 1.5397627353668213\n",
      "Loss: 1.458688497543335\n",
      "Loss: 1.5335770845413208\n",
      "Loss: 1.5929964780807495\n",
      "Loss: 1.705870509147644\n",
      "Loss: 1.5552974939346313\n",
      "Loss: 1.5136550664901733\n",
      "Loss: 1.5646765232086182\n",
      "Loss: 1.392611026763916\n",
      "Loss: 1.5995692014694214\n",
      "Loss: 1.4531805515289307\n",
      "Loss: 1.6103997230529785\n",
      "Loss: 1.6716846227645874\n",
      "Loss: 1.549573540687561\n",
      "Loss: 1.4017820358276367\n",
      "Loss: 1.6390647888183594\n",
      "Loss: 1.5101181268692017\n",
      "Loss: 1.5625238418579102\n",
      "Loss: 1.4571006298065186\n",
      "Loss: 1.574476957321167\n",
      "Loss: 1.424648404121399\n",
      "Loss: 1.5549055337905884\n",
      "Loss: 1.4935333728790283\n",
      "Loss: 1.6002296209335327\n",
      "Loss: 1.5620155334472656\n",
      "Loss: 1.6560494899749756\n",
      "Loss: 1.5104341506958008\n",
      "Loss: 1.5055127143859863\n",
      "Loss: 1.7962449789047241\n",
      "Loss: 1.4280176162719727\n",
      "Loss: 1.4739768505096436\n",
      "Loss: 1.3707084655761719\n",
      "Loss: 2.106766700744629\n",
      "Loss: 1.5505783557891846\n",
      "Loss: 1.3097163438796997\n",
      "Loss: 1.777113914489746\n",
      "Loss: 1.3507236242294312\n",
      "Loss: 1.7479238510131836\n",
      "Loss: 1.7770371437072754\n",
      "Loss: 1.390498399734497\n",
      "Loss: 1.6129238605499268\n",
      "Loss: 1.6359107494354248\n",
      "Loss: 1.4551031589508057\n",
      "Loss: 1.5794916152954102\n",
      "Loss: 1.7027055025100708\n",
      "Loss: 1.384650707244873\n",
      "Loss: 1.6227943897247314\n",
      "Loss: 1.5602810382843018\n",
      "Loss: 1.638441562652588\n",
      "Loss: 1.2709004878997803\n",
      "Loss: 1.4861770868301392\n",
      "Loss: 1.663499355316162\n",
      "Loss: 1.4804283380508423\n",
      "Loss: 1.6152435541152954\n",
      "Loss: 1.5918046236038208\n",
      "Loss: 1.4579856395721436\n",
      "Loss: 1.5907222032546997\n",
      "Loss: 1.621343731880188\n",
      "Loss: 1.5673729181289673\n",
      "Loss: 1.4909920692443848\n",
      "Loss: 1.528672218322754\n",
      "Loss: 1.4115391969680786\n",
      "Loss: 1.4243489503860474\n",
      "Loss: 1.6038563251495361\n",
      "Loss: 1.6333248615264893\n",
      "Loss: 1.6235246658325195\n",
      "Loss: 1.6829451322555542\n",
      "Loss: 1.5916616916656494\n",
      "Loss: 1.4439927339553833\n",
      "Loss: 1.6825995445251465\n",
      "Loss: 1.5691903829574585\n",
      "Loss: 1.4316097497940063\n",
      "Loss: 1.7475290298461914\n",
      "Loss: 1.6994175910949707\n",
      "Loss: 1.5749497413635254\n",
      "Loss: 1.659165859222412\n",
      "Loss: 1.6758341789245605\n",
      "Loss: 1.6737756729125977\n",
      "Loss: 1.5939319133758545\n",
      "Loss: 1.6920057535171509\n",
      "Loss: 1.597353219985962\n",
      "Loss: 1.4669060707092285\n",
      "Loss: 1.5401506423950195\n",
      "Loss: 1.5392425060272217\n",
      "Loss: 1.3521902561187744\n",
      "Loss: 1.6511791944503784\n",
      "Loss: 1.64912748336792\n",
      "Loss: 1.5819220542907715\n",
      "Loss: 1.5033568143844604\n",
      "Loss: 1.6022647619247437\n",
      "Loss: 1.4391857385635376\n",
      "Loss: 1.6254489421844482\n",
      "Loss: 1.596197247505188\n",
      "Loss: 1.5687733888626099\n",
      "Loss: 1.5790891647338867\n",
      "Loss: 1.5681740045547485\n",
      "Loss: 1.590164303779602\n",
      "Loss: 1.3778759241104126\n",
      "Loss: 1.3419548273086548\n",
      "Loss: 1.647099256515503\n",
      "Loss: 1.4481297731399536\n",
      "Loss: 1.4846818447113037\n",
      "Loss: 1.5988149642944336\n",
      "Loss: 1.5195213556289673\n",
      "Loss: 1.633169412612915\n",
      "Loss: 1.6118196249008179\n",
      "Loss: 1.428260326385498\n",
      "Loss: 1.4640355110168457\n",
      "Loss: 1.4841848611831665\n",
      "Loss: 1.9670422077178955\n",
      "Loss: 1.4622858762741089\n",
      "Loss: 1.6675320863723755\n",
      "Loss: 1.4284231662750244\n",
      "Loss: 1.4797112941741943\n",
      "Loss: 1.4810982942581177\n",
      "Loss: 1.4590855836868286\n",
      "Loss: 1.4949618577957153\n",
      "Loss: 1.3983267545700073\n",
      "Loss: 1.6099759340286255\n",
      "Loss: 1.6294236183166504\n",
      "Loss: 1.4829295873641968\n",
      "Loss: 1.5615054368972778\n",
      "Loss: 1.537269115447998\n",
      "Loss: 1.400254249572754\n",
      "Loss: 1.5005472898483276\n",
      "Loss: 1.7669275999069214\n",
      "Loss: 1.38670015335083\n",
      "Loss: 1.4989023208618164\n",
      "Loss: 1.8539766073226929\n",
      "Loss: 1.599676251411438\n",
      "Loss: 1.6344985961914062\n",
      "Loss: 1.7219427824020386\n",
      "Loss: 1.515519618988037\n",
      "Loss: 1.5778532028198242\n",
      "Loss: 1.7104159593582153\n",
      "Loss: 1.6265486478805542\n",
      "Loss: 1.5569829940795898\n",
      "Loss: 1.3978215456008911\n",
      "Loss: 1.5287452936172485\n",
      "Loss: 1.5393568277359009\n",
      "Loss: 1.6861801147460938\n",
      "Loss: 1.5201188325881958\n",
      "Loss: 1.5393409729003906\n",
      "Loss: 1.460595965385437\n",
      "Loss: 1.4774037599563599\n",
      "Loss: 1.3996373414993286\n",
      "Loss: 1.6843987703323364\n",
      "Loss: 1.4568686485290527\n",
      "Loss: 1.6146607398986816\n",
      "Loss: 1.4948500394821167\n",
      "Loss: 1.5927820205688477\n",
      "Loss: 1.8413398265838623\n",
      "Loss: 1.5195305347442627\n",
      "Loss: 1.5434958934783936\n",
      "Loss: 1.437991738319397\n",
      "Loss: 1.5307523012161255\n",
      "Loss: 1.4471113681793213\n",
      "Loss: 1.3936467170715332\n",
      "Loss: 1.5155798196792603\n",
      "Loss: 1.5646121501922607\n",
      "Loss: 1.6177020072937012\n",
      "Loss: 1.3290575742721558\n",
      "Loss: 1.4489772319793701\n",
      "Loss: 1.3382295370101929\n",
      "Loss: 1.6492629051208496\n",
      "Loss: 1.4417616128921509\n",
      "Loss: 1.578925609588623\n",
      "Loss: 1.6021395921707153\n",
      "Loss: 1.573384404182434\n",
      "Loss: 1.614251971244812\n",
      "Loss: 1.5287230014801025\n",
      "Loss: 1.605767011642456\n",
      "Loss: 1.5922834873199463\n",
      "Loss: 1.5023020505905151\n",
      "Loss: 1.5897899866104126\n",
      "Loss: 1.6713165044784546\n",
      "Loss: 1.4963496923446655\n",
      "Loss: 1.5939724445343018\n",
      "Loss: 1.537688136100769\n",
      "Loss: 1.5991227626800537\n",
      "Loss: 1.5547136068344116\n",
      "Loss: 1.5811593532562256\n",
      "Loss: 1.7833503484725952\n",
      "Loss: 1.4555660486221313\n",
      "Loss: 1.591063380241394\n",
      "Loss: 1.5137027502059937\n",
      "Loss: 1.367160439491272\n",
      "Loss: 1.2959015369415283\n",
      "Loss: 1.5772099494934082\n",
      "Loss: 1.6178503036499023\n",
      "Loss: 1.6101313829421997\n",
      "Loss: 1.5224809646606445\n",
      "Loss: 1.5360145568847656\n",
      "Loss: 1.4580786228179932\n",
      "Loss: 1.8025439977645874\n",
      "Loss: 1.4006925821304321\n",
      "Loss: 1.5741125345230103\n",
      "Loss: 1.3538310527801514\n",
      "Loss: 1.4688515663146973\n",
      "Loss: 1.4943941831588745\n",
      "Loss: 1.4495532512664795\n",
      "Loss: 1.6634716987609863\n",
      "Loss: 1.4460095167160034\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 1.9112\n",
      "a: -0.0046\n",
      "step:  1\n",
      "loss: 1.7840\n",
      "a: -0.0035\n",
      "step:  2\n",
      "loss: 2.0550\n",
      "a: -0.0024\n",
      "step:  3\n",
      "loss: 1.8941\n",
      "a: -0.0013\n",
      "step:  4\n",
      "loss: 1.9640\n",
      "a: -0.0002\n",
      "step:  5\n",
      "loss: 7.8278\n",
      "a: 0.0008\n",
      "step:  6\n",
      "loss: 2.1500\n",
      "a: -0.0019\n",
      "step:  7\n",
      "loss: 1.8856\n",
      "a: -0.0040\n",
      "step:  8\n",
      "loss: 1.9853\n",
      "a: -0.0058\n",
      "step:  9\n",
      "loss: 2.0946\n",
      "a: -0.0074\n",
      "step:  10\n",
      "loss: 2.3781\n",
      "a: -0.0090\n",
      "step:  11\n",
      "loss: 2.0241\n",
      "a: -0.0104\n",
      "step:  12\n",
      "loss: 4.2390\n",
      "a: -0.0117\n",
      "step:  13\n",
      "loss: 2.7668\n",
      "a: -0.0130\n",
      "step:  14\n",
      "loss: 2.7723\n",
      "a: -0.0142\n",
      "step:  15\n",
      "loss: 3.4524\n",
      "a: -0.0154\n",
      "step:  16\n",
      "loss: 3.0168\n",
      "a: -0.0165\n",
      "step:  17\n",
      "loss: 3.9154\n",
      "a: -0.0175\n",
      "step:  18\n",
      "loss: 3.5155\n",
      "a: -0.0185\n",
      "step:  19\n",
      "loss: 4.0209\n",
      "a: -0.0195\n",
      "step:  20\n",
      "loss: 4.7518\n",
      "a: -0.0204\n",
      "step:  21\n",
      "loss: 4.4742\n",
      "a: -0.0213\n",
      "step:  22\n",
      "loss: 5.2670\n",
      "a: -0.0221\n",
      "step:  23\n",
      "loss: 4.9377\n",
      "a: -0.0228\n",
      "step:  24\n",
      "loss: 5.6557\n",
      "a: -0.0236\n",
      "step:  25\n",
      "loss: 5.5684\n",
      "a: -0.0242\n",
      "step:  26\n",
      "loss: 6.0723\n",
      "a: -0.0248\n",
      "step:  27\n",
      "loss: 6.4312\n",
      "a: -0.0253\n",
      "step:  28\n",
      "loss: 6.2490\n",
      "a: -0.0258\n",
      "step:  29\n",
      "loss: 6.5367\n",
      "a: -0.0262\n",
      "step:  30\n",
      "loss: 7.0034\n",
      "a: -0.0265\n",
      "step:  31\n",
      "loss: 6.9628\n",
      "a: -0.0268\n",
      "step:  32\n",
      "loss: 7.0506\n",
      "a: -0.0269\n",
      "step:  33\n",
      "loss: 7.1292\n",
      "a: -0.0268\n",
      "step:  34\n",
      "loss: 7.0048\n",
      "a: -0.0266\n",
      "step:  35\n",
      "loss: 6.9771\n",
      "a: -0.0263\n",
      "step:  36\n",
      "loss: 7.9454\n",
      "a: -0.0258\n",
      "step:  37\n",
      "loss: 6.9255\n",
      "a: -0.0251\n",
      "step:  38\n",
      "loss: 5.7923\n",
      "a: -0.0243\n",
      "step:  39\n",
      "loss: 4.4841\n",
      "a: -0.0233\n",
      "step:  40\n",
      "loss: 4.1702\n",
      "a: -0.0223\n",
      "step:  41\n",
      "loss: 4.2952\n",
      "a: -0.0211\n",
      "step:  42\n",
      "loss: 4.0100\n",
      "a: -0.0199\n",
      "step:  43\n",
      "loss: 3.0919\n",
      "a: -0.0187\n",
      "step:  44\n",
      "loss: 3.8541\n",
      "a: -0.0174\n",
      "step:  45\n",
      "loss: 2.6544\n",
      "a: -0.0161\n",
      "step:  46\n",
      "loss: 2.3464\n",
      "a: -0.0148\n",
      "step:  47\n",
      "loss: 3.2280\n",
      "a: -0.0135\n",
      "step:  48\n",
      "loss: 1.9947\n",
      "a: -0.0122\n",
      "step:  49\n",
      "loss: 2.1393\n",
      "a: -0.0110\n",
      "step:  50\n",
      "loss: 1.7025\n",
      "a: -0.0097\n",
      "step:  51\n",
      "loss: 1.8857\n",
      "a: -0.0085\n",
      "step:  52\n",
      "loss: 1.8408\n",
      "a: -0.0073\n",
      "step:  53\n",
      "loss: 1.8876\n",
      "a: -0.0062\n",
      "step:  54\n",
      "loss: 2.4536\n",
      "a: -0.0050\n",
      "step:  55\n",
      "loss: 1.9511\n",
      "a: -0.0039\n",
      "step:  56\n",
      "loss: 1.7897\n",
      "a: -0.0028\n",
      "step:  57\n",
      "loss: 1.7812\n",
      "a: -0.0018\n",
      "step:  58\n",
      "loss: 1.9120\n",
      "a: -0.0007\n",
      "step:  59\n",
      "loss: 3.8198\n",
      "a: 0.0003\n",
      "step:  60\n",
      "loss: 2.2113\n",
      "a: 0.0000\n",
      "step:  61\n",
      "loss: 3.6576\n",
      "a: 0.0000\n",
      "step:  62\n",
      "loss: 4.5032\n",
      "a: 0.0005\n",
      "step:  63\n",
      "loss: 1.5942\n",
      "a: -0.0006\n",
      "step:  64\n",
      "loss: 1.9247\n",
      "a: -0.0016\n",
      "step:  65\n",
      "loss: 1.8607\n",
      "a: -0.0027\n",
      "step:  66\n",
      "loss: 2.1601\n",
      "a: -0.0037\n",
      "step:  67\n",
      "loss: 1.7842\n",
      "a: -0.0048\n",
      "step:  68\n",
      "loss: 1.7146\n",
      "a: -0.0058\n",
      "step:  69\n",
      "loss: 1.7523\n",
      "a: -0.0069\n",
      "step:  70\n",
      "loss: 1.7932\n",
      "a: -0.0079\n",
      "step:  71\n",
      "loss: 2.0097\n",
      "a: -0.0089\n",
      "step:  72\n",
      "loss: 1.9358\n",
      "a: -0.0099\n",
      "step:  73\n",
      "loss: 2.0036\n",
      "a: -0.0109\n",
      "step:  74\n",
      "loss: 2.4533\n",
      "a: -0.0118\n",
      "step:  75\n",
      "loss: 2.4840\n",
      "a: -0.0128\n",
      "step:  76\n",
      "loss: 2.9484\n",
      "a: -0.0137\n",
      "step:  77\n",
      "loss: 2.9594\n",
      "a: -0.0145\n",
      "step:  78\n",
      "loss: 3.2059\n",
      "a: -0.0154\n",
      "step:  79\n",
      "loss: 3.0971\n",
      "a: -0.0161\n",
      "step:  80\n",
      "loss: 3.4030\n",
      "a: -0.0169\n",
      "step:  81\n",
      "loss: 4.0484\n",
      "a: -0.0176\n",
      "step:  82\n",
      "loss: 4.4350\n",
      "a: -0.0182\n",
      "step:  83\n",
      "loss: 3.9533\n",
      "a: -0.0187\n",
      "step:  84\n",
      "loss: 4.1126\n",
      "a: -0.0192\n",
      "step:  85\n",
      "loss: 3.8566\n",
      "a: -0.0196\n",
      "step:  86\n",
      "loss: 4.6015\n",
      "a: -0.0200\n",
      "step:  87\n",
      "loss: 4.3123\n",
      "a: -0.0202\n",
      "step:  88\n",
      "loss: 4.3499\n",
      "a: -0.0203\n",
      "step:  89\n",
      "loss: 4.5025\n",
      "a: -0.0203\n",
      "step:  90\n",
      "loss: 3.5283\n",
      "a: -0.0202\n",
      "step:  91\n",
      "loss: 4.1015\n",
      "a: -0.0199\n",
      "step:  92\n",
      "loss: 3.0973\n",
      "a: -0.0195\n",
      "step:  93\n",
      "loss: 3.6867\n",
      "a: -0.0190\n",
      "step:  94\n",
      "loss: 3.3121\n",
      "a: -0.0183\n",
      "step:  95\n",
      "loss: 3.1743\n",
      "a: -0.0175\n",
      "step:  96\n",
      "loss: 4.7782\n",
      "a: -0.0166\n",
      "step:  97\n",
      "loss: 2.6728\n",
      "a: -0.0157\n",
      "step:  98\n",
      "loss: 2.6301\n",
      "a: -0.0146\n",
      "step:  99\n",
      "loss: 2.5713\n",
      "a: -0.0135\n",
      "step:  100\n",
      "loss: 1.8928\n",
      "a: -0.0124\n",
      "step:  101\n",
      "loss: 2.5135\n",
      "a: -0.0113\n",
      "step:  102\n",
      "loss: 1.9717\n",
      "a: -0.0101\n",
      "step:  103\n",
      "loss: 2.6846\n",
      "a: -0.0090\n",
      "step:  104\n",
      "loss: 1.6616\n",
      "a: -0.0078\n",
      "step:  105\n",
      "loss: 1.7068\n",
      "a: -0.0067\n",
      "step:  106\n",
      "loss: 1.8574\n",
      "a: -0.0056\n",
      "step:  107\n",
      "loss: 1.5191\n",
      "a: -0.0045\n",
      "step:  108\n",
      "loss: 1.7885\n",
      "a: -0.0034\n",
      "step:  109\n",
      "loss: 1.9104\n",
      "a: -0.0024\n",
      "step:  110\n",
      "loss: 2.7828\n",
      "a: -0.0013\n",
      "step:  111\n",
      "loss: 1.8223\n",
      "a: -0.0004\n",
      "step:  112\n",
      "loss: 4.7906\n",
      "a: 0.0006\n",
      "step:  113\n",
      "loss: 6.4411\n",
      "a: -0.0010\n",
      "step:  114\n",
      "loss: 2.0955\n",
      "a: -0.0025\n",
      "step:  115\n",
      "loss: 1.7975\n",
      "a: -0.0039\n",
      "step:  116\n",
      "loss: 1.9079\n",
      "a: -0.0053\n",
      "step:  117\n",
      "loss: 1.7804\n",
      "a: -0.0066\n",
      "step:  118\n",
      "loss: 1.7188\n",
      "a: -0.0079\n",
      "step:  119\n",
      "loss: 2.3176\n",
      "a: -0.0091\n",
      "step:  120\n",
      "loss: 1.9437\n",
      "a: -0.0102\n",
      "step:  121\n",
      "loss: 15.0140\n",
      "a: -0.0114\n",
      "step:  122\n",
      "loss: 2.4966\n",
      "a: -0.0125\n",
      "step:  123\n",
      "loss: 2.4749\n",
      "a: -0.0136\n",
      "step:  124\n",
      "loss: 2.6912\n",
      "a: -0.0147\n",
      "step:  125\n",
      "loss: 2.5219\n",
      "a: -0.0157\n",
      "step:  126\n",
      "loss: 3.4226\n",
      "a: -0.0166\n",
      "step:  127\n",
      "loss: 3.7123\n",
      "a: -0.0175\n",
      "step:  128\n",
      "loss: 4.0949\n",
      "a: -0.0183\n",
      "step:  129\n",
      "loss: 3.3054\n",
      "a: -0.0191\n",
      "step:  130\n",
      "loss: 4.0454\n",
      "a: -0.0197\n",
      "step:  131\n",
      "loss: 3.9607\n",
      "a: -0.0204\n",
      "step:  132\n",
      "loss: 5.1968\n",
      "a: -0.0209\n",
      "step:  133\n",
      "loss: 4.4655\n",
      "a: -0.0214\n",
      "step:  134\n",
      "loss: 5.0004\n",
      "a: -0.0217\n",
      "step:  135\n",
      "loss: 4.9179\n",
      "a: -0.0220\n",
      "step:  136\n",
      "loss: 4.2852\n",
      "a: -0.0221\n",
      "step:  137\n",
      "loss: 5.6144\n",
      "a: -0.0221\n",
      "step:  138\n",
      "loss: 4.5734\n",
      "a: -0.0220\n",
      "step:  139\n",
      "loss: 4.8852\n",
      "a: -0.0217\n",
      "step:  140\n",
      "loss: 4.3494\n",
      "a: -0.0213\n",
      "step:  141\n",
      "loss: 4.0266\n",
      "a: -0.0206\n",
      "step:  142\n",
      "loss: 4.0772\n",
      "a: -0.0198\n",
      "step:  143\n",
      "loss: 3.1464\n",
      "a: -0.0189\n",
      "step:  144\n",
      "loss: 3.6832\n",
      "a: -0.0178\n",
      "step:  145\n",
      "loss: 4.5697\n",
      "a: -0.0167\n",
      "step:  146\n",
      "loss: 2.4048\n",
      "a: -0.0154\n",
      "step:  147\n",
      "loss: 3.0046\n",
      "a: -0.0142\n",
      "step:  148\n",
      "loss: 2.2820\n",
      "a: -0.0129\n",
      "step:  149\n",
      "loss: 1.9993\n",
      "a: -0.0116\n",
      "step:  150\n",
      "loss: 2.0083\n",
      "a: -0.0103\n",
      "step:  151\n",
      "loss: 1.8193\n",
      "a: -0.0091\n",
      "step:  152\n",
      "loss: 1.7756\n",
      "a: -0.0078\n",
      "step:  153\n",
      "loss: 2.5521\n",
      "a: -0.0066\n",
      "step:  154\n",
      "loss: 1.6922\n",
      "a: -0.0055\n",
      "step:  155\n",
      "loss: 4.5244\n",
      "a: -0.0043\n",
      "step:  156\n",
      "loss: 1.8708\n",
      "a: -0.0032\n",
      "step:  157\n",
      "loss: 1.7860\n",
      "a: -0.0021\n",
      "step:  158\n",
      "loss: 1.8749\n",
      "a: -0.0010\n",
      "step:  159\n",
      "loss: 1.7063\n",
      "a: 0.0000\n",
      "step:  160\n",
      "loss: 11.4024\n",
      "a: 0.0013\n",
      "step:  161\n",
      "loss: 3.2777\n",
      "a: 0.0004\n",
      "step:  162\n",
      "loss: 1.7792\n",
      "a: -0.0017\n",
      "step:  163\n",
      "loss: 5.7285\n",
      "a: -0.0035\n",
      "step:  164\n",
      "loss: 1.8591\n",
      "a: -0.0052\n",
      "step:  165\n",
      "loss: 2.2865\n",
      "a: -0.0068\n",
      "step:  166\n",
      "loss: 1.8551\n",
      "a: -0.0082\n",
      "step:  167\n",
      "loss: 2.3937\n",
      "a: -0.0096\n",
      "step:  168\n",
      "loss: 2.0933\n",
      "a: -0.0109\n",
      "step:  169\n",
      "loss: 2.6789\n",
      "a: -0.0121\n",
      "step:  170\n",
      "loss: 3.3641\n",
      "a: -0.0133\n",
      "step:  171\n",
      "loss: 2.6924\n",
      "a: -0.0145\n",
      "step:  172\n",
      "loss: 2.7334\n",
      "a: -0.0156\n",
      "step:  173\n",
      "loss: 3.3727\n",
      "a: -0.0167\n",
      "step:  174\n",
      "loss: 3.7323\n",
      "a: -0.0178\n",
      "step:  175\n",
      "loss: 3.8484\n",
      "a: -0.0188\n",
      "step:  176\n",
      "loss: 3.4632\n",
      "a: -0.0197\n",
      "step:  177\n",
      "loss: 4.4685\n",
      "a: -0.0207\n",
      "step:  178\n",
      "loss: 4.6221\n",
      "a: -0.0216\n",
      "step:  179\n",
      "loss: 5.3208\n",
      "a: -0.0225\n",
      "step:  180\n",
      "loss: 5.2722\n",
      "a: -0.0234\n",
      "step:  181\n",
      "loss: 7.3584\n",
      "a: -0.0242\n",
      "step:  182\n",
      "loss: 6.2890\n",
      "a: -0.0250\n",
      "step:  183\n",
      "loss: 5.4666\n",
      "a: -0.0257\n",
      "step:  184\n",
      "loss: 7.6184\n",
      "a: -0.0265\n",
      "step:  185\n",
      "loss: 6.9602\n",
      "a: -0.0271\n",
      "step:  186\n",
      "loss: 5.8382\n",
      "a: -0.0277\n",
      "step:  187\n",
      "loss: 7.7491\n",
      "a: -0.0283\n",
      "step:  188\n",
      "loss: 8.8363\n",
      "a: -0.0288\n",
      "step:  189\n",
      "loss: 7.5183\n",
      "a: -0.0292\n",
      "step:  190\n",
      "loss: 8.0290\n",
      "a: -0.0295\n",
      "step:  191\n",
      "loss: 7.1833\n",
      "a: -0.0298\n",
      "step:  192\n",
      "loss: 7.3376\n",
      "a: -0.0299\n",
      "step:  193\n",
      "loss: 7.4277\n",
      "a: -0.0300\n",
      "step:  194\n",
      "loss: 10.8164\n",
      "a: -0.0300\n",
      "step:  195\n",
      "loss: 6.2381\n",
      "a: -0.0299\n",
      "step:  196\n",
      "loss: 8.3446\n",
      "a: -0.0297\n",
      "step:  197\n",
      "loss: 9.4137\n",
      "a: -0.0293\n",
      "step:  198\n",
      "loss: 6.5753\n",
      "a: -0.0288\n",
      "step:  199\n",
      "loss: 6.5563\n",
      "a: -0.0281\n",
      "step:  200\n",
      "loss: 5.6234\n",
      "a: -0.0273\n",
      "step:  201\n",
      "loss: 6.3335\n",
      "a: -0.0263\n",
      "step:  202\n",
      "loss: 4.8065\n",
      "a: -0.0253\n",
      "step:  203\n",
      "loss: 4.5791\n",
      "a: -0.0241\n",
      "step:  204\n",
      "loss: 4.5181\n",
      "a: -0.0229\n",
      "step:  205\n",
      "loss: 4.7398\n",
      "a: -0.0217\n",
      "step:  206\n",
      "loss: 3.0130\n",
      "a: -0.0204\n",
      "step:  207\n",
      "loss: 3.9698\n",
      "a: -0.0191\n",
      "step:  208\n",
      "loss: 3.2476\n",
      "a: -0.0178\n",
      "step:  209\n",
      "loss: 2.7581\n",
      "a: -0.0165\n",
      "step:  210\n",
      "loss: 2.4339\n",
      "a: -0.0152\n",
      "step:  211\n",
      "loss: 2.0958\n",
      "a: -0.0140\n",
      "step:  212\n",
      "loss: 2.3050\n",
      "a: -0.0127\n",
      "step:  213\n",
      "loss: 2.9807\n",
      "a: -0.0115\n",
      "step:  214\n",
      "loss: 2.0118\n",
      "a: -0.0103\n",
      "step:  215\n",
      "loss: 2.1655\n",
      "a: -0.0091\n",
      "step:  216\n",
      "loss: 1.9005\n",
      "a: -0.0080\n",
      "step:  217\n",
      "loss: 2.7999\n",
      "a: -0.0068\n",
      "step:  218\n",
      "loss: 1.8888\n",
      "a: -0.0057\n",
      "step:  219\n",
      "loss: 2.7431\n",
      "a: -0.0046\n",
      "step:  220\n",
      "loss: 1.9449\n",
      "a: -0.0035\n",
      "step:  221\n",
      "loss: 1.9160\n",
      "a: -0.0025\n",
      "step:  222\n",
      "loss: 2.8817\n",
      "a: -0.0014\n",
      "step:  223\n",
      "loss: 1.7107\n",
      "a: -0.0004\n",
      "step:  224\n",
      "loss: 6.0909\n",
      "a: 0.0006\n",
      "step:  225\n",
      "loss: 3.8532\n",
      "a: 0.0005\n",
      "step:  226\n",
      "loss: 2.5442\n",
      "a: -0.0006\n",
      "step:  227\n",
      "loss: 1.8957\n",
      "a: -0.0017\n",
      "step:  228\n",
      "loss: 1.8567\n",
      "a: -0.0027\n",
      "step:  229\n",
      "loss: 1.6187\n",
      "a: -0.0038\n",
      "step:  230\n",
      "loss: 1.9826\n",
      "a: -0.0049\n",
      "step:  231\n",
      "loss: 1.9114\n",
      "a: -0.0060\n",
      "step:  232\n",
      "loss: 1.9908\n",
      "a: -0.0070\n",
      "step:  233\n",
      "loss: 2.8211\n",
      "a: -0.0081\n",
      "step:  234\n",
      "loss: 2.2837\n",
      "a: -0.0091\n",
      "step:  235\n",
      "loss: 1.9186\n",
      "a: -0.0102\n",
      "step:  236\n",
      "loss: 2.0546\n",
      "a: -0.0112\n",
      "step:  237\n",
      "loss: 2.0113\n",
      "a: -0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  238\n",
      "loss: 2.3429\n",
      "a: -0.0133\n",
      "step:  239\n",
      "loss: 4.1186\n",
      "a: -0.0142\n",
      "step:  240\n",
      "loss: 2.6592\n",
      "a: -0.0152\n",
      "step:  241\n",
      "loss: 2.8555\n",
      "a: -0.0161\n",
      "step:  242\n",
      "loss: 3.5493\n",
      "a: -0.0170\n",
      "step:  243\n",
      "loss: 3.4798\n",
      "a: -0.0179\n",
      "step:  244\n",
      "loss: 3.4678\n",
      "a: -0.0187\n",
      "step:  245\n",
      "loss: 4.0456\n",
      "a: -0.0195\n",
      "step:  246\n",
      "loss: 4.9414\n",
      "a: -0.0203\n",
      "step:  247\n",
      "loss: 4.3551\n",
      "a: -0.0210\n",
      "step:  248\n",
      "loss: 4.3241\n",
      "a: -0.0216\n",
      "step:  249\n",
      "loss: 4.5995\n",
      "a: -0.0222\n",
      "step:  250\n",
      "loss: 5.7498\n",
      "a: -0.0228\n",
      "step:  251\n",
      "loss: 6.6830\n",
      "a: -0.0233\n",
      "step:  252\n",
      "loss: 6.9842\n",
      "a: -0.0237\n",
      "step:  253\n",
      "loss: 5.1153\n",
      "a: -0.0240\n",
      "step:  254\n",
      "loss: 5.8529\n",
      "a: -0.0243\n",
      "step:  255\n",
      "loss: 5.4233\n",
      "a: -0.0244\n",
      "step:  256\n",
      "loss: 4.8622\n",
      "a: -0.0244\n",
      "step:  257\n",
      "loss: 6.5796\n",
      "a: -0.0243\n",
      "step:  258\n",
      "loss: 6.2915\n",
      "a: -0.0241\n",
      "step:  259\n",
      "loss: 4.3274\n",
      "a: -0.0237\n",
      "step:  260\n",
      "loss: 4.3350\n",
      "a: -0.0232\n",
      "step:  261\n",
      "loss: 6.8774\n",
      "a: -0.0226\n",
      "step:  262\n",
      "loss: 4.7455\n",
      "a: -0.0218\n",
      "step:  263\n",
      "loss: 3.7258\n",
      "a: -0.0209\n",
      "step:  264\n",
      "loss: 3.7092\n",
      "a: -0.0198\n",
      "step:  265\n",
      "loss: 3.4305\n",
      "a: -0.0187\n",
      "step:  266\n",
      "loss: 2.8643\n",
      "a: -0.0175\n",
      "step:  267\n",
      "loss: 3.1406\n",
      "a: -0.0163\n",
      "step:  268\n",
      "loss: 2.7579\n",
      "a: -0.0150\n",
      "step:  269\n",
      "loss: 1.6568\n",
      "a: -0.0138\n",
      "step:  270\n",
      "loss: 2.2474\n",
      "a: -0.0126\n",
      "step:  271\n",
      "loss: 2.2312\n",
      "a: -0.0113\n",
      "step:  272\n",
      "loss: 2.6884\n",
      "a: -0.0101\n",
      "step:  273\n",
      "loss: 2.0875\n",
      "a: -0.0089\n",
      "step:  274\n",
      "loss: 1.8246\n",
      "a: -0.0078\n",
      "step:  275\n",
      "loss: 2.4175\n",
      "a: -0.0066\n",
      "step:  276\n",
      "loss: 1.6182\n",
      "a: -0.0055\n",
      "step:  277\n",
      "loss: 2.1251\n",
      "a: -0.0044\n",
      "step:  278\n",
      "loss: 2.4804\n",
      "a: -0.0033\n",
      "step:  279\n",
      "loss: 1.7358\n",
      "a: -0.0022\n",
      "step:  280\n",
      "loss: 3.3266\n",
      "a: -0.0011\n",
      "step:  281\n",
      "loss: 2.1670\n",
      "a: -0.0001\n",
      "step:  282\n",
      "loss: 4.9441\n",
      "a: 0.0009\n",
      "step:  283\n",
      "loss: 4.2727\n",
      "a: 0.0007\n",
      "step:  284\n",
      "loss: 2.0701\n",
      "a: -0.0008\n",
      "step:  285\n",
      "loss: 1.9077\n",
      "a: -0.0023\n",
      "step:  286\n",
      "loss: 2.0503\n",
      "a: -0.0037\n",
      "step:  287\n",
      "loss: 1.8153\n",
      "a: -0.0050\n",
      "step:  288\n",
      "loss: 1.9766\n",
      "a: -0.0063\n",
      "step:  289\n",
      "loss: 1.7255\n",
      "a: -0.0075\n",
      "step:  290\n",
      "loss: 1.9097\n",
      "a: -0.0087\n",
      "step:  291\n",
      "loss: 2.1717\n",
      "a: -0.0099\n",
      "step:  292\n",
      "loss: 2.0138\n",
      "a: -0.0110\n",
      "step:  293\n",
      "loss: 2.3214\n",
      "a: -0.0121\n",
      "step:  294\n",
      "loss: 2.1037\n",
      "a: -0.0132\n",
      "step:  295\n",
      "loss: 2.0748\n",
      "a: -0.0143\n",
      "step:  296\n",
      "loss: 2.7760\n",
      "a: -0.0153\n",
      "step:  297\n",
      "loss: 2.8185\n",
      "a: -0.0164\n",
      "step:  298\n",
      "loss: 2.8876\n",
      "a: -0.0173\n",
      "step:  299\n",
      "loss: 3.1428\n",
      "a: -0.0183\n",
      "step:  300\n",
      "loss: 4.1835\n",
      "a: -0.0192\n",
      "step:  301\n",
      "loss: 3.7923\n",
      "a: -0.0201\n",
      "step:  302\n",
      "loss: 3.7379\n",
      "a: -0.0209\n",
      "step:  303\n",
      "loss: 3.8283\n",
      "a: -0.0217\n",
      "step:  304\n",
      "loss: 3.9245\n",
      "a: -0.0225\n",
      "step:  305\n",
      "loss: 4.3683\n",
      "a: -0.0233\n",
      "step:  306\n",
      "loss: 5.0079\n",
      "a: -0.0240\n",
      "step:  307\n",
      "loss: 6.1064\n",
      "a: -0.0246\n",
      "step:  308\n",
      "loss: 4.9110\n",
      "a: -0.0252\n",
      "step:  309\n",
      "loss: 5.2536\n",
      "a: -0.0258\n",
      "step:  310\n",
      "loss: 5.4527\n",
      "a: -0.0263\n",
      "step:  311\n",
      "loss: 6.3319\n",
      "a: -0.0267\n",
      "step:  312\n",
      "loss: 6.8353\n",
      "a: -0.0270\n",
      "step:  313\n",
      "loss: 6.9795\n",
      "a: -0.0273\n",
      "step:  314\n",
      "loss: 6.3911\n",
      "a: -0.0274\n",
      "step:  315\n",
      "loss: 5.8888\n",
      "a: -0.0275\n",
      "step:  316\n",
      "loss: 6.3088\n",
      "a: -0.0273\n",
      "step:  317\n",
      "loss: 7.4330\n",
      "a: -0.0271\n",
      "step:  318\n",
      "loss: 5.8241\n",
      "a: -0.0266\n",
      "step:  319\n",
      "loss: 4.8865\n",
      "a: -0.0260\n",
      "step:  320\n",
      "loss: 5.3292\n",
      "a: -0.0253\n",
      "step:  321\n",
      "loss: 4.1944\n",
      "a: -0.0244\n",
      "step:  322\n",
      "loss: 4.6823\n",
      "a: -0.0234\n",
      "step:  323\n",
      "loss: 4.4549\n",
      "a: -0.0223\n",
      "step:  324\n",
      "loss: 3.5608\n",
      "a: -0.0210\n",
      "step:  325\n",
      "loss: 3.7388\n",
      "a: -0.0198\n",
      "step:  326\n",
      "loss: 3.1346\n",
      "a: -0.0184\n",
      "step:  327\n",
      "loss: 2.8988\n",
      "a: -0.0171\n",
      "step:  328\n",
      "loss: 2.7745\n",
      "a: -0.0158\n",
      "step:  329\n",
      "loss: 2.5749\n",
      "a: -0.0145\n",
      "step:  330\n",
      "loss: 2.1533\n",
      "a: -0.0132\n",
      "step:  331\n",
      "loss: 2.1578\n",
      "a: -0.0119\n",
      "step:  332\n",
      "loss: 1.9630\n",
      "a: -0.0106\n",
      "step:  333\n",
      "loss: 1.9605\n",
      "a: -0.0094\n",
      "step:  334\n",
      "loss: 1.8389\n",
      "a: -0.0082\n",
      "step:  335\n",
      "loss: 1.8509\n",
      "a: -0.0070\n",
      "step:  336\n",
      "loss: 1.9770\n",
      "a: -0.0058\n",
      "step:  337\n",
      "loss: 1.9619\n",
      "a: -0.0047\n",
      "step:  338\n",
      "loss: 3.9435\n",
      "a: -0.0036\n",
      "step:  339\n",
      "loss: 2.1418\n",
      "a: -0.0025\n",
      "step:  340\n",
      "loss: 1.8662\n",
      "a: -0.0014\n",
      "step:  341\n",
      "loss: 2.1180\n",
      "a: -0.0004\n",
      "step:  342\n",
      "loss: 4.5421\n",
      "a: 0.0007\n",
      "step:  343\n",
      "loss: 3.1724\n",
      "a: 0.0004\n",
      "step:  344\n",
      "loss: 2.1919\n",
      "a: -0.0010\n",
      "step:  345\n",
      "loss: 1.8002\n",
      "a: -0.0023\n",
      "step:  346\n",
      "loss: 2.0773\n",
      "a: -0.0036\n",
      "step:  347\n",
      "loss: 2.0240\n",
      "a: -0.0048\n",
      "step:  348\n",
      "loss: 2.4443\n",
      "a: -0.0061\n",
      "step:  349\n",
      "loss: 1.7934\n",
      "a: -0.0073\n",
      "step:  350\n",
      "loss: 2.0335\n",
      "a: -0.0084\n",
      "step:  351\n",
      "loss: 2.2978\n",
      "a: -0.0096\n",
      "step:  352\n",
      "loss: 2.0163\n",
      "a: -0.0107\n",
      "step:  353\n",
      "loss: 2.0382\n",
      "a: -0.0118\n",
      "step:  354\n",
      "loss: 1.9623\n",
      "a: -0.0129\n",
      "step:  355\n",
      "loss: 2.0220\n",
      "a: -0.0140\n",
      "step:  356\n",
      "loss: 2.1379\n",
      "a: -0.0150\n",
      "step:  357\n",
      "loss: 2.6408\n",
      "a: -0.0160\n",
      "step:  358\n",
      "loss: 3.2649\n",
      "a: -0.0170\n",
      "step:  359\n",
      "loss: 2.5964\n",
      "a: -0.0180\n",
      "step:  360\n",
      "loss: 3.2290\n",
      "a: -0.0189\n",
      "step:  361\n",
      "loss: 3.6584\n",
      "a: -0.0198\n",
      "step:  362\n",
      "loss: 3.8350\n",
      "a: -0.0207\n",
      "step:  363\n",
      "loss: 3.7669\n",
      "a: -0.0216\n",
      "step:  364\n",
      "loss: 3.4717\n",
      "a: -0.0224\n",
      "step:  365\n",
      "loss: 4.1464\n",
      "a: -0.0232\n",
      "step:  366\n",
      "loss: 4.5682\n",
      "a: -0.0239\n",
      "step:  367\n",
      "loss: 4.6585\n",
      "a: -0.0246\n",
      "step:  368\n",
      "loss: 4.6211\n",
      "a: -0.0253\n",
      "step:  369\n",
      "loss: 6.1418\n",
      "a: -0.0259\n",
      "step:  370\n",
      "loss: 5.3813\n",
      "a: -0.0265\n",
      "step:  371\n",
      "loss: 5.5698\n",
      "a: -0.0270\n",
      "step:  372\n",
      "loss: 5.4091\n",
      "a: -0.0274\n",
      "step:  373\n",
      "loss: 7.1837\n",
      "a: -0.0278\n",
      "step:  374\n",
      "loss: 6.0650\n",
      "a: -0.0281\n",
      "step:  375\n",
      "loss: 6.2172\n",
      "a: -0.0283\n",
      "step:  376\n",
      "loss: 6.1534\n",
      "a: -0.0284\n",
      "step:  377\n",
      "loss: 5.4486\n",
      "a: -0.0284\n",
      "step:  378\n",
      "loss: 5.0883\n",
      "a: -0.0283\n",
      "step:  379\n",
      "loss: 5.0352\n",
      "a: -0.0281\n",
      "step:  380\n",
      "loss: 5.9248\n",
      "a: -0.0277\n",
      "step:  381\n",
      "loss: 6.0717\n",
      "a: -0.0273\n",
      "step:  382\n",
      "loss: 4.5527\n",
      "a: -0.0267\n",
      "step:  383\n",
      "loss: 5.7180\n",
      "a: -0.0259\n",
      "step:  384\n",
      "loss: 4.4703\n",
      "a: -0.0251\n",
      "step:  385\n",
      "loss: 4.3072\n",
      "a: -0.0241\n",
      "step:  386\n",
      "loss: 4.1437\n",
      "a: -0.0230\n",
      "step:  387\n",
      "loss: 3.5647\n",
      "a: -0.0218\n",
      "step:  388\n",
      "loss: 2.9731\n",
      "a: -0.0206\n",
      "step:  389\n",
      "loss: 3.1059\n",
      "a: -0.0193\n",
      "step:  390\n",
      "loss: 2.7179\n",
      "a: -0.0180\n",
      "step:  391\n",
      "loss: 2.1647\n",
      "a: -0.0167\n",
      "step:  392\n",
      "loss: 2.4585\n",
      "a: -0.0154\n",
      "step:  393\n",
      "loss: 2.4248\n",
      "a: -0.0142\n",
      "step:  394\n",
      "loss: 2.4923\n",
      "a: -0.0129\n",
      "step:  395\n",
      "loss: 2.2423\n",
      "a: -0.0117\n",
      "step:  396\n",
      "loss: 2.0270\n",
      "a: -0.0104\n",
      "step:  397\n",
      "loss: 1.9267\n",
      "a: -0.0092\n",
      "step:  398\n",
      "loss: 2.1546\n",
      "a: -0.0080\n",
      "step:  399\n",
      "loss: 1.7426\n",
      "a: -0.0069\n",
      "step:  400\n",
      "loss: 1.8854\n",
      "a: -0.0058\n",
      "step:  401\n",
      "loss: 3.5518\n",
      "a: -0.0046\n",
      "step:  402\n",
      "loss: 2.3317\n",
      "a: -0.0035\n",
      "step:  403\n",
      "loss: 2.0041\n",
      "a: -0.0024\n",
      "step:  404\n",
      "loss: 2.8087\n",
      "a: -0.0013\n",
      "step:  405\n",
      "loss: 2.0206\n",
      "a: -0.0003\n",
      "step:  406\n",
      "loss: 5.6223\n",
      "a: 0.0008\n",
      "step:  407\n",
      "loss: 4.3144\n",
      "a: 0.0006\n",
      "step:  408\n",
      "loss: 1.9737\n",
      "a: -0.0010\n",
      "step:  409\n",
      "loss: 1.9458\n",
      "a: -0.0024\n",
      "step:  410\n",
      "loss: 1.9429\n",
      "a: -0.0037\n",
      "step:  411\n",
      "loss: 2.8913\n",
      "a: -0.0050\n",
      "step:  412\n",
      "loss: 2.2064\n",
      "a: -0.0063\n",
      "step:  413\n",
      "loss: 1.9807\n",
      "a: -0.0076\n",
      "step:  414\n",
      "loss: 1.9371\n",
      "a: -0.0088\n",
      "step:  415\n",
      "loss: 2.1470\n",
      "a: -0.0099\n",
      "step:  416\n",
      "loss: 2.2097\n",
      "a: -0.0111\n",
      "step:  417\n",
      "loss: 2.3288\n",
      "a: -0.0122\n",
      "step:  418\n",
      "loss: 3.0113\n",
      "a: -0.0133\n",
      "step:  419\n",
      "loss: 2.4829\n",
      "a: -0.0143\n",
      "step:  420\n",
      "loss: 2.6363\n",
      "a: -0.0154\n",
      "step:  421\n",
      "loss: 2.3067\n",
      "a: -0.0163\n",
      "step:  422\n",
      "loss: 2.9710\n",
      "a: -0.0173\n",
      "step:  423\n",
      "loss: 3.5301\n",
      "a: -0.0183\n",
      "step:  424\n",
      "loss: 3.8755\n",
      "a: -0.0192\n",
      "step:  425\n",
      "loss: 3.9684\n",
      "a: -0.0200\n",
      "step:  426\n",
      "loss: 3.2403\n",
      "a: -0.0209\n",
      "step:  427\n",
      "loss: 4.1009\n",
      "a: -0.0217\n",
      "step:  428\n",
      "loss: 4.5039\n",
      "a: -0.0225\n",
      "step:  429\n",
      "loss: 5.0757\n",
      "a: -0.0232\n",
      "step:  430\n",
      "loss: 4.8860\n",
      "a: -0.0239\n",
      "step:  431\n",
      "loss: 5.0044\n",
      "a: -0.0245\n",
      "step:  432\n",
      "loss: 4.6041\n",
      "a: -0.0251\n",
      "step:  433\n",
      "loss: 6.6702\n",
      "a: -0.0257\n",
      "step:  434\n",
      "loss: 6.3371\n",
      "a: -0.0262\n",
      "step:  435\n",
      "loss: 6.0973\n",
      "a: -0.0266\n",
      "step:  436\n",
      "loss: 5.6466\n",
      "a: -0.0269\n",
      "step:  437\n",
      "loss: 5.3914\n",
      "a: -0.0272\n",
      "step:  438\n",
      "loss: 6.0183\n",
      "a: -0.0274\n",
      "step:  439\n",
      "loss: 5.4516\n",
      "a: -0.0275\n",
      "step:  440\n",
      "loss: 5.1280\n",
      "a: -0.0275\n",
      "step:  441\n",
      "loss: 5.2929\n",
      "a: -0.0274\n",
      "step:  442\n",
      "loss: 6.1551\n",
      "a: -0.0272\n",
      "step:  443\n",
      "loss: 5.9406\n",
      "a: -0.0268\n",
      "step:  444\n",
      "loss: 5.2173\n",
      "a: -0.0263\n",
      "step:  445\n",
      "loss: 3.6333\n",
      "a: -0.0256\n",
      "step:  446\n",
      "loss: 4.6856\n",
      "a: -0.0248\n",
      "step:  447\n",
      "loss: 3.7179\n",
      "a: -0.0238\n",
      "step:  448\n",
      "loss: 4.0406\n",
      "a: -0.0228\n",
      "step:  449\n",
      "loss: 3.5620\n",
      "a: -0.0217\n",
      "step:  450\n",
      "loss: 2.5914\n",
      "a: -0.0205\n",
      "step:  451\n",
      "loss: 3.0438\n",
      "a: -0.0193\n",
      "step:  452\n",
      "loss: 2.6780\n",
      "a: -0.0180\n",
      "step:  453\n",
      "loss: 2.5687\n",
      "a: -0.0168\n",
      "step:  454\n",
      "loss: 2.3746\n",
      "a: -0.0155\n",
      "step:  455\n",
      "loss: 2.3246\n",
      "a: -0.0143\n",
      "step:  456\n",
      "loss: 2.2193\n",
      "a: -0.0130\n",
      "step:  457\n",
      "loss: 1.9677\n",
      "a: -0.0118\n",
      "step:  458\n",
      "loss: 2.0417\n",
      "a: -0.0105\n",
      "step:  459\n",
      "loss: 2.1100\n",
      "a: -0.0093\n",
      "step:  460\n",
      "loss: 2.0060\n",
      "a: -0.0082\n",
      "step:  461\n",
      "loss: 1.9316\n",
      "a: -0.0070\n",
      "step:  462\n",
      "loss: 1.9811\n",
      "a: -0.0058\n",
      "step:  463\n",
      "loss: 1.8245\n",
      "a: -0.0047\n",
      "step:  464\n",
      "loss: 2.0849\n",
      "a: -0.0036\n",
      "step:  465\n",
      "loss: 2.5090\n",
      "a: -0.0025\n",
      "step:  466\n",
      "loss: 2.3761\n",
      "a: -0.0015\n",
      "step:  467\n",
      "loss: 2.0411\n",
      "a: -0.0004\n",
      "step:  468\n",
      "loss: 3.4313\n",
      "a: 0.0006\n",
      "step:  469\n",
      "loss: 5.0766\n",
      "a: 0.0006\n",
      "step:  470\n",
      "loss: 2.8136\n",
      "a: -0.0007\n",
      "step:  471\n",
      "loss: 2.1222\n",
      "a: -0.0019\n",
      "step:  472\n",
      "loss: 2.2237\n",
      "a: -0.0031\n",
      "step:  473\n",
      "loss: 1.9817\n",
      "a: -0.0043\n",
      "step:  474\n",
      "loss: 1.9214\n",
      "a: -0.0055\n",
      "step:  475\n",
      "loss: 2.6429\n",
      "a: -0.0066\n",
      "step:  476\n",
      "loss: 2.0375\n",
      "a: -0.0078\n",
      "step:  477\n",
      "loss: 2.2179\n",
      "a: -0.0089\n",
      "step:  478\n",
      "loss: 2.7307\n",
      "a: -0.0100\n",
      "step:  479\n",
      "loss: 1.9136\n",
      "a: -0.0111\n",
      "step:  480\n",
      "loss: 1.9650\n",
      "a: -0.0122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  481\n",
      "loss: 2.6574\n",
      "a: -0.0133\n",
      "step:  482\n",
      "loss: 2.2000\n",
      "a: -0.0143\n",
      "step:  483\n",
      "loss: 2.5232\n",
      "a: -0.0153\n",
      "step:  484\n",
      "loss: 2.2585\n",
      "a: -0.0163\n",
      "step:  485\n",
      "loss: 3.0406\n",
      "a: -0.0173\n",
      "step:  486\n",
      "loss: 2.5412\n",
      "a: -0.0183\n",
      "step:  487\n",
      "loss: 3.5723\n",
      "a: -0.0192\n",
      "step:  488\n",
      "loss: 2.7401\n",
      "a: -0.0201\n",
      "step:  489\n",
      "loss: 3.6369\n",
      "a: -0.0209\n",
      "step:  490\n",
      "loss: 3.3026\n",
      "a: -0.0218\n",
      "step:  491\n",
      "loss: 3.3768\n",
      "a: -0.0226\n",
      "step:  492\n",
      "loss: 3.8662\n",
      "a: -0.0233\n",
      "step:  493\n",
      "loss: 4.7595\n",
      "a: -0.0240\n",
      "step:  494\n",
      "loss: 4.8280\n",
      "a: -0.0247\n",
      "step:  495\n",
      "loss: 4.4165\n",
      "a: -0.0253\n",
      "step:  496\n",
      "loss: 5.3931\n",
      "a: -0.0258\n",
      "step:  497\n",
      "loss: 5.4893\n",
      "a: -0.0262\n",
      "step:  498\n",
      "loss: 5.2539\n",
      "a: -0.0266\n",
      "step:  499\n",
      "loss: 5.3053\n",
      "a: -0.0269\n",
      "step:  500\n",
      "loss: 6.1748\n",
      "a: -0.0271\n",
      "step:  501\n",
      "loss: 5.0958\n",
      "a: -0.0271\n",
      "step:  502\n",
      "loss: 5.1425\n",
      "a: -0.0271\n",
      "step:  503\n",
      "loss: 5.2241\n",
      "a: -0.0269\n",
      "step:  504\n",
      "loss: 3.9033\n",
      "a: -0.0266\n",
      "step:  505\n",
      "loss: 5.4040\n",
      "a: -0.0261\n",
      "step:  506\n",
      "loss: 3.7165\n",
      "a: -0.0255\n",
      "step:  507\n",
      "loss: 4.5106\n",
      "a: -0.0248\n",
      "step:  508\n",
      "loss: 4.5630\n",
      "a: -0.0239\n",
      "step:  509\n",
      "loss: 4.1567\n",
      "a: -0.0229\n",
      "step:  510\n",
      "loss: 3.9421\n",
      "a: -0.0217\n",
      "step:  511\n",
      "loss: 3.1681\n",
      "a: -0.0205\n",
      "step:  512\n",
      "loss: 2.8956\n",
      "a: -0.0193\n",
      "step:  513\n",
      "loss: 3.4936\n",
      "a: -0.0180\n",
      "step:  514\n",
      "loss: 2.4138\n",
      "a: -0.0166\n",
      "step:  515\n",
      "loss: 2.4834\n",
      "a: -0.0153\n",
      "step:  516\n",
      "loss: 2.1091\n",
      "a: -0.0140\n",
      "step:  517\n",
      "loss: 1.9786\n",
      "a: -0.0128\n",
      "step:  518\n",
      "loss: 2.0554\n",
      "a: -0.0115\n",
      "step:  519\n",
      "loss: 2.0077\n",
      "a: -0.0103\n",
      "step:  520\n",
      "loss: 2.0697\n",
      "a: -0.0091\n",
      "step:  521\n",
      "loss: 1.9477\n",
      "a: -0.0079\n",
      "step:  522\n",
      "loss: 2.1480\n",
      "a: -0.0067\n",
      "step:  523\n",
      "loss: 2.2779\n",
      "a: -0.0056\n",
      "step:  524\n",
      "loss: 1.8733\n",
      "a: -0.0044\n",
      "step:  525\n",
      "loss: 1.9281\n",
      "a: -0.0033\n",
      "step:  526\n",
      "loss: 2.6269\n",
      "a: -0.0022\n",
      "step:  527\n",
      "loss: 2.6690\n",
      "a: -0.0012\n",
      "step:  528\n",
      "loss: 1.8946\n",
      "a: -0.0002\n",
      "step:  529\n",
      "loss: 5.8208\n",
      "a: 0.0008\n",
      "step:  530\n",
      "loss: 2.0520\n",
      "a: 0.0001\n",
      "step:  531\n",
      "loss: 1.9825\n",
      "a: -0.0010\n",
      "step:  532\n",
      "loss: 2.5058\n",
      "a: -0.0021\n",
      "step:  533\n",
      "loss: 2.3298\n",
      "a: -0.0032\n",
      "step:  534\n",
      "loss: 2.2520\n",
      "a: -0.0044\n",
      "step:  535\n",
      "loss: 1.9620\n",
      "a: -0.0055\n",
      "step:  536\n",
      "loss: 1.9418\n",
      "a: -0.0066\n",
      "step:  537\n",
      "loss: 2.1290\n",
      "a: -0.0077\n",
      "step:  538\n",
      "loss: 2.0883\n",
      "a: -0.0088\n",
      "step:  539\n",
      "loss: 1.7134\n",
      "a: -0.0099\n",
      "step:  540\n",
      "loss: 2.0457\n",
      "a: -0.0109\n",
      "step:  541\n",
      "loss: 2.1886\n",
      "a: -0.0119\n",
      "step:  542\n",
      "loss: 2.0588\n",
      "a: -0.0130\n",
      "step:  543\n",
      "loss: 2.2014\n",
      "a: -0.0140\n",
      "step:  544\n",
      "loss: 2.4902\n",
      "a: -0.0150\n",
      "step:  545\n",
      "loss: 3.0350\n",
      "a: -0.0160\n",
      "step:  546\n",
      "loss: 3.0028\n",
      "a: -0.0169\n",
      "step:  547\n",
      "loss: 2.9815\n",
      "a: -0.0177\n",
      "step:  548\n",
      "loss: 3.2968\n",
      "a: -0.0186\n",
      "step:  549\n",
      "loss: 3.2082\n",
      "a: -0.0194\n",
      "step:  550\n",
      "loss: 3.6387\n",
      "a: -0.0201\n",
      "step:  551\n",
      "loss: 3.9815\n",
      "a: -0.0208\n",
      "step:  552\n",
      "loss: 3.4525\n",
      "a: -0.0215\n",
      "step:  553\n",
      "loss: 4.2547\n",
      "a: -0.0220\n",
      "step:  554\n",
      "loss: 3.9007\n",
      "a: -0.0226\n",
      "step:  555\n",
      "loss: 4.8983\n",
      "a: -0.0230\n",
      "step:  556\n",
      "loss: 5.3806\n",
      "a: -0.0234\n",
      "step:  557\n",
      "loss: 4.9145\n",
      "a: -0.0236\n",
      "step:  558\n",
      "loss: 4.6411\n",
      "a: -0.0237\n",
      "step:  559\n",
      "loss: 4.5506\n",
      "a: -0.0237\n",
      "step:  560\n",
      "loss: 4.6457\n",
      "a: -0.0235\n",
      "step:  561\n",
      "loss: 3.7283\n",
      "a: -0.0232\n",
      "step:  562\n",
      "loss: 4.5907\n",
      "a: -0.0227\n",
      "step:  563\n",
      "loss: 3.7951\n",
      "a: -0.0221\n",
      "step:  564\n",
      "loss: 4.0209\n",
      "a: -0.0214\n",
      "step:  565\n",
      "loss: 3.1292\n",
      "a: -0.0206\n",
      "step:  566\n",
      "loss: 3.2049\n",
      "a: -0.0196\n",
      "step:  567\n",
      "loss: 3.0137\n",
      "a: -0.0185\n",
      "step:  568\n",
      "loss: 2.5970\n",
      "a: -0.0173\n",
      "step:  569\n",
      "loss: 2.6248\n",
      "a: -0.0161\n",
      "step:  570\n",
      "loss: 3.5774\n",
      "a: -0.0149\n",
      "step:  571\n",
      "loss: 2.3446\n",
      "a: -0.0137\n",
      "step:  572\n",
      "loss: 2.2417\n",
      "a: -0.0125\n",
      "step:  573\n",
      "loss: 1.8481\n",
      "a: -0.0112\n",
      "step:  574\n",
      "loss: 2.1158\n",
      "a: -0.0100\n",
      "step:  575\n",
      "loss: 1.8563\n",
      "a: -0.0089\n",
      "step:  576\n",
      "loss: 2.0574\n",
      "a: -0.0077\n",
      "step:  577\n",
      "loss: 2.0506\n",
      "a: -0.0066\n",
      "step:  578\n",
      "loss: 2.0024\n",
      "a: -0.0055\n",
      "step:  579\n",
      "loss: 2.6163\n",
      "a: -0.0043\n",
      "step:  580\n",
      "loss: 2.0491\n",
      "a: -0.0033\n",
      "step:  581\n",
      "loss: 2.8278\n",
      "a: -0.0022\n",
      "step:  582\n",
      "loss: 2.4489\n",
      "a: -0.0012\n",
      "step:  583\n",
      "loss: 2.0309\n",
      "a: -0.0002\n",
      "step:  584\n",
      "loss: 3.5467\n",
      "a: 0.0007\n",
      "step:  585\n",
      "loss: 2.4493\n",
      "a: 0.0002\n",
      "step:  586\n",
      "loss: 2.0244\n",
      "a: -0.0014\n",
      "step:  587\n",
      "loss: 1.9929\n",
      "a: -0.0030\n",
      "step:  588\n",
      "loss: 2.0677\n",
      "a: -0.0044\n",
      "step:  589\n",
      "loss: 2.3420\n",
      "a: -0.0058\n",
      "step:  590\n",
      "loss: 2.7979\n",
      "a: -0.0072\n",
      "step:  591\n",
      "loss: 2.1503\n",
      "a: -0.0085\n",
      "step:  592\n",
      "loss: 1.9210\n",
      "a: -0.0098\n",
      "step:  593\n",
      "loss: 2.3125\n",
      "a: -0.0110\n",
      "step:  594\n",
      "loss: 2.2227\n",
      "a: -0.0122\n",
      "step:  595\n",
      "loss: 2.6529\n",
      "a: -0.0134\n",
      "step:  596\n",
      "loss: 2.1564\n",
      "a: -0.0145\n",
      "step:  597\n",
      "loss: 2.4523\n",
      "a: -0.0156\n",
      "step:  598\n",
      "loss: 3.0177\n",
      "a: -0.0166\n",
      "step:  599\n",
      "loss: 2.8246\n",
      "a: -0.0176\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

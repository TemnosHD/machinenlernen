{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 50 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in trainset and testset. The trainset consists of 60000 images, the testset of 10000 imgs.\n",
    "\n",
    "trainset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "testset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = [str(i) for i in range(10)]\n",
    "\n",
    "def imshow(img, title=\"\", cmap = \"Greys_r\"): #convert tensor to image\n",
    "    plt.title(title)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    numpyimg = img.numpy()[0]\n",
    "    plt.imshow(numpyimg, cmap = cmap)\n",
    "    \n",
    "\n",
    "def display_10_images_from_dataset(dataset, class_names):\n",
    "    \"\"\"\n",
    "    plots 10 randomly chosen images from a given dataset\n",
    "    \"\"\"\n",
    "    display = [] #holds tuples of image and respective label\n",
    "    for _ in range(10):\n",
    "        index = np.random.randint(0,len(dataset)+1)\n",
    "        display.append(dataset[index])\n",
    "    \n",
    "    nr = 1\n",
    "    fig, axes = plt.subplots(2,5,sharex='col',sharey='row', figsize = (14,9))\n",
    "    for image, label in display:\n",
    "        axes[(nr-1)//5][(nr-1)%5] = plt.subplot(2,5,nr)\n",
    "        plt.title(class_names[label], fontsize = 16)\n",
    "        imshow(image, title = class_names[label])\n",
    "        nr+=1\n",
    "    fig.subplots_adjust(hspace=-0.4)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[0:5]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in fig.axes[1:5]+fig.axes[6:]], visible=False)\n",
    "    plt.show()\n",
    "    plt.savefig(\"previewMNIST.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAFtCAYAAADccl8mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1118a1208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e404e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_10_images_from_dataset(testset, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_basic(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval, model, p1=0.8, p2=0.7):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = model(data.reshape(mb_size, 784), w_h, w_h2, w_o, p1, p2)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader, model):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data.reshape(mb_size, 784), w_h, w_h2, w_o, 1.0, 1.0)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.4408\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.3066\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.4402\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.2579\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.2007\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.2997\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 9388/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.2579\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.2185\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.3849\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.1824\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.0942\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.2576\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 9486/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.1828\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.0530\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.0865\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.1963\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.1528\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.3935\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 9468/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.0341\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.1429\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.3236\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.4762\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.1533\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.3359\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 9454/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.3076\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.0917\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.0599\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.0643\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.1580\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.0222\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.1595\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.1635\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.0779\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.3226\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.2386\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.2976\n",
      "\n",
      "Test set: Average loss: 0.0049, Accuracy: 9464/10000 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.4710\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.2307\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.2651\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.2439\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.1469\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.3424\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 9445/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.0929\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.0463\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.0899\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.1672\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.0908\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.1478\n",
      "\n",
      "Test set: Average loss: 0.0054, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.4364\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.1221\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.1349\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.2647\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.1902\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.1354\n",
      "\n",
      "Test set: Average loss: 0.0055, Accuracy: 9432/10000 (94%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.4092\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.0839\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.1617\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.0785\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.1088\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.1450\n",
      "\n",
      "Test set: Average loss: 0.0057, Accuracy: 9440/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(0,10):\n",
    "    train(trainloader, epoch+1, 200, model_basic)\n",
    "    test(testloader, model_basic)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not observe an improvement of the network and each epoch takes a significant amount of computational time, we stop after 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dropout(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.7383\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.4138\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.4144\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.2107\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.4385\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.2747\n",
      "\n",
      "Test set: Average loss: 0.0044, Accuracy: 9341/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.4805\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.3889\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.4626\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.4177\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.4522\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.5430\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 9373/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.6419\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.3810\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.5625\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.4755\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.3418\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.6732\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 9280/10000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.7448\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.4369\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.4183\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.7162\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.3073\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.3249\n",
      "\n",
      "Test set: Average loss: 0.0050, Accuracy: 9250/10000 (92%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.4597\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.6151\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.6245\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.2370\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.5193\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.6582\n",
      "\n",
      "Test set: Average loss: 0.0054, Accuracy: 9246/10000 (92%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.5256\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.3703\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.1063\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.6956\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.5589\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.6975\n",
      "\n",
      "Test set: Average loss: 0.0055, Accuracy: 9258/10000 (92%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.5182\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.6936\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.4678\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.3183\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.4963\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.2943\n",
      "\n",
      "Test set: Average loss: 0.0060, Accuracy: 9142/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.3692\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.8300\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.6805\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.7760\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.6545\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.0338\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 9182/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.6173\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.4710\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.8373\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.4592\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.7002\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.4411\n",
      "\n",
      "Test set: Average loss: 0.0066, Accuracy: 9148/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.4531\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.5780\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.2168\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.5786\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.6915\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 3.0317\n",
      "\n",
      "Test set: Average loss: 0.0070, Accuracy: 8991/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(0,10):\n",
    "    train(trainloader, epoch+1, 200, model_dropout)\n",
    "    test(testloader, model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "Dropout should in theory improve a networks performance since it prevents interdependencies between different neurons and thereby reduces overfitting as the network learns more robust features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prelu(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "a = torch.tensor([+0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 2.8695\n",
      "a: 0.1000\n",
      "step:  1\n",
      "loss: 2.6444\n",
      "a: 0.1003\n",
      "step:  2\n",
      "loss: 2.2642\n",
      "a: 0.1006\n",
      "step:  3\n",
      "loss: 2.1641\n",
      "a: 0.1003\n",
      "step:  4\n",
      "loss: 2.2761\n",
      "a: 0.0999\n",
      "step:  5\n",
      "loss: 1.9386\n",
      "a: 0.0997\n",
      "step:  6\n",
      "loss: 1.6756\n",
      "a: 0.0994\n",
      "step:  7\n",
      "loss: 1.5967\n",
      "a: 0.0992\n",
      "step:  8\n",
      "loss: 1.3209\n",
      "a: 0.0991\n",
      "step:  9\n",
      "loss: 1.4423\n",
      "a: 0.0990\n",
      "step:  10\n",
      "loss: 1.2537\n",
      "a: 0.0989\n",
      "step:  11\n",
      "loss: 1.4239\n",
      "a: 0.0989\n",
      "step:  12\n",
      "loss: 1.2187\n",
      "a: 0.0989\n",
      "step:  13\n",
      "loss: 1.0060\n",
      "a: 0.0989\n",
      "step:  14\n",
      "loss: 1.1271\n",
      "a: 0.0990\n",
      "step:  15\n",
      "loss: 0.9518\n",
      "a: 0.0991\n",
      "step:  16\n",
      "loss: 1.0893\n",
      "a: 0.0993\n",
      "step:  17\n",
      "loss: 0.8143\n",
      "a: 0.0994\n",
      "step:  18\n",
      "loss: 0.8337\n",
      "a: 0.0996\n",
      "step:  19\n",
      "loss: 0.8590\n",
      "a: 0.0998\n",
      "step:  20\n",
      "loss: 0.8619\n",
      "a: 0.1000\n",
      "step:  21\n",
      "loss: 0.7422\n",
      "a: 0.1001\n",
      "step:  22\n",
      "loss: 0.7360\n",
      "a: 0.1003\n",
      "step:  23\n",
      "loss: 0.8316\n",
      "a: 0.1004\n",
      "step:  24\n",
      "loss: 0.5170\n",
      "a: 0.1005\n",
      "step:  25\n",
      "loss: 0.7050\n",
      "a: 0.1007\n",
      "step:  26\n",
      "loss: 0.6568\n",
      "a: 0.1008\n",
      "step:  27\n",
      "loss: 0.8379\n",
      "a: 0.1009\n",
      "step:  28\n",
      "loss: 0.8159\n",
      "a: 0.1011\n",
      "step:  29\n",
      "loss: 0.5672\n",
      "a: 0.1012\n",
      "step:  30\n",
      "loss: 0.6586\n",
      "a: 0.1013\n",
      "step:  31\n",
      "loss: 0.7188\n",
      "a: 0.1015\n",
      "step:  32\n",
      "loss: 0.6846\n",
      "a: 0.1016\n",
      "step:  33\n",
      "loss: 0.6243\n",
      "a: 0.1017\n",
      "step:  34\n",
      "loss: 0.6144\n",
      "a: 0.1018\n",
      "step:  35\n",
      "loss: 0.6147\n",
      "a: 0.1020\n",
      "step:  36\n",
      "loss: 0.5981\n",
      "a: 0.1021\n",
      "step:  37\n",
      "loss: 0.7010\n",
      "a: 0.1022\n",
      "step:  38\n",
      "loss: 0.6129\n",
      "a: 0.1023\n",
      "step:  39\n",
      "loss: 0.7101\n",
      "a: 0.1024\n",
      "step:  40\n",
      "loss: 0.6138\n",
      "a: 0.1025\n",
      "step:  41\n",
      "loss: 0.5521\n",
      "a: 0.1026\n",
      "step:  42\n",
      "loss: 0.2934\n",
      "a: 0.1027\n",
      "step:  43\n",
      "loss: 0.5303\n",
      "a: 0.1028\n",
      "step:  44\n",
      "loss: 0.3261\n",
      "a: 0.1029\n",
      "step:  45\n",
      "loss: 1.1060\n",
      "a: 0.1030\n",
      "step:  46\n",
      "loss: 0.6481\n",
      "a: 0.1031\n",
      "step:  47\n",
      "loss: 0.4442\n",
      "a: 0.1032\n",
      "step:  48\n",
      "loss: 0.9211\n",
      "a: 0.1033\n",
      "step:  49\n",
      "loss: 0.6399\n",
      "a: 0.1034\n",
      "step:  50\n",
      "loss: 0.9552\n",
      "a: 0.1034\n",
      "step:  51\n",
      "loss: 0.3941\n",
      "a: 0.1035\n",
      "step:  52\n",
      "loss: 0.4963\n",
      "a: 0.1036\n",
      "step:  53\n",
      "loss: 0.4677\n",
      "a: 0.1037\n",
      "step:  54\n",
      "loss: 0.7973\n",
      "a: 0.1037\n",
      "step:  55\n",
      "loss: 0.5706\n",
      "a: 0.1038\n",
      "step:  56\n",
      "loss: 0.6625\n",
      "a: 0.1038\n",
      "step:  57\n",
      "loss: 0.3379\n",
      "a: 0.1039\n",
      "step:  58\n",
      "loss: 0.3228\n",
      "a: 0.1039\n",
      "step:  59\n",
      "loss: 0.6866\n",
      "a: 0.1040\n",
      "step:  60\n",
      "loss: 0.5905\n",
      "a: 0.1041\n",
      "step:  61\n",
      "loss: 0.3992\n",
      "a: 0.1042\n",
      "step:  62\n",
      "loss: 0.4526\n",
      "a: 0.1043\n",
      "step:  63\n",
      "loss: 0.5019\n",
      "a: 0.1044\n",
      "step:  64\n",
      "loss: 0.3547\n",
      "a: 0.1044\n",
      "step:  65\n",
      "loss: 0.4362\n",
      "a: 0.1045\n",
      "step:  66\n",
      "loss: 0.5824\n",
      "a: 0.1045\n",
      "step:  67\n",
      "loss: 0.8538\n",
      "a: 0.1045\n",
      "step:  68\n",
      "loss: 0.8591\n",
      "a: 0.1045\n",
      "step:  69\n",
      "loss: 0.5407\n",
      "a: 0.1045\n",
      "step:  70\n",
      "loss: 0.2475\n",
      "a: 0.1044\n",
      "step:  71\n",
      "loss: 0.6167\n",
      "a: 0.1044\n",
      "step:  72\n",
      "loss: 0.5088\n",
      "a: 0.1043\n",
      "step:  73\n",
      "loss: 0.7319\n",
      "a: 0.1043\n",
      "step:  74\n",
      "loss: 0.3707\n",
      "a: 0.1043\n",
      "step:  75\n",
      "loss: 0.5780\n",
      "a: 0.1042\n",
      "step:  76\n",
      "loss: 0.2711\n",
      "a: 0.1042\n",
      "step:  77\n",
      "loss: 0.6610\n",
      "a: 0.1042\n",
      "step:  78\n",
      "loss: 0.1764\n",
      "a: 0.1041\n",
      "step:  79\n",
      "loss: 0.5185\n",
      "a: 0.1041\n",
      "step:  80\n",
      "loss: 0.7410\n",
      "a: 0.1041\n",
      "step:  81\n",
      "loss: 0.6013\n",
      "a: 0.1040\n",
      "step:  82\n",
      "loss: 0.2105\n",
      "a: 0.1039\n",
      "step:  83\n",
      "loss: 0.6175\n",
      "a: 0.1038\n",
      "step:  84\n",
      "loss: 0.3202\n",
      "a: 0.1037\n",
      "step:  85\n",
      "loss: 0.4393\n",
      "a: 0.1035\n",
      "step:  86\n",
      "loss: 0.7248\n",
      "a: 0.1033\n",
      "step:  87\n",
      "loss: 0.5758\n",
      "a: 0.1031\n",
      "step:  88\n",
      "loss: 0.9899\n",
      "a: 0.1030\n",
      "step:  89\n",
      "loss: 0.7433\n",
      "a: 0.1028\n",
      "step:  90\n",
      "loss: 0.7543\n",
      "a: 0.1027\n",
      "step:  91\n",
      "loss: 0.7785\n",
      "a: 0.1025\n",
      "step:  92\n",
      "loss: 0.5842\n",
      "a: 0.1023\n",
      "step:  93\n",
      "loss: 0.4250\n",
      "a: 0.1022\n",
      "step:  94\n",
      "loss: 0.5418\n",
      "a: 0.1020\n",
      "step:  95\n",
      "loss: 0.5416\n",
      "a: 0.1019\n",
      "step:  96\n",
      "loss: 0.7406\n",
      "a: 0.1017\n",
      "step:  97\n",
      "loss: 0.4992\n",
      "a: 0.1016\n",
      "step:  98\n",
      "loss: 0.5464\n",
      "a: 0.1014\n",
      "step:  99\n",
      "loss: 0.5581\n",
      "a: 0.1013\n",
      "step:  100\n",
      "loss: 0.3529\n",
      "a: 0.1012\n",
      "step:  101\n",
      "loss: 0.4340\n",
      "a: 0.1011\n",
      "step:  102\n",
      "loss: 0.6002\n",
      "a: 0.1009\n",
      "step:  103\n",
      "loss: 0.5471\n",
      "a: 0.1008\n",
      "step:  104\n",
      "loss: 0.7694\n",
      "a: 0.1007\n",
      "step:  105\n",
      "loss: 0.7118\n",
      "a: 0.1006\n",
      "step:  106\n",
      "loss: 0.9442\n",
      "a: 0.1004\n",
      "step:  107\n",
      "loss: 0.7990\n",
      "a: 0.1003\n",
      "step:  108\n",
      "loss: 1.1215\n",
      "a: 0.1001\n",
      "step:  109\n",
      "loss: 0.6990\n",
      "a: 0.1000\n",
      "step:  110\n",
      "loss: 0.8075\n",
      "a: 0.0998\n",
      "step:  111\n",
      "loss: 0.2074\n",
      "a: 0.0997\n",
      "step:  112\n",
      "loss: 0.3236\n",
      "a: 0.0996\n",
      "step:  113\n",
      "loss: 0.6958\n",
      "a: 0.0994\n",
      "step:  114\n",
      "loss: 0.4689\n",
      "a: 0.0993\n",
      "step:  115\n",
      "loss: 0.5551\n",
      "a: 0.0992\n",
      "step:  116\n",
      "loss: 0.2331\n",
      "a: 0.0991\n",
      "step:  117\n",
      "loss: 0.4841\n",
      "a: 0.0989\n",
      "step:  118\n",
      "loss: 0.5168\n",
      "a: 0.0988\n",
      "step:  119\n",
      "loss: 0.4867\n",
      "a: 0.0987\n",
      "step:  120\n",
      "loss: 0.7880\n",
      "a: 0.0986\n",
      "step:  121\n",
      "loss: 0.4595\n",
      "a: 0.0985\n",
      "step:  122\n",
      "loss: 0.3513\n",
      "a: 0.0984\n",
      "step:  123\n",
      "loss: 0.7303\n",
      "a: 0.0983\n",
      "step:  124\n",
      "loss: 0.5194\n",
      "a: 0.0981\n",
      "step:  125\n",
      "loss: 0.3634\n",
      "a: 0.0980\n",
      "step:  126\n",
      "loss: 0.7437\n",
      "a: 0.0979\n",
      "step:  127\n",
      "loss: 0.4846\n",
      "a: 0.0978\n",
      "step:  128\n",
      "loss: 0.5484\n",
      "a: 0.0977\n",
      "step:  129\n",
      "loss: 0.6450\n",
      "a: 0.0976\n",
      "step:  130\n",
      "loss: 0.7624\n",
      "a: 0.0975\n",
      "step:  131\n",
      "loss: 0.4801\n",
      "a: 0.0974\n",
      "step:  132\n",
      "loss: 0.7852\n",
      "a: 0.0973\n",
      "step:  133\n",
      "loss: 0.5107\n",
      "a: 0.0972\n",
      "step:  134\n",
      "loss: 0.3306\n",
      "a: 0.0970\n",
      "step:  135\n",
      "loss: 0.5248\n",
      "a: 0.0969\n",
      "step:  136\n",
      "loss: 0.3949\n",
      "a: 0.0968\n",
      "step:  137\n",
      "loss: 0.7173\n",
      "a: 0.0967\n",
      "step:  138\n",
      "loss: 0.4622\n",
      "a: 0.0966\n",
      "step:  139\n",
      "loss: 0.5705\n",
      "a: 0.0964\n",
      "step:  140\n",
      "loss: 0.3975\n",
      "a: 0.0963\n",
      "step:  141\n",
      "loss: 0.4904\n",
      "a: 0.0962\n",
      "step:  142\n",
      "loss: 0.5318\n",
      "a: 0.0960\n",
      "step:  143\n",
      "loss: 0.2643\n",
      "a: 0.0959\n",
      "step:  144\n",
      "loss: 0.2807\n",
      "a: 0.0958\n",
      "step:  145\n",
      "loss: 0.2290\n",
      "a: 0.0957\n",
      "step:  146\n",
      "loss: 0.9060\n",
      "a: 0.0956\n",
      "step:  147\n",
      "loss: 0.2551\n",
      "a: 0.0955\n",
      "step:  148\n",
      "loss: 0.8873\n",
      "a: 0.0954\n",
      "step:  149\n",
      "loss: 0.3506\n",
      "a: 0.0953\n",
      "step:  150\n",
      "loss: 0.4240\n",
      "a: 0.0952\n",
      "step:  151\n",
      "loss: 0.7548\n",
      "a: 0.0951\n",
      "step:  152\n",
      "loss: 0.3878\n",
      "a: 0.0950\n",
      "step:  153\n",
      "loss: 0.9485\n",
      "a: 0.0949\n",
      "step:  154\n",
      "loss: 0.2635\n",
      "a: 0.0948\n",
      "step:  155\n",
      "loss: 0.5735\n",
      "a: 0.0947\n",
      "step:  156\n",
      "loss: 0.4963\n",
      "a: 0.0946\n",
      "step:  157\n",
      "loss: 0.3570\n",
      "a: 0.0945\n",
      "step:  158\n",
      "loss: 0.4771\n",
      "a: 0.0943\n",
      "step:  159\n",
      "loss: 0.5681\n",
      "a: 0.0942\n",
      "step:  160\n",
      "loss: 1.0294\n",
      "a: 0.0941\n",
      "step:  161\n",
      "loss: 0.5427\n",
      "a: 0.0940\n",
      "step:  162\n",
      "loss: 0.4969\n",
      "a: 0.0939\n",
      "step:  163\n",
      "loss: 0.4648\n",
      "a: 0.0938\n",
      "step:  164\n",
      "loss: 0.5548\n",
      "a: 0.0936\n",
      "step:  165\n",
      "loss: 0.2439\n",
      "a: 0.0935\n",
      "step:  166\n",
      "loss: 0.5712\n",
      "a: 0.0934\n",
      "step:  167\n",
      "loss: 0.3426\n",
      "a: 0.0933\n",
      "step:  168\n",
      "loss: 0.3962\n",
      "a: 0.0932\n",
      "step:  169\n",
      "loss: 0.4183\n",
      "a: 0.0931\n",
      "step:  170\n",
      "loss: 0.3223\n",
      "a: 0.0929\n",
      "step:  171\n",
      "loss: 1.0261\n",
      "a: 0.0928\n",
      "step:  172\n",
      "loss: 0.5715\n",
      "a: 0.0927\n",
      "step:  173\n",
      "loss: 0.9139\n",
      "a: 0.0926\n",
      "step:  174\n",
      "loss: 0.4505\n",
      "a: 0.0925\n",
      "step:  175\n",
      "loss: 0.5717\n",
      "a: 0.0924\n",
      "step:  176\n",
      "loss: 0.2927\n",
      "a: 0.0923\n",
      "step:  177\n",
      "loss: 0.2411\n",
      "a: 0.0922\n",
      "step:  178\n",
      "loss: 0.6303\n",
      "a: 0.0921\n",
      "step:  179\n",
      "loss: 0.5089\n",
      "a: 0.0920\n",
      "step:  180\n",
      "loss: 0.5607\n",
      "a: 0.0919\n",
      "step:  181\n",
      "loss: 0.3994\n",
      "a: 0.0918\n",
      "step:  182\n",
      "loss: 0.4066\n",
      "a: 0.0917\n",
      "step:  183\n",
      "loss: 0.3838\n",
      "a: 0.0916\n",
      "step:  184\n",
      "loss: 0.4464\n",
      "a: 0.0914\n",
      "step:  185\n",
      "loss: 0.2026\n",
      "a: 0.0913\n",
      "step:  186\n",
      "loss: 0.4949\n",
      "a: 0.0912\n",
      "step:  187\n",
      "loss: 0.4077\n",
      "a: 0.0911\n",
      "step:  188\n",
      "loss: 0.4240\n",
      "a: 0.0910\n",
      "step:  189\n",
      "loss: 0.4325\n",
      "a: 0.0909\n",
      "step:  190\n",
      "loss: 0.2501\n",
      "a: 0.0908\n",
      "step:  191\n",
      "loss: 0.4405\n",
      "a: 0.0907\n",
      "step:  192\n",
      "loss: 0.2513\n",
      "a: 0.0906\n",
      "step:  193\n",
      "loss: 0.2371\n",
      "a: 0.0905\n",
      "step:  194\n",
      "loss: 0.3036\n",
      "a: 0.0904\n",
      "step:  195\n",
      "loss: 0.3995\n",
      "a: 0.0903\n",
      "step:  196\n",
      "loss: 0.3146\n",
      "a: 0.0902\n",
      "step:  197\n",
      "loss: 0.3922\n",
      "a: 0.0901\n",
      "step:  198\n",
      "loss: 0.1985\n",
      "a: 0.0900\n",
      "step:  199\n",
      "loss: 0.4615\n",
      "a: 0.0899\n",
      "step:  200\n",
      "loss: 0.3532\n",
      "a: 0.0898\n",
      "step:  201\n",
      "loss: 0.5650\n",
      "a: 0.0897\n",
      "step:  202\n",
      "loss: 0.3326\n",
      "a: 0.0896\n",
      "step:  203\n",
      "loss: 0.6506\n",
      "a: 0.0895\n",
      "step:  204\n",
      "loss: 0.3222\n",
      "a: 0.0894\n",
      "step:  205\n",
      "loss: 0.2700\n",
      "a: 0.0893\n",
      "step:  206\n",
      "loss: 0.3591\n",
      "a: 0.0892\n",
      "step:  207\n",
      "loss: 0.5123\n",
      "a: 0.0891\n",
      "step:  208\n",
      "loss: 0.3935\n",
      "a: 0.0890\n",
      "step:  209\n",
      "loss: 0.2899\n",
      "a: 0.0889\n",
      "step:  210\n",
      "loss: 0.6972\n",
      "a: 0.0888\n",
      "step:  211\n",
      "loss: 0.3175\n",
      "a: 0.0886\n",
      "step:  212\n",
      "loss: 0.3425\n",
      "a: 0.0885\n",
      "step:  213\n",
      "loss: 0.5082\n",
      "a: 0.0884\n",
      "step:  214\n",
      "loss: 0.2159\n",
      "a: 0.0883\n",
      "step:  215\n",
      "loss: 0.3766\n",
      "a: 0.0882\n",
      "step:  216\n",
      "loss: 0.1801\n",
      "a: 0.0881\n",
      "step:  217\n",
      "loss: 0.2499\n",
      "a: 0.0880\n",
      "step:  218\n",
      "loss: 0.5866\n",
      "a: 0.0879\n",
      "step:  219\n",
      "loss: 0.3274\n",
      "a: 0.0878\n",
      "step:  220\n",
      "loss: 0.5915\n",
      "a: 0.0876\n",
      "step:  221\n",
      "loss: 0.2019\n",
      "a: 0.0875\n",
      "step:  222\n",
      "loss: 0.3395\n",
      "a: 0.0874\n",
      "step:  223\n",
      "loss: 0.5233\n",
      "a: 0.0873\n",
      "step:  224\n",
      "loss: 0.3951\n",
      "a: 0.0872\n",
      "step:  225\n",
      "loss: 0.2708\n",
      "a: 0.0871\n",
      "step:  226\n",
      "loss: 0.2717\n",
      "a: 0.0870\n",
      "step:  227\n",
      "loss: 0.7070\n",
      "a: 0.0869\n",
      "step:  228\n",
      "loss: 0.5642\n",
      "a: 0.0868\n",
      "step:  229\n",
      "loss: 0.3917\n",
      "a: 0.0867\n",
      "step:  230\n",
      "loss: 0.3595\n",
      "a: 0.0866\n",
      "step:  231\n",
      "loss: 0.2290\n",
      "a: 0.0865\n",
      "step:  232\n",
      "loss: 0.2904\n",
      "a: 0.0864\n",
      "step:  233\n",
      "loss: 0.5371\n",
      "a: 0.0863\n",
      "step:  234\n",
      "loss: 0.3279\n",
      "a: 0.0862\n",
      "step:  235\n",
      "loss: 0.5734\n",
      "a: 0.0861\n",
      "step:  236\n",
      "loss: 0.3425\n",
      "a: 0.0860\n",
      "step:  237\n",
      "loss: 0.4810\n",
      "a: 0.0859\n",
      "step:  238\n",
      "loss: 0.2482\n",
      "a: 0.0857\n",
      "step:  239\n",
      "loss: 0.4535\n",
      "a: 0.0856\n",
      "step:  240\n",
      "loss: 0.4895\n",
      "a: 0.0855\n",
      "step:  241\n",
      "loss: 0.2489\n",
      "a: 0.0854\n",
      "step:  242\n",
      "loss: 0.6242\n",
      "a: 0.0853\n",
      "step:  243\n",
      "loss: 0.4171\n",
      "a: 0.0852\n",
      "step:  244\n",
      "loss: 0.2699\n",
      "a: 0.0851\n",
      "step:  245\n",
      "loss: 0.4689\n",
      "a: 0.0850\n",
      "step:  246\n",
      "loss: 0.3517\n",
      "a: 0.0849\n",
      "step:  247\n",
      "loss: 0.6437\n",
      "a: 0.0848\n",
      "step:  248\n",
      "loss: 0.7357\n",
      "a: 0.0846\n",
      "step:  249\n",
      "loss: 0.4180\n",
      "a: 0.0845\n",
      "step:  250\n",
      "loss: 0.5150\n",
      "a: 0.0844\n",
      "step:  251\n",
      "loss: 0.4107\n",
      "a: 0.0843\n",
      "step:  252\n",
      "loss: 0.4545\n",
      "a: 0.0842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  253\n",
      "loss: 0.2602\n",
      "a: 0.0841\n",
      "step:  254\n",
      "loss: 0.5854\n",
      "a: 0.0840\n",
      "step:  255\n",
      "loss: 0.3106\n",
      "a: 0.0839\n",
      "step:  256\n",
      "loss: 0.3359\n",
      "a: 0.0838\n",
      "step:  257\n",
      "loss: 0.4227\n",
      "a: 0.0837\n",
      "step:  258\n",
      "loss: 0.3912\n",
      "a: 0.0836\n",
      "step:  259\n",
      "loss: 0.4491\n",
      "a: 0.0834\n",
      "step:  260\n",
      "loss: 0.2380\n",
      "a: 0.0833\n",
      "step:  261\n",
      "loss: 0.3762\n",
      "a: 0.0832\n",
      "step:  262\n",
      "loss: 0.7197\n",
      "a: 0.0831\n",
      "step:  263\n",
      "loss: 0.3371\n",
      "a: 0.0830\n",
      "step:  264\n",
      "loss: 0.4006\n",
      "a: 0.0829\n",
      "step:  265\n",
      "loss: 0.3642\n",
      "a: 0.0828\n",
      "step:  266\n",
      "loss: 0.4813\n",
      "a: 0.0827\n",
      "step:  267\n",
      "loss: 0.1526\n",
      "a: 0.0826\n",
      "step:  268\n",
      "loss: 0.3358\n",
      "a: 0.0825\n",
      "step:  269\n",
      "loss: 0.1905\n",
      "a: 0.0824\n",
      "step:  270\n",
      "loss: 0.3461\n",
      "a: 0.0823\n",
      "step:  271\n",
      "loss: 0.2367\n",
      "a: 0.0822\n",
      "step:  272\n",
      "loss: 0.1619\n",
      "a: 0.0821\n",
      "step:  273\n",
      "loss: 0.2477\n",
      "a: 0.0820\n",
      "step:  274\n",
      "loss: 0.2093\n",
      "a: 0.0819\n",
      "step:  275\n",
      "loss: 0.6187\n",
      "a: 0.0818\n",
      "step:  276\n",
      "loss: 0.2599\n",
      "a: 0.0817\n",
      "step:  277\n",
      "loss: 0.2628\n",
      "a: 0.0816\n",
      "step:  278\n",
      "loss: 0.4253\n",
      "a: 0.0815\n",
      "step:  279\n",
      "loss: 0.3503\n",
      "a: 0.0814\n",
      "step:  280\n",
      "loss: 0.2659\n",
      "a: 0.0812\n",
      "step:  281\n",
      "loss: 0.3310\n",
      "a: 0.0811\n",
      "step:  282\n",
      "loss: 0.2030\n",
      "a: 0.0810\n",
      "step:  283\n",
      "loss: 0.4052\n",
      "a: 0.0809\n",
      "step:  284\n",
      "loss: 0.5076\n",
      "a: 0.0808\n",
      "step:  285\n",
      "loss: 0.4342\n",
      "a: 0.0807\n",
      "step:  286\n",
      "loss: 0.2067\n",
      "a: 0.0806\n",
      "step:  287\n",
      "loss: 0.2290\n",
      "a: 0.0805\n",
      "step:  288\n",
      "loss: 0.4006\n",
      "a: 0.0804\n",
      "step:  289\n",
      "loss: 0.3713\n",
      "a: 0.0803\n",
      "step:  290\n",
      "loss: 0.4151\n",
      "a: 0.0802\n",
      "step:  291\n",
      "loss: 0.7741\n",
      "a: 0.0801\n",
      "step:  292\n",
      "loss: 0.3781\n",
      "a: 0.0800\n",
      "step:  293\n",
      "loss: 0.7498\n",
      "a: 0.0799\n",
      "step:  294\n",
      "loss: 0.4539\n",
      "a: 0.0798\n",
      "step:  295\n",
      "loss: 0.2223\n",
      "a: 0.0797\n",
      "step:  296\n",
      "loss: 0.2822\n",
      "a: 0.0796\n",
      "step:  297\n",
      "loss: 0.3587\n",
      "a: 0.0795\n",
      "step:  298\n",
      "loss: 0.4087\n",
      "a: 0.0793\n",
      "step:  299\n",
      "loss: 0.9054\n",
      "a: 0.0792\n",
      "step:  300\n",
      "loss: 0.4390\n",
      "a: 0.0791\n",
      "step:  301\n",
      "loss: 0.4792\n",
      "a: 0.0790\n",
      "step:  302\n",
      "loss: 0.7205\n",
      "a: 0.0789\n",
      "step:  303\n",
      "loss: 0.2955\n",
      "a: 0.0788\n",
      "step:  304\n",
      "loss: 0.1884\n",
      "a: 0.0787\n",
      "step:  305\n",
      "loss: 0.3281\n",
      "a: 0.0786\n",
      "step:  306\n",
      "loss: 0.1623\n",
      "a: 0.0785\n",
      "step:  307\n",
      "loss: 0.2032\n",
      "a: 0.0784\n",
      "step:  308\n",
      "loss: 0.4461\n",
      "a: 0.0783\n",
      "step:  309\n",
      "loss: 0.1633\n",
      "a: 0.0782\n",
      "step:  310\n",
      "loss: 0.6885\n",
      "a: 0.0781\n",
      "step:  311\n",
      "loss: 0.2355\n",
      "a: 0.0780\n",
      "step:  312\n",
      "loss: 0.3972\n",
      "a: 0.0779\n",
      "step:  313\n",
      "loss: 0.3390\n",
      "a: 0.0778\n",
      "step:  314\n",
      "loss: 0.4658\n",
      "a: 0.0777\n",
      "step:  315\n",
      "loss: 0.4092\n",
      "a: 0.0776\n",
      "step:  316\n",
      "loss: 0.3602\n",
      "a: 0.0775\n",
      "step:  317\n",
      "loss: 0.2604\n",
      "a: 0.0774\n",
      "step:  318\n",
      "loss: 0.5875\n",
      "a: 0.0773\n",
      "step:  319\n",
      "loss: 0.3241\n",
      "a: 0.0772\n",
      "step:  320\n",
      "loss: 0.6886\n",
      "a: 0.0771\n",
      "step:  321\n",
      "loss: 0.1452\n",
      "a: 0.0770\n",
      "step:  322\n",
      "loss: 0.4654\n",
      "a: 0.0769\n",
      "step:  323\n",
      "loss: 0.5123\n",
      "a: 0.0768\n",
      "step:  324\n",
      "loss: 0.4961\n",
      "a: 0.0767\n",
      "step:  325\n",
      "loss: 0.2850\n",
      "a: 0.0766\n",
      "step:  326\n",
      "loss: 0.8568\n",
      "a: 0.0765\n",
      "step:  327\n",
      "loss: 0.2650\n",
      "a: 0.0764\n",
      "step:  328\n",
      "loss: 0.1791\n",
      "a: 0.0763\n",
      "step:  329\n",
      "loss: 0.3415\n",
      "a: 0.0762\n",
      "step:  330\n",
      "loss: 0.1948\n",
      "a: 0.0760\n",
      "step:  331\n",
      "loss: 0.5381\n",
      "a: 0.0759\n",
      "step:  332\n",
      "loss: 0.3528\n",
      "a: 0.0758\n",
      "step:  333\n",
      "loss: 0.0935\n",
      "a: 0.0757\n",
      "step:  334\n",
      "loss: 0.6249\n",
      "a: 0.0756\n",
      "step:  335\n",
      "loss: 0.4746\n",
      "a: 0.0755\n",
      "step:  336\n",
      "loss: 0.3025\n",
      "a: 0.0754\n",
      "step:  337\n",
      "loss: 0.4598\n",
      "a: 0.0753\n",
      "step:  338\n",
      "loss: 0.2205\n",
      "a: 0.0752\n",
      "step:  339\n",
      "loss: 0.2169\n",
      "a: 0.0751\n",
      "step:  340\n",
      "loss: 0.2278\n",
      "a: 0.0749\n",
      "step:  341\n",
      "loss: 0.2473\n",
      "a: 0.0748\n",
      "step:  342\n",
      "loss: 0.2519\n",
      "a: 0.0747\n",
      "step:  343\n",
      "loss: 0.9207\n",
      "a: 0.0746\n",
      "step:  344\n",
      "loss: 0.3389\n",
      "a: 0.0745\n",
      "step:  345\n",
      "loss: 0.1137\n",
      "a: 0.0744\n",
      "step:  346\n",
      "loss: 0.1410\n",
      "a: 0.0743\n",
      "step:  347\n",
      "loss: 0.6396\n",
      "a: 0.0742\n",
      "step:  348\n",
      "loss: 0.3034\n",
      "a: 0.0741\n",
      "step:  349\n",
      "loss: 0.2398\n",
      "a: 0.0740\n",
      "step:  350\n",
      "loss: 0.4534\n",
      "a: 0.0739\n",
      "step:  351\n",
      "loss: 0.2188\n",
      "a: 0.0738\n",
      "step:  352\n",
      "loss: 0.4769\n",
      "a: 0.0737\n",
      "step:  353\n",
      "loss: 0.3624\n",
      "a: 0.0736\n",
      "step:  354\n",
      "loss: 0.1812\n",
      "a: 0.0735\n",
      "step:  355\n",
      "loss: 0.4181\n",
      "a: 0.0734\n",
      "step:  356\n",
      "loss: 0.4173\n",
      "a: 0.0733\n",
      "step:  357\n",
      "loss: 0.2645\n",
      "a: 0.0732\n",
      "step:  358\n",
      "loss: 0.2949\n",
      "a: 0.0731\n",
      "step:  359\n",
      "loss: 0.3250\n",
      "a: 0.0730\n",
      "step:  360\n",
      "loss: 0.2912\n",
      "a: 0.0729\n",
      "step:  361\n",
      "loss: 0.7616\n",
      "a: 0.0728\n",
      "step:  362\n",
      "loss: 0.3472\n",
      "a: 0.0727\n",
      "step:  363\n",
      "loss: 0.1061\n",
      "a: 0.0726\n",
      "step:  364\n",
      "loss: 0.1573\n",
      "a: 0.0725\n",
      "step:  365\n",
      "loss: 0.2603\n",
      "a: 0.0724\n",
      "step:  366\n",
      "loss: 0.1995\n",
      "a: 0.0723\n",
      "step:  367\n",
      "loss: 0.2452\n",
      "a: 0.0722\n",
      "step:  368\n",
      "loss: 0.4744\n",
      "a: 0.0721\n",
      "step:  369\n",
      "loss: 0.3173\n",
      "a: 0.0720\n",
      "step:  370\n",
      "loss: 0.2350\n",
      "a: 0.0719\n",
      "step:  371\n",
      "loss: 0.6170\n",
      "a: 0.0718\n",
      "step:  372\n",
      "loss: 0.3643\n",
      "a: 0.0717\n",
      "step:  373\n",
      "loss: 0.1743\n",
      "a: 0.0716\n",
      "step:  374\n",
      "loss: 0.3579\n",
      "a: 0.0715\n",
      "step:  375\n",
      "loss: 0.2203\n",
      "a: 0.0714\n",
      "step:  376\n",
      "loss: 0.4067\n",
      "a: 0.0713\n",
      "step:  377\n",
      "loss: 0.2685\n",
      "a: 0.0711\n",
      "step:  378\n",
      "loss: 0.3240\n",
      "a: 0.0710\n",
      "step:  379\n",
      "loss: 0.2644\n",
      "a: 0.0709\n",
      "step:  380\n",
      "loss: 0.3080\n",
      "a: 0.0708\n",
      "step:  381\n",
      "loss: 0.5068\n",
      "a: 0.0707\n",
      "step:  382\n",
      "loss: 0.3854\n",
      "a: 0.0706\n",
      "step:  383\n",
      "loss: 0.3814\n",
      "a: 0.0705\n",
      "step:  384\n",
      "loss: 0.2344\n",
      "a: 0.0704\n",
      "step:  385\n",
      "loss: 0.5430\n",
      "a: 0.0703\n",
      "step:  386\n",
      "loss: 0.4889\n",
      "a: 0.0702\n",
      "step:  387\n",
      "loss: 0.4619\n",
      "a: 0.0701\n",
      "step:  388\n",
      "loss: 0.4922\n",
      "a: 0.0699\n",
      "step:  389\n",
      "loss: 0.2302\n",
      "a: 0.0698\n",
      "step:  390\n",
      "loss: 0.6132\n",
      "a: 0.0698\n",
      "step:  391\n",
      "loss: 0.1275\n",
      "a: 0.0697\n",
      "step:  392\n",
      "loss: 0.2672\n",
      "a: 0.0696\n",
      "step:  393\n",
      "loss: 0.3428\n",
      "a: 0.0695\n",
      "step:  394\n",
      "loss: 0.1834\n",
      "a: 0.0694\n",
      "step:  395\n",
      "loss: 0.2858\n",
      "a: 0.0693\n",
      "step:  396\n",
      "loss: 0.5098\n",
      "a: 0.0692\n",
      "step:  397\n",
      "loss: 0.1524\n",
      "a: 0.0691\n",
      "step:  398\n",
      "loss: 0.2302\n",
      "a: 0.0690\n",
      "step:  399\n",
      "loss: 0.3400\n",
      "a: 0.0689\n",
      "step:  400\n",
      "loss: 0.2956\n",
      "a: 0.0688\n",
      "step:  401\n",
      "loss: 0.3984\n",
      "a: 0.0687\n",
      "step:  402\n",
      "loss: 0.5308\n",
      "a: 0.0686\n",
      "step:  403\n",
      "loss: 0.2647\n",
      "a: 0.0685\n",
      "step:  404\n",
      "loss: 0.2134\n",
      "a: 0.0684\n",
      "step:  405\n",
      "loss: 0.3521\n",
      "a: 0.0683\n",
      "step:  406\n",
      "loss: 0.4508\n",
      "a: 0.0682\n",
      "step:  407\n",
      "loss: 0.4821\n",
      "a: 0.0681\n",
      "step:  408\n",
      "loss: 0.3168\n",
      "a: 0.0679\n",
      "step:  409\n",
      "loss: 0.3594\n",
      "a: 0.0678\n",
      "step:  410\n",
      "loss: 0.2744\n",
      "a: 0.0677\n",
      "step:  411\n",
      "loss: 0.4276\n",
      "a: 0.0676\n",
      "step:  412\n",
      "loss: 0.1965\n",
      "a: 0.0675\n",
      "step:  413\n",
      "loss: 0.2784\n",
      "a: 0.0674\n",
      "step:  414\n",
      "loss: 0.5207\n",
      "a: 0.0673\n",
      "step:  415\n",
      "loss: 0.3346\n",
      "a: 0.0672\n",
      "step:  416\n",
      "loss: 0.2130\n",
      "a: 0.0671\n",
      "step:  417\n",
      "loss: 0.4983\n",
      "a: 0.0670\n",
      "step:  418\n",
      "loss: 0.4099\n",
      "a: 0.0668\n",
      "step:  419\n",
      "loss: 0.2934\n",
      "a: 0.0667\n",
      "step:  420\n",
      "loss: 0.2816\n",
      "a: 0.0666\n",
      "step:  421\n",
      "loss: 0.5226\n",
      "a: 0.0665\n",
      "step:  422\n",
      "loss: 0.4801\n",
      "a: 0.0664\n",
      "step:  423\n",
      "loss: 0.1907\n",
      "a: 0.0663\n",
      "step:  424\n",
      "loss: 0.2927\n",
      "a: 0.0663\n",
      "step:  425\n",
      "loss: 0.1649\n",
      "a: 0.0662\n",
      "step:  426\n",
      "loss: 0.3454\n",
      "a: 0.0661\n",
      "step:  427\n",
      "loss: 0.1350\n",
      "a: 0.0660\n",
      "step:  428\n",
      "loss: 0.3299\n",
      "a: 0.0659\n",
      "step:  429\n",
      "loss: 0.2811\n",
      "a: 0.0658\n",
      "step:  430\n",
      "loss: 0.4640\n",
      "a: 0.0656\n",
      "step:  431\n",
      "loss: 0.2565\n",
      "a: 0.0655\n",
      "step:  432\n",
      "loss: 0.3016\n",
      "a: 0.0654\n",
      "step:  433\n",
      "loss: 0.1862\n",
      "a: 0.0653\n",
      "step:  434\n",
      "loss: 0.2061\n",
      "a: 0.0652\n",
      "step:  435\n",
      "loss: 0.1262\n",
      "a: 0.0651\n",
      "step:  436\n",
      "loss: 0.2806\n",
      "a: 0.0650\n",
      "step:  437\n",
      "loss: 0.3455\n",
      "a: 0.0649\n",
      "step:  438\n",
      "loss: 0.2712\n",
      "a: 0.0648\n",
      "step:  439\n",
      "loss: 0.1929\n",
      "a: 0.0647\n",
      "step:  440\n",
      "loss: 0.5005\n",
      "a: 0.0646\n",
      "step:  441\n",
      "loss: 0.2305\n",
      "a: 0.0645\n",
      "step:  442\n",
      "loss: 0.5359\n",
      "a: 0.0644\n",
      "step:  443\n",
      "loss: 0.6479\n",
      "a: 0.0643\n",
      "step:  444\n",
      "loss: 0.5230\n",
      "a: 0.0642\n",
      "step:  445\n",
      "loss: 0.6597\n",
      "a: 0.0641\n",
      "step:  446\n",
      "loss: 0.1205\n",
      "a: 0.0639\n",
      "step:  447\n",
      "loss: 0.3528\n",
      "a: 0.0638\n",
      "step:  448\n",
      "loss: 0.3211\n",
      "a: 0.0637\n",
      "step:  449\n",
      "loss: 0.2927\n",
      "a: 0.0636\n",
      "step:  450\n",
      "loss: 0.2042\n",
      "a: 0.0635\n",
      "step:  451\n",
      "loss: 0.4510\n",
      "a: 0.0634\n",
      "step:  452\n",
      "loss: 0.1559\n",
      "a: 0.0633\n",
      "step:  453\n",
      "loss: 0.3945\n",
      "a: 0.0632\n",
      "step:  454\n",
      "loss: 0.1396\n",
      "a: 0.0631\n",
      "step:  455\n",
      "loss: 0.4259\n",
      "a: 0.0630\n",
      "step:  456\n",
      "loss: 0.3185\n",
      "a: 0.0629\n",
      "step:  457\n",
      "loss: 0.1396\n",
      "a: 0.0628\n",
      "step:  458\n",
      "loss: 0.2548\n",
      "a: 0.0627\n",
      "step:  459\n",
      "loss: 0.4340\n",
      "a: 0.0626\n",
      "step:  460\n",
      "loss: 0.3476\n",
      "a: 0.0625\n",
      "step:  461\n",
      "loss: 0.8714\n",
      "a: 0.0624\n",
      "step:  462\n",
      "loss: 0.3397\n",
      "a: 0.0623\n",
      "step:  463\n",
      "loss: 0.1431\n",
      "a: 0.0622\n",
      "step:  464\n",
      "loss: 0.2949\n",
      "a: 0.0621\n",
      "step:  465\n",
      "loss: 0.1900\n",
      "a: 0.0620\n",
      "step:  466\n",
      "loss: 0.2002\n",
      "a: 0.0619\n",
      "step:  467\n",
      "loss: 0.4394\n",
      "a: 0.0618\n",
      "step:  468\n",
      "loss: 0.3325\n",
      "a: 0.0617\n",
      "step:  469\n",
      "loss: 0.1187\n",
      "a: 0.0616\n",
      "step:  470\n",
      "loss: 0.3695\n",
      "a: 0.0615\n",
      "step:  471\n",
      "loss: 0.1888\n",
      "a: 0.0614\n",
      "step:  472\n",
      "loss: 0.2637\n",
      "a: 0.0613\n",
      "step:  473\n",
      "loss: 0.4914\n",
      "a: 0.0612\n",
      "step:  474\n",
      "loss: 0.3485\n",
      "a: 0.0611\n",
      "step:  475\n",
      "loss: 0.2970\n",
      "a: 0.0610\n",
      "step:  476\n",
      "loss: 0.1518\n",
      "a: 0.0609\n",
      "step:  477\n",
      "loss: 0.1461\n",
      "a: 0.0608\n",
      "step:  478\n",
      "loss: 0.3210\n",
      "a: 0.0607\n",
      "step:  479\n",
      "loss: 0.5489\n",
      "a: 0.0606\n",
      "step:  480\n",
      "loss: 0.1912\n",
      "a: 0.0605\n",
      "step:  481\n",
      "loss: 0.2718\n",
      "a: 0.0603\n",
      "step:  482\n",
      "loss: 0.2178\n",
      "a: 0.0602\n",
      "step:  483\n",
      "loss: 0.3310\n",
      "a: 0.0601\n",
      "step:  484\n",
      "loss: 0.2158\n",
      "a: 0.0600\n",
      "step:  485\n",
      "loss: 0.2787\n",
      "a: 0.0599\n",
      "step:  486\n",
      "loss: 0.2776\n",
      "a: 0.0598\n",
      "step:  487\n",
      "loss: 0.4840\n",
      "a: 0.0597\n",
      "step:  488\n",
      "loss: 0.1370\n",
      "a: 0.0596\n",
      "step:  489\n",
      "loss: 0.4328\n",
      "a: 0.0595\n",
      "step:  490\n",
      "loss: 0.2405\n",
      "a: 0.0594\n",
      "step:  491\n",
      "loss: 0.6654\n",
      "a: 0.0593\n",
      "step:  492\n",
      "loss: 0.4271\n",
      "a: 0.0592\n",
      "step:  493\n",
      "loss: 0.1909\n",
      "a: 0.0591\n",
      "step:  494\n",
      "loss: 0.2066\n",
      "a: 0.0589\n",
      "step:  495\n",
      "loss: 0.6862\n",
      "a: 0.0588\n",
      "step:  496\n",
      "loss: 0.4057\n",
      "a: 0.0587\n",
      "step:  497\n",
      "loss: 0.2340\n",
      "a: 0.0586\n",
      "step:  498\n",
      "loss: 0.8338\n",
      "a: 0.0585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  499\n",
      "loss: 0.2109\n",
      "a: 0.0584\n",
      "step:  500\n",
      "loss: 0.2357\n",
      "a: 0.0583\n",
      "step:  501\n",
      "loss: 0.6648\n",
      "a: 0.0582\n",
      "step:  502\n",
      "loss: 0.2000\n",
      "a: 0.0581\n",
      "step:  503\n",
      "loss: 0.2157\n",
      "a: 0.0580\n",
      "step:  504\n",
      "loss: 0.5423\n",
      "a: 0.0579\n",
      "step:  505\n",
      "loss: 0.5822\n",
      "a: 0.0578\n",
      "step:  506\n",
      "loss: 0.4235\n",
      "a: 0.0577\n",
      "step:  507\n",
      "loss: 0.6527\n",
      "a: 0.0576\n",
      "step:  508\n",
      "loss: 0.2446\n",
      "a: 0.0575\n",
      "step:  509\n",
      "loss: 0.1842\n",
      "a: 0.0574\n",
      "step:  510\n",
      "loss: 0.2776\n",
      "a: 0.0573\n",
      "step:  511\n",
      "loss: 0.2527\n",
      "a: 0.0572\n",
      "step:  512\n",
      "loss: 0.2808\n",
      "a: 0.0571\n",
      "step:  513\n",
      "loss: 0.2451\n",
      "a: 0.0570\n",
      "step:  514\n",
      "loss: 0.2110\n",
      "a: 0.0569\n",
      "step:  515\n",
      "loss: 0.1546\n",
      "a: 0.0568\n",
      "step:  516\n",
      "loss: 0.2203\n",
      "a: 0.0567\n",
      "step:  517\n",
      "loss: 0.1267\n",
      "a: 0.0566\n",
      "step:  518\n",
      "loss: 0.6799\n",
      "a: 0.0565\n",
      "step:  519\n",
      "loss: 0.4170\n",
      "a: 0.0564\n",
      "step:  520\n",
      "loss: 0.1625\n",
      "a: 0.0563\n",
      "step:  521\n",
      "loss: 0.3857\n",
      "a: 0.0561\n",
      "step:  522\n",
      "loss: 0.4404\n",
      "a: 0.0560\n",
      "step:  523\n",
      "loss: 0.2860\n",
      "a: 0.0559\n",
      "step:  524\n",
      "loss: 0.3420\n",
      "a: 0.0558\n",
      "step:  525\n",
      "loss: 0.1183\n",
      "a: 0.0557\n",
      "step:  526\n",
      "loss: 0.3281\n",
      "a: 0.0556\n",
      "step:  527\n",
      "loss: 0.2367\n",
      "a: 0.0555\n",
      "step:  528\n",
      "loss: 0.4138\n",
      "a: 0.0554\n",
      "step:  529\n",
      "loss: 0.4196\n",
      "a: 0.0553\n",
      "step:  530\n",
      "loss: 0.3185\n",
      "a: 0.0552\n",
      "step:  531\n",
      "loss: 0.3145\n",
      "a: 0.0551\n",
      "step:  532\n",
      "loss: 0.2303\n",
      "a: 0.0550\n",
      "step:  533\n",
      "loss: 0.1449\n",
      "a: 0.0549\n",
      "step:  534\n",
      "loss: 0.2384\n",
      "a: 0.0548\n",
      "step:  535\n",
      "loss: 0.3743\n",
      "a: 0.0547\n",
      "step:  536\n",
      "loss: 0.1904\n",
      "a: 0.0546\n",
      "step:  537\n",
      "loss: 0.4224\n",
      "a: 0.0545\n",
      "step:  538\n",
      "loss: 0.2076\n",
      "a: 0.0544\n",
      "step:  539\n",
      "loss: 0.2654\n",
      "a: 0.0543\n",
      "step:  540\n",
      "loss: 0.2487\n",
      "a: 0.0542\n",
      "step:  541\n",
      "loss: 0.1898\n",
      "a: 0.0541\n",
      "step:  542\n",
      "loss: 0.3772\n",
      "a: 0.0540\n",
      "step:  543\n",
      "loss: 0.2148\n",
      "a: 0.0539\n",
      "step:  544\n",
      "loss: 0.3712\n",
      "a: 0.0538\n",
      "step:  545\n",
      "loss: 0.1957\n",
      "a: 0.0537\n",
      "step:  546\n",
      "loss: 0.3222\n",
      "a: 0.0536\n",
      "step:  547\n",
      "loss: 0.7374\n",
      "a: 0.0535\n",
      "step:  548\n",
      "loss: 0.3762\n",
      "a: 0.0534\n",
      "step:  549\n",
      "loss: 0.2760\n",
      "a: 0.0533\n",
      "step:  550\n",
      "loss: 0.3295\n",
      "a: 0.0532\n",
      "step:  551\n",
      "loss: 0.6301\n",
      "a: 0.0531\n",
      "step:  552\n",
      "loss: 0.1940\n",
      "a: 0.0530\n",
      "step:  553\n",
      "loss: 0.2048\n",
      "a: 0.0529\n",
      "step:  554\n",
      "loss: 0.1936\n",
      "a: 0.0528\n",
      "step:  555\n",
      "loss: 0.4048\n",
      "a: 0.0527\n",
      "step:  556\n",
      "loss: 0.2344\n",
      "a: 0.0526\n",
      "step:  557\n",
      "loss: 0.4496\n",
      "a: 0.0525\n",
      "step:  558\n",
      "loss: 0.2333\n",
      "a: 0.0523\n",
      "step:  559\n",
      "loss: 0.3409\n",
      "a: 0.0522\n",
      "step:  560\n",
      "loss: 0.3055\n",
      "a: 0.0521\n",
      "step:  561\n",
      "loss: 0.2167\n",
      "a: 0.0520\n",
      "step:  562\n",
      "loss: 0.2101\n",
      "a: 0.0519\n",
      "step:  563\n",
      "loss: 0.1813\n",
      "a: 0.0518\n",
      "step:  564\n",
      "loss: 0.3863\n",
      "a: 0.0517\n",
      "step:  565\n",
      "loss: 0.5845\n",
      "a: 0.0516\n",
      "step:  566\n",
      "loss: 0.5068\n",
      "a: 0.0515\n",
      "step:  567\n",
      "loss: 0.2562\n",
      "a: 0.0514\n",
      "step:  568\n",
      "loss: 0.4084\n",
      "a: 0.0513\n",
      "step:  569\n",
      "loss: 0.2895\n",
      "a: 0.0512\n",
      "step:  570\n",
      "loss: 0.0845\n",
      "a: 0.0511\n",
      "step:  571\n",
      "loss: 0.2783\n",
      "a: 0.0510\n",
      "step:  572\n",
      "loss: 0.3518\n",
      "a: 0.0509\n",
      "step:  573\n",
      "loss: 0.3986\n",
      "a: 0.0508\n",
      "step:  574\n",
      "loss: 0.4470\n",
      "a: 0.0507\n",
      "step:  575\n",
      "loss: 0.3032\n",
      "a: 0.0506\n",
      "step:  576\n",
      "loss: 0.1021\n",
      "a: 0.0505\n",
      "step:  577\n",
      "loss: 0.1255\n",
      "a: 0.0504\n",
      "step:  578\n",
      "loss: 0.2432\n",
      "a: 0.0503\n",
      "step:  579\n",
      "loss: 0.3272\n",
      "a: 0.0502\n",
      "step:  580\n",
      "loss: 0.2118\n",
      "a: 0.0501\n",
      "step:  581\n",
      "loss: 0.3632\n",
      "a: 0.0500\n",
      "step:  582\n",
      "loss: 0.3668\n",
      "a: 0.0499\n",
      "step:  583\n",
      "loss: 0.3164\n",
      "a: 0.0498\n",
      "step:  584\n",
      "loss: 0.1700\n",
      "a: 0.0497\n",
      "step:  585\n",
      "loss: 0.2420\n",
      "a: 0.0496\n",
      "step:  586\n",
      "loss: 0.5205\n",
      "a: 0.0495\n",
      "step:  587\n",
      "loss: 0.3134\n",
      "a: 0.0494\n",
      "step:  588\n",
      "loss: 0.2284\n",
      "a: 0.0493\n",
      "step:  589\n",
      "loss: 0.1924\n",
      "a: 0.0492\n",
      "step:  590\n",
      "loss: 0.3620\n",
      "a: 0.0491\n",
      "step:  591\n",
      "loss: 0.1872\n",
      "a: 0.0490\n",
      "step:  592\n",
      "loss: 0.2490\n",
      "a: 0.0489\n",
      "step:  593\n",
      "loss: 0.2225\n",
      "a: 0.0488\n",
      "step:  594\n",
      "loss: 0.2409\n",
      "a: 0.0487\n",
      "step:  595\n",
      "loss: 0.2276\n",
      "a: 0.0486\n",
      "step:  596\n",
      "loss: 0.2671\n",
      "a: 0.0485\n",
      "step:  597\n",
      "loss: 0.2473\n",
      "a: 0.0484\n",
      "step:  598\n",
      "loss: 0.1694\n",
      "a: 0.0483\n",
      "step:  599\n",
      "loss: 0.3600\n",
      "a: 0.0482\n",
      "step:  600\n",
      "loss: 0.0667\n",
      "a: 0.0481\n",
      "step:  601\n",
      "loss: 0.3622\n",
      "a: 0.0480\n",
      "step:  602\n",
      "loss: 0.4333\n",
      "a: 0.0479\n",
      "step:  603\n",
      "loss: 0.3363\n",
      "a: 0.0478\n",
      "step:  604\n",
      "loss: 0.1867\n",
      "a: 0.0477\n",
      "step:  605\n",
      "loss: 0.3048\n",
      "a: 0.0476\n",
      "step:  606\n",
      "loss: 0.1988\n",
      "a: 0.0475\n",
      "step:  607\n",
      "loss: 0.6283\n",
      "a: 0.0474\n",
      "step:  608\n",
      "loss: 0.3376\n",
      "a: 0.0473\n",
      "step:  609\n",
      "loss: 0.1744\n",
      "a: 0.0472\n",
      "step:  610\n",
      "loss: 0.2204\n",
      "a: 0.0471\n",
      "step:  611\n",
      "loss: 0.3585\n",
      "a: 0.0470\n",
      "step:  612\n",
      "loss: 0.2502\n",
      "a: 0.0469\n",
      "step:  613\n",
      "loss: 0.6212\n",
      "a: 0.0468\n",
      "step:  614\n",
      "loss: 0.2851\n",
      "a: 0.0467\n",
      "step:  615\n",
      "loss: 0.1928\n",
      "a: 0.0466\n",
      "step:  616\n",
      "loss: 0.4568\n",
      "a: 0.0465\n",
      "step:  617\n",
      "loss: 0.3987\n",
      "a: 0.0464\n",
      "step:  618\n",
      "loss: 0.1860\n",
      "a: 0.0463\n",
      "step:  619\n",
      "loss: 0.3965\n",
      "a: 0.0462\n",
      "step:  620\n",
      "loss: 0.0383\n",
      "a: 0.0461\n",
      "step:  621\n",
      "loss: 0.2211\n",
      "a: 0.0460\n",
      "step:  622\n",
      "loss: 0.3562\n",
      "a: 0.0459\n",
      "step:  623\n",
      "loss: 0.6257\n",
      "a: 0.0458\n",
      "step:  624\n",
      "loss: 0.4119\n",
      "a: 0.0457\n",
      "step:  625\n",
      "loss: 0.3231\n",
      "a: 0.0456\n",
      "step:  626\n",
      "loss: 0.5264\n",
      "a: 0.0455\n",
      "step:  627\n",
      "loss: 0.3225\n",
      "a: 0.0453\n",
      "step:  628\n",
      "loss: 0.2227\n",
      "a: 0.0452\n",
      "step:  629\n",
      "loss: 0.3289\n",
      "a: 0.0451\n",
      "step:  630\n",
      "loss: 0.2923\n",
      "a: 0.0450\n",
      "step:  631\n",
      "loss: 0.1896\n",
      "a: 0.0449\n",
      "step:  632\n",
      "loss: 0.4506\n",
      "a: 0.0448\n",
      "step:  633\n",
      "loss: 0.3282\n",
      "a: 0.0447\n",
      "step:  634\n",
      "loss: 0.3788\n",
      "a: 0.0446\n",
      "step:  635\n",
      "loss: 0.2600\n",
      "a: 0.0445\n",
      "step:  636\n",
      "loss: 0.3651\n",
      "a: 0.0444\n",
      "step:  637\n",
      "loss: 0.2067\n",
      "a: 0.0443\n",
      "step:  638\n",
      "loss: 0.1793\n",
      "a: 0.0442\n",
      "step:  639\n",
      "loss: 0.1745\n",
      "a: 0.0441\n",
      "step:  640\n",
      "loss: 0.2179\n",
      "a: 0.0440\n",
      "step:  641\n",
      "loss: 0.3630\n",
      "a: 0.0439\n",
      "step:  642\n",
      "loss: 0.1811\n",
      "a: 0.0438\n",
      "step:  643\n",
      "loss: 0.1554\n",
      "a: 0.0437\n",
      "step:  644\n",
      "loss: 0.2798\n",
      "a: 0.0436\n",
      "step:  645\n",
      "loss: 0.2648\n",
      "a: 0.0435\n",
      "step:  646\n",
      "loss: 0.4305\n",
      "a: 0.0434\n",
      "step:  647\n",
      "loss: 0.4254\n",
      "a: 0.0433\n",
      "step:  648\n",
      "loss: 0.1828\n",
      "a: 0.0432\n",
      "step:  649\n",
      "loss: 0.2495\n",
      "a: 0.0431\n",
      "step:  650\n",
      "loss: 0.2258\n",
      "a: 0.0430\n",
      "step:  651\n",
      "loss: 0.2869\n",
      "a: 0.0429\n",
      "step:  652\n",
      "loss: 0.4577\n",
      "a: 0.0428\n",
      "step:  653\n",
      "loss: 0.2021\n",
      "a: 0.0427\n",
      "step:  654\n",
      "loss: 0.4576\n",
      "a: 0.0426\n",
      "step:  655\n",
      "loss: 0.2404\n",
      "a: 0.0425\n",
      "step:  656\n",
      "loss: 0.1411\n",
      "a: 0.0424\n",
      "step:  657\n",
      "loss: 0.1974\n",
      "a: 0.0423\n",
      "step:  658\n",
      "loss: 0.3120\n",
      "a: 0.0422\n",
      "step:  659\n",
      "loss: 0.1645\n",
      "a: 0.0421\n",
      "step:  660\n",
      "loss: 0.4855\n",
      "a: 0.0420\n",
      "step:  661\n",
      "loss: 0.7088\n",
      "a: 0.0419\n",
      "step:  662\n",
      "loss: 0.2436\n",
      "a: 0.0418\n",
      "step:  663\n",
      "loss: 0.2134\n",
      "a: 0.0417\n",
      "step:  664\n",
      "loss: 0.0852\n",
      "a: 0.0416\n",
      "step:  665\n",
      "loss: 0.4661\n",
      "a: 0.0415\n",
      "step:  666\n",
      "loss: 0.1515\n",
      "a: 0.0414\n",
      "step:  667\n",
      "loss: 0.1646\n",
      "a: 0.0413\n",
      "step:  668\n",
      "loss: 0.1136\n",
      "a: 0.0412\n",
      "step:  669\n",
      "loss: 0.3850\n",
      "a: 0.0411\n",
      "step:  670\n",
      "loss: 0.2272\n",
      "a: 0.0410\n",
      "step:  671\n",
      "loss: 0.1098\n",
      "a: 0.0409\n",
      "step:  672\n",
      "loss: 0.3331\n",
      "a: 0.0408\n",
      "step:  673\n",
      "loss: 0.2319\n",
      "a: 0.0407\n",
      "step:  674\n",
      "loss: 0.1706\n",
      "a: 0.0406\n",
      "step:  675\n",
      "loss: 0.1754\n",
      "a: 0.0405\n",
      "step:  676\n",
      "loss: 0.1727\n",
      "a: 0.0404\n",
      "step:  677\n",
      "loss: 0.2314\n",
      "a: 0.0403\n",
      "step:  678\n",
      "loss: 0.4295\n",
      "a: 0.0402\n",
      "step:  679\n",
      "loss: 0.2328\n",
      "a: 0.0401\n",
      "step:  680\n",
      "loss: 0.1800\n",
      "a: 0.0400\n",
      "step:  681\n",
      "loss: 0.1010\n",
      "a: 0.0399\n",
      "step:  682\n",
      "loss: 0.2054\n",
      "a: 0.0398\n",
      "step:  683\n",
      "loss: 0.2710\n",
      "a: 0.0397\n",
      "step:  684\n",
      "loss: 0.3653\n",
      "a: 0.0396\n",
      "step:  685\n",
      "loss: 0.3986\n",
      "a: 0.0395\n",
      "step:  686\n",
      "loss: 0.1013\n",
      "a: 0.0394\n",
      "step:  687\n",
      "loss: 0.2105\n",
      "a: 0.0393\n",
      "step:  688\n",
      "loss: 0.3050\n",
      "a: 0.0392\n",
      "step:  689\n",
      "loss: 0.4611\n",
      "a: 0.0391\n",
      "step:  690\n",
      "loss: 0.2843\n",
      "a: 0.0390\n",
      "step:  691\n",
      "loss: 0.2020\n",
      "a: 0.0389\n",
      "step:  692\n",
      "loss: 0.2702\n",
      "a: 0.0388\n",
      "step:  693\n",
      "loss: 0.2200\n",
      "a: 0.0387\n",
      "step:  694\n",
      "loss: 0.1079\n",
      "a: 0.0386\n",
      "step:  695\n",
      "loss: 0.1655\n",
      "a: 0.0385\n",
      "step:  696\n",
      "loss: 0.1306\n",
      "a: 0.0384\n",
      "step:  697\n",
      "loss: 0.1976\n",
      "a: 0.0383\n",
      "step:  698\n",
      "loss: 0.0710\n",
      "a: 0.0382\n",
      "step:  699\n",
      "loss: 0.3169\n",
      "a: 0.0381\n",
      "step:  700\n",
      "loss: 0.3774\n",
      "a: 0.0380\n",
      "step:  701\n",
      "loss: 0.1304\n",
      "a: 0.0379\n",
      "step:  702\n",
      "loss: 0.3457\n",
      "a: 0.0378\n",
      "step:  703\n",
      "loss: 0.3120\n",
      "a: 0.0377\n",
      "step:  704\n",
      "loss: 0.1817\n",
      "a: 0.0376\n",
      "step:  705\n",
      "loss: 0.1931\n",
      "a: 0.0375\n",
      "step:  706\n",
      "loss: 0.2588\n",
      "a: 0.0374\n",
      "step:  707\n",
      "loss: 0.2969\n",
      "a: 0.0373\n",
      "step:  708\n",
      "loss: 0.6244\n",
      "a: 0.0372\n",
      "step:  709\n",
      "loss: 0.1802\n",
      "a: 0.0371\n",
      "step:  710\n",
      "loss: 0.1041\n",
      "a: 0.0370\n",
      "step:  711\n",
      "loss: 0.0546\n",
      "a: 0.0368\n",
      "step:  712\n",
      "loss: 0.4140\n",
      "a: 0.0367\n",
      "step:  713\n",
      "loss: 0.2046\n",
      "a: 0.0366\n",
      "step:  714\n",
      "loss: 0.0636\n",
      "a: 0.0365\n",
      "step:  715\n",
      "loss: 0.2324\n",
      "a: 0.0364\n",
      "step:  716\n",
      "loss: 0.3462\n",
      "a: 0.0363\n",
      "step:  717\n",
      "loss: 0.2806\n",
      "a: 0.0362\n",
      "step:  718\n",
      "loss: 0.4595\n",
      "a: 0.0361\n",
      "step:  719\n",
      "loss: 0.1139\n",
      "a: 0.0360\n",
      "step:  720\n",
      "loss: 0.2645\n",
      "a: 0.0359\n",
      "step:  721\n",
      "loss: 0.4116\n",
      "a: 0.0358\n",
      "step:  722\n",
      "loss: 0.2835\n",
      "a: 0.0357\n",
      "step:  723\n",
      "loss: 0.5575\n",
      "a: 0.0356\n",
      "step:  724\n",
      "loss: 0.1376\n",
      "a: 0.0355\n",
      "step:  725\n",
      "loss: 0.1536\n",
      "a: 0.0354\n",
      "step:  726\n",
      "loss: 0.1218\n",
      "a: 0.0353\n",
      "step:  727\n",
      "loss: 0.0875\n",
      "a: 0.0352\n",
      "step:  728\n",
      "loss: 0.2175\n",
      "a: 0.0351\n",
      "step:  729\n",
      "loss: 0.1064\n",
      "a: 0.0350\n",
      "step:  730\n",
      "loss: 0.1731\n",
      "a: 0.0349\n",
      "step:  731\n",
      "loss: 0.3876\n",
      "a: 0.0348\n",
      "step:  732\n",
      "loss: 0.1144\n",
      "a: 0.0347\n",
      "step:  733\n",
      "loss: 0.1303\n",
      "a: 0.0346\n",
      "step:  734\n",
      "loss: 0.0588\n",
      "a: 0.0345\n",
      "step:  735\n",
      "loss: 0.1875\n",
      "a: 0.0344\n",
      "step:  736\n",
      "loss: 0.2913\n",
      "a: 0.0343\n",
      "step:  737\n",
      "loss: 0.2325\n",
      "a: 0.0342\n",
      "step:  738\n",
      "loss: 0.1973\n",
      "a: 0.0341\n",
      "step:  739\n",
      "loss: 0.1243\n",
      "a: 0.0340\n",
      "step:  740\n",
      "loss: 0.3132\n",
      "a: 0.0339\n",
      "step:  741\n",
      "loss: 0.2285\n",
      "a: 0.0338\n",
      "step:  742\n",
      "loss: 0.2993\n",
      "a: 0.0337\n",
      "step:  743\n",
      "loss: 0.1220\n",
      "a: 0.0336\n",
      "step:  744\n",
      "loss: 0.2334\n",
      "a: 0.0335\n",
      "step:  745\n",
      "loss: 0.2616\n",
      "a: 0.0334\n",
      "step:  746\n",
      "loss: 0.4244\n",
      "a: 0.0333\n",
      "step:  747\n",
      "loss: 0.1839\n",
      "a: 0.0332\n",
      "step:  748\n",
      "loss: 0.3448\n",
      "a: 0.0331\n",
      "step:  749\n",
      "loss: 0.1725\n",
      "a: 0.0330\n",
      "step:  750\n",
      "loss: 0.3517\n",
      "a: 0.0329\n",
      "step:  751\n",
      "loss: 0.1615\n",
      "a: 0.0328\n",
      "step:  752\n",
      "loss: 0.6675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 0.0327\n",
      "step:  753\n",
      "loss: 0.1376\n",
      "a: 0.0326\n",
      "step:  754\n",
      "loss: 0.2635\n",
      "a: 0.0325\n",
      "step:  755\n",
      "loss: 0.2925\n",
      "a: 0.0324\n",
      "step:  756\n",
      "loss: 0.2000\n",
      "a: 0.0323\n",
      "step:  757\n",
      "loss: 0.1995\n",
      "a: 0.0322\n",
      "step:  758\n",
      "loss: 0.1545\n",
      "a: 0.0321\n",
      "step:  759\n",
      "loss: 0.0560\n",
      "a: 0.0320\n",
      "step:  760\n",
      "loss: 0.4216\n",
      "a: 0.0319\n",
      "step:  761\n",
      "loss: 0.5103\n",
      "a: 0.0318\n",
      "step:  762\n",
      "loss: 0.1678\n",
      "a: 0.0317\n",
      "step:  763\n",
      "loss: 0.1046\n",
      "a: 0.0316\n",
      "step:  764\n",
      "loss: 0.2829\n",
      "a: 0.0315\n",
      "step:  765\n",
      "loss: 0.2714\n",
      "a: 0.0314\n",
      "step:  766\n",
      "loss: 0.1551\n",
      "a: 0.0313\n",
      "step:  767\n",
      "loss: 0.3221\n",
      "a: 0.0312\n",
      "step:  768\n",
      "loss: 0.1445\n",
      "a: 0.0311\n",
      "step:  769\n",
      "loss: 0.2356\n",
      "a: 0.0310\n",
      "step:  770\n",
      "loss: 0.2047\n",
      "a: 0.0309\n",
      "step:  771\n",
      "loss: 0.1800\n",
      "a: 0.0308\n",
      "step:  772\n",
      "loss: 0.0576\n",
      "a: 0.0307\n",
      "step:  773\n",
      "loss: 0.1544\n",
      "a: 0.0306\n",
      "step:  774\n",
      "loss: 0.1457\n",
      "a: 0.0305\n",
      "step:  775\n",
      "loss: 0.2190\n",
      "a: 0.0304\n",
      "step:  776\n",
      "loss: 0.4300\n",
      "a: 0.0303\n",
      "step:  777\n",
      "loss: 0.2516\n",
      "a: 0.0302\n",
      "step:  778\n",
      "loss: 0.1178\n",
      "a: 0.0301\n",
      "step:  779\n",
      "loss: 0.2166\n",
      "a: 0.0300\n",
      "step:  780\n",
      "loss: 0.3147\n",
      "a: 0.0299\n",
      "step:  781\n",
      "loss: 0.3132\n",
      "a: 0.0298\n",
      "step:  782\n",
      "loss: 0.2035\n",
      "a: 0.0297\n",
      "step:  783\n",
      "loss: 0.2462\n",
      "a: 0.0295\n",
      "step:  784\n",
      "loss: 0.0999\n",
      "a: 0.0294\n",
      "step:  785\n",
      "loss: 0.6484\n",
      "a: 0.0293\n",
      "step:  786\n",
      "loss: 0.4406\n",
      "a: 0.0292\n",
      "step:  787\n",
      "loss: 0.1080\n",
      "a: 0.0291\n",
      "step:  788\n",
      "loss: 0.4751\n",
      "a: 0.0290\n",
      "step:  789\n",
      "loss: 0.5473\n",
      "a: 0.0289\n",
      "step:  790\n",
      "loss: 0.3646\n",
      "a: 0.0288\n",
      "step:  791\n",
      "loss: 0.1145\n",
      "a: 0.0287\n",
      "step:  792\n",
      "loss: 0.1192\n",
      "a: 0.0286\n",
      "step:  793\n",
      "loss: 0.5565\n",
      "a: 0.0285\n",
      "step:  794\n",
      "loss: 0.2599\n",
      "a: 0.0284\n",
      "step:  795\n",
      "loss: 0.2424\n",
      "a: 0.0283\n",
      "step:  796\n",
      "loss: 0.1447\n",
      "a: 0.0282\n",
      "step:  797\n",
      "loss: 0.2072\n",
      "a: 0.0281\n",
      "step:  798\n",
      "loss: 0.1877\n",
      "a: 0.0280\n",
      "step:  799\n",
      "loss: 0.0814\n",
      "a: 0.0279\n",
      "step:  800\n",
      "loss: 0.2078\n",
      "a: 0.0278\n",
      "step:  801\n",
      "loss: 0.2617\n",
      "a: 0.0277\n",
      "step:  802\n",
      "loss: 0.0916\n",
      "a: 0.0276\n",
      "step:  803\n",
      "loss: 0.1006\n",
      "a: 0.0275\n",
      "step:  804\n",
      "loss: 0.2016\n",
      "a: 0.0274\n",
      "step:  805\n",
      "loss: 0.0594\n",
      "a: 0.0273\n",
      "step:  806\n",
      "loss: 0.0882\n",
      "a: 0.0272\n",
      "step:  807\n",
      "loss: 0.2932\n",
      "a: 0.0271\n",
      "step:  808\n",
      "loss: 0.1848\n",
      "a: 0.0270\n",
      "step:  809\n",
      "loss: 0.2803\n",
      "a: 0.0269\n",
      "step:  810\n",
      "loss: 0.2526\n",
      "a: 0.0268\n",
      "step:  811\n",
      "loss: 0.1916\n",
      "a: 0.0267\n",
      "step:  812\n",
      "loss: 0.2064\n",
      "a: 0.0266\n",
      "step:  813\n",
      "loss: 0.3714\n",
      "a: 0.0265\n",
      "step:  814\n",
      "loss: 0.1562\n",
      "a: 0.0264\n",
      "step:  815\n",
      "loss: 0.2432\n",
      "a: 0.0263\n",
      "step:  816\n",
      "loss: 0.3480\n",
      "a: 0.0262\n",
      "step:  817\n",
      "loss: 0.0995\n",
      "a: 0.0261\n",
      "step:  818\n",
      "loss: 0.0877\n",
      "a: 0.0260\n",
      "step:  819\n",
      "loss: 0.2271\n",
      "a: 0.0259\n",
      "step:  820\n",
      "loss: 0.3206\n",
      "a: 0.0258\n",
      "step:  821\n",
      "loss: 0.4552\n",
      "a: 0.0257\n",
      "step:  822\n",
      "loss: 0.4952\n",
      "a: 0.0256\n",
      "step:  823\n",
      "loss: 0.2857\n",
      "a: 0.0255\n",
      "step:  824\n",
      "loss: 0.1166\n",
      "a: 0.0254\n",
      "step:  825\n",
      "loss: 0.1429\n",
      "a: 0.0253\n",
      "step:  826\n",
      "loss: 0.3819\n",
      "a: 0.0252\n",
      "step:  827\n",
      "loss: 0.2952\n",
      "a: 0.0251\n",
      "step:  828\n",
      "loss: 0.4234\n",
      "a: 0.0250\n",
      "step:  829\n",
      "loss: 0.2971\n",
      "a: 0.0249\n",
      "step:  830\n",
      "loss: 0.2761\n",
      "a: 0.0248\n",
      "step:  831\n",
      "loss: 0.2263\n",
      "a: 0.0247\n",
      "step:  832\n",
      "loss: 0.2488\n",
      "a: 0.0246\n",
      "step:  833\n",
      "loss: 0.0605\n",
      "a: 0.0245\n",
      "step:  834\n",
      "loss: 0.1462\n",
      "a: 0.0244\n",
      "step:  835\n",
      "loss: 0.1673\n",
      "a: 0.0243\n",
      "step:  836\n",
      "loss: 0.1463\n",
      "a: 0.0242\n",
      "step:  837\n",
      "loss: 0.4555\n",
      "a: 0.0241\n",
      "step:  838\n",
      "loss: 0.2471\n",
      "a: 0.0240\n",
      "step:  839\n",
      "loss: 0.2998\n",
      "a: 0.0239\n",
      "step:  840\n",
      "loss: 0.1597\n",
      "a: 0.0238\n",
      "step:  841\n",
      "loss: 0.1758\n",
      "a: 0.0237\n",
      "step:  842\n",
      "loss: 0.1331\n",
      "a: 0.0236\n",
      "step:  843\n",
      "loss: 0.4635\n",
      "a: 0.0235\n",
      "step:  844\n",
      "loss: 0.1341\n",
      "a: 0.0234\n",
      "step:  845\n",
      "loss: 0.1528\n",
      "a: 0.0233\n",
      "step:  846\n",
      "loss: 0.4140\n",
      "a: 0.0232\n",
      "step:  847\n",
      "loss: 0.2222\n",
      "a: 0.0231\n",
      "step:  848\n",
      "loss: 0.2230\n",
      "a: 0.0230\n",
      "step:  849\n",
      "loss: 0.1279\n",
      "a: 0.0229\n",
      "step:  850\n",
      "loss: 0.0597\n",
      "a: 0.0228\n",
      "step:  851\n",
      "loss: 0.3783\n",
      "a: 0.0227\n",
      "step:  852\n",
      "loss: 0.1919\n",
      "a: 0.0226\n",
      "step:  853\n",
      "loss: 0.1643\n",
      "a: 0.0225\n",
      "step:  854\n",
      "loss: 0.2022\n",
      "a: 0.0224\n",
      "step:  855\n",
      "loss: 0.2295\n",
      "a: 0.0223\n",
      "step:  856\n",
      "loss: 0.1156\n",
      "a: 0.0222\n",
      "step:  857\n",
      "loss: 0.1393\n",
      "a: 0.0221\n",
      "step:  858\n",
      "loss: 0.1867\n",
      "a: 0.0220\n",
      "step:  859\n",
      "loss: 0.1721\n",
      "a: 0.0220\n",
      "step:  860\n",
      "loss: 0.5883\n",
      "a: 0.0219\n",
      "step:  861\n",
      "loss: 0.0828\n",
      "a: 0.0218\n",
      "step:  862\n",
      "loss: 0.2521\n",
      "a: 0.0217\n",
      "step:  863\n",
      "loss: 0.4077\n",
      "a: 0.0216\n",
      "step:  864\n",
      "loss: 0.1321\n",
      "a: 0.0215\n",
      "step:  865\n",
      "loss: 0.1413\n",
      "a: 0.0214\n",
      "step:  866\n",
      "loss: 0.1323\n",
      "a: 0.0212\n",
      "step:  867\n",
      "loss: 0.2206\n",
      "a: 0.0211\n",
      "step:  868\n",
      "loss: 0.3386\n",
      "a: 0.0210\n",
      "step:  869\n",
      "loss: 0.3370\n",
      "a: 0.0209\n",
      "step:  870\n",
      "loss: 0.1235\n",
      "a: 0.0208\n",
      "step:  871\n",
      "loss: 0.2085\n",
      "a: 0.0207\n",
      "step:  872\n",
      "loss: 0.3339\n",
      "a: 0.0206\n",
      "step:  873\n",
      "loss: 0.2233\n",
      "a: 0.0205\n",
      "step:  874\n",
      "loss: 0.0809\n",
      "a: 0.0204\n",
      "step:  875\n",
      "loss: 0.3125\n",
      "a: 0.0204\n",
      "step:  876\n",
      "loss: 0.3386\n",
      "a: 0.0203\n",
      "step:  877\n",
      "loss: 0.4686\n",
      "a: 0.0202\n",
      "step:  878\n",
      "loss: 0.1166\n",
      "a: 0.0201\n",
      "step:  879\n",
      "loss: 0.1134\n",
      "a: 0.0200\n",
      "step:  880\n",
      "loss: 0.4143\n",
      "a: 0.0199\n",
      "step:  881\n",
      "loss: 0.1095\n",
      "a: 0.0198\n",
      "step:  882\n",
      "loss: 0.2189\n",
      "a: 0.0197\n",
      "step:  883\n",
      "loss: 0.2199\n",
      "a: 0.0196\n",
      "step:  884\n",
      "loss: 0.3950\n",
      "a: 0.0195\n",
      "step:  885\n",
      "loss: 0.1968\n",
      "a: 0.0194\n",
      "step:  886\n",
      "loss: 0.3321\n",
      "a: 0.0193\n",
      "step:  887\n",
      "loss: 0.0670\n",
      "a: 0.0192\n",
      "step:  888\n",
      "loss: 0.3541\n",
      "a: 0.0191\n",
      "step:  889\n",
      "loss: 0.1054\n",
      "a: 0.0190\n",
      "step:  890\n",
      "loss: 0.1819\n",
      "a: 0.0189\n",
      "step:  891\n",
      "loss: 0.0887\n",
      "a: 0.0188\n",
      "step:  892\n",
      "loss: 0.2808\n",
      "a: 0.0187\n",
      "step:  893\n",
      "loss: 0.2487\n",
      "a: 0.0186\n",
      "step:  894\n",
      "loss: 0.2543\n",
      "a: 0.0185\n",
      "step:  895\n",
      "loss: 0.1409\n",
      "a: 0.0184\n",
      "step:  896\n",
      "loss: 0.3433\n",
      "a: 0.0182\n",
      "step:  897\n",
      "loss: 0.4987\n",
      "a: 0.0181\n",
      "step:  898\n",
      "loss: 0.0688\n",
      "a: 0.0180\n",
      "step:  899\n",
      "loss: 0.1449\n",
      "a: 0.0179\n",
      "step:  900\n",
      "loss: 0.3147\n",
      "a: 0.0178\n",
      "step:  901\n",
      "loss: 0.3002\n",
      "a: 0.0177\n",
      "step:  902\n",
      "loss: 0.1973\n",
      "a: 0.0176\n",
      "step:  903\n",
      "loss: 0.3440\n",
      "a: 0.0175\n",
      "step:  904\n",
      "loss: 0.1812\n",
      "a: 0.0174\n",
      "step:  905\n",
      "loss: 0.3029\n",
      "a: 0.0173\n",
      "step:  906\n",
      "loss: 0.2657\n",
      "a: 0.0172\n",
      "step:  907\n",
      "loss: 0.2341\n",
      "a: 0.0171\n",
      "step:  908\n",
      "loss: 0.0583\n",
      "a: 0.0170\n",
      "step:  909\n",
      "loss: 0.4324\n",
      "a: 0.0169\n",
      "step:  910\n",
      "loss: 0.5643\n",
      "a: 0.0168\n",
      "step:  911\n",
      "loss: 0.1133\n",
      "a: 0.0167\n",
      "step:  912\n",
      "loss: 0.1656\n",
      "a: 0.0166\n",
      "step:  913\n",
      "loss: 0.3894\n",
      "a: 0.0165\n",
      "step:  914\n",
      "loss: 0.1785\n",
      "a: 0.0164\n",
      "step:  915\n",
      "loss: 0.2075\n",
      "a: 0.0163\n",
      "step:  916\n",
      "loss: 0.2486\n",
      "a: 0.0162\n",
      "step:  917\n",
      "loss: 0.2793\n",
      "a: 0.0161\n",
      "step:  918\n",
      "loss: 0.4373\n",
      "a: 0.0160\n",
      "step:  919\n",
      "loss: 0.1843\n",
      "a: 0.0159\n",
      "step:  920\n",
      "loss: 0.4518\n",
      "a: 0.0158\n",
      "step:  921\n",
      "loss: 0.2990\n",
      "a: 0.0157\n",
      "step:  922\n",
      "loss: 0.2311\n",
      "a: 0.0156\n",
      "step:  923\n",
      "loss: 0.0596\n",
      "a: 0.0155\n",
      "step:  924\n",
      "loss: 0.0913\n",
      "a: 0.0154\n",
      "step:  925\n",
      "loss: 0.3046\n",
      "a: 0.0153\n",
      "step:  926\n",
      "loss: 0.3059\n",
      "a: 0.0152\n",
      "step:  927\n",
      "loss: 0.1566\n",
      "a: 0.0151\n",
      "step:  928\n",
      "loss: 0.3764\n",
      "a: 0.0150\n",
      "step:  929\n",
      "loss: 0.2636\n",
      "a: 0.0149\n",
      "step:  930\n",
      "loss: 0.4074\n",
      "a: 0.0148\n",
      "step:  931\n",
      "loss: 0.3141\n",
      "a: 0.0147\n",
      "step:  932\n",
      "loss: 0.1256\n",
      "a: 0.0146\n",
      "step:  933\n",
      "loss: 0.2915\n",
      "a: 0.0145\n",
      "step:  934\n",
      "loss: 0.2875\n",
      "a: 0.0144\n",
      "step:  935\n",
      "loss: 0.1287\n",
      "a: 0.0143\n",
      "step:  936\n",
      "loss: 0.2608\n",
      "a: 0.0142\n",
      "step:  937\n",
      "loss: 0.1248\n",
      "a: 0.0141\n",
      "step:  938\n",
      "loss: 0.3777\n",
      "a: 0.0140\n",
      "step:  939\n",
      "loss: 0.1477\n",
      "a: 0.0139\n",
      "step:  940\n",
      "loss: 0.2480\n",
      "a: 0.0138\n",
      "step:  941\n",
      "loss: 0.2840\n",
      "a: 0.0137\n",
      "step:  942\n",
      "loss: 0.2570\n",
      "a: 0.0136\n",
      "step:  943\n",
      "loss: 0.1269\n",
      "a: 0.0135\n",
      "step:  944\n",
      "loss: 0.1234\n",
      "a: 0.0135\n",
      "step:  945\n",
      "loss: 0.4953\n",
      "a: 0.0134\n",
      "step:  946\n",
      "loss: 0.1533\n",
      "a: 0.0133\n",
      "step:  947\n",
      "loss: 0.2695\n",
      "a: 0.0132\n",
      "step:  948\n",
      "loss: 0.1175\n",
      "a: 0.0131\n",
      "step:  949\n",
      "loss: 0.1683\n",
      "a: 0.0130\n",
      "step:  950\n",
      "loss: 0.2145\n",
      "a: 0.0129\n",
      "step:  951\n",
      "loss: 0.1206\n",
      "a: 0.0128\n",
      "step:  952\n",
      "loss: 0.2055\n",
      "a: 0.0126\n",
      "step:  953\n",
      "loss: 0.1935\n",
      "a: 0.0125\n",
      "step:  954\n",
      "loss: 0.2168\n",
      "a: 0.0124\n",
      "step:  955\n",
      "loss: 0.2548\n",
      "a: 0.0123\n",
      "step:  956\n",
      "loss: 0.0944\n",
      "a: 0.0122\n",
      "step:  957\n",
      "loss: 0.1873\n",
      "a: 0.0121\n",
      "step:  958\n",
      "loss: 0.1626\n",
      "a: 0.0120\n",
      "step:  959\n",
      "loss: 0.3380\n",
      "a: 0.0119\n",
      "step:  960\n",
      "loss: 0.2718\n",
      "a: 0.0118\n",
      "step:  961\n",
      "loss: 0.0935\n",
      "a: 0.0116\n",
      "step:  962\n",
      "loss: 0.2040\n",
      "a: 0.0115\n",
      "step:  963\n",
      "loss: 0.4894\n",
      "a: 0.0114\n",
      "step:  964\n",
      "loss: 0.2174\n",
      "a: 0.0113\n",
      "step:  965\n",
      "loss: 0.5335\n",
      "a: 0.0112\n",
      "step:  966\n",
      "loss: 0.1073\n",
      "a: 0.0111\n",
      "step:  967\n",
      "loss: 0.0550\n",
      "a: 0.0110\n",
      "step:  968\n",
      "loss: 0.0602\n",
      "a: 0.0109\n",
      "step:  969\n",
      "loss: 0.2411\n",
      "a: 0.0108\n",
      "step:  970\n",
      "loss: 0.1653\n",
      "a: 0.0107\n",
      "step:  971\n",
      "loss: 0.3738\n",
      "a: 0.0106\n",
      "step:  972\n",
      "loss: 0.3294\n",
      "a: 0.0105\n",
      "step:  973\n",
      "loss: 0.3583\n",
      "a: 0.0104\n",
      "step:  974\n",
      "loss: 0.1723\n",
      "a: 0.0103\n",
      "step:  975\n",
      "loss: 0.2321\n",
      "a: 0.0102\n",
      "step:  976\n",
      "loss: 0.3803\n",
      "a: 0.0102\n",
      "step:  977\n",
      "loss: 0.2442\n",
      "a: 0.0101\n",
      "step:  978\n",
      "loss: 0.2672\n",
      "a: 0.0100\n",
      "step:  979\n",
      "loss: 0.1894\n",
      "a: 0.0099\n",
      "step:  980\n",
      "loss: 0.1498\n",
      "a: 0.0098\n",
      "step:  981\n",
      "loss: 0.4400\n",
      "a: 0.0097\n",
      "step:  982\n",
      "loss: 0.1462\n",
      "a: 0.0096\n",
      "step:  983\n",
      "loss: 0.1738\n",
      "a: 0.0095\n",
      "step:  984\n",
      "loss: 0.1882\n",
      "a: 0.0094\n",
      "step:  985\n",
      "loss: 0.2502\n",
      "a: 0.0093\n",
      "step:  986\n",
      "loss: 0.1820\n",
      "a: 0.0092\n",
      "step:  987\n",
      "loss: 0.4059\n",
      "a: 0.0091\n",
      "step:  988\n",
      "loss: 0.1277\n",
      "a: 0.0090\n",
      "step:  989\n",
      "loss: 0.1370\n",
      "a: 0.0089\n",
      "step:  990\n",
      "loss: 0.2582\n",
      "a: 0.0088\n",
      "step:  991\n",
      "loss: 0.3347\n",
      "a: 0.0087\n",
      "step:  992\n",
      "loss: 0.3280\n",
      "a: 0.0086\n",
      "step:  993\n",
      "loss: 0.3108\n",
      "a: 0.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  994\n",
      "loss: 0.2734\n",
      "a: 0.0083\n",
      "step:  995\n",
      "loss: 0.2718\n",
      "a: 0.0082\n",
      "step:  996\n",
      "loss: 0.3278\n",
      "a: 0.0081\n",
      "step:  997\n",
      "loss: 0.3749\n",
      "a: 0.0080\n",
      "step:  998\n",
      "loss: 0.1701\n",
      "a: 0.0079\n",
      "step:  999\n",
      "loss: 0.2717\n",
      "a: 0.0078\n",
      "step:  1000\n",
      "loss: 0.1175\n",
      "a: 0.0077\n",
      "step:  1001\n",
      "loss: 0.2728\n",
      "a: 0.0076\n",
      "step:  1002\n",
      "loss: 0.1936\n",
      "a: 0.0076\n",
      "step:  1003\n",
      "loss: 0.1517\n",
      "a: 0.0075\n",
      "step:  1004\n",
      "loss: 0.1753\n",
      "a: 0.0074\n",
      "step:  1005\n",
      "loss: 0.1013\n",
      "a: 0.0073\n",
      "step:  1006\n",
      "loss: 0.2378\n",
      "a: 0.0072\n",
      "step:  1007\n",
      "loss: 0.2896\n",
      "a: 0.0071\n",
      "step:  1008\n",
      "loss: 0.1855\n",
      "a: 0.0071\n",
      "step:  1009\n",
      "loss: 0.2725\n",
      "a: 0.0070\n",
      "step:  1010\n",
      "loss: 0.2275\n",
      "a: 0.0069\n",
      "step:  1011\n",
      "loss: 0.1296\n",
      "a: 0.0068\n",
      "step:  1012\n",
      "loss: 0.4042\n",
      "a: 0.0067\n",
      "step:  1013\n",
      "loss: 0.1108\n",
      "a: 0.0066\n",
      "step:  1014\n",
      "loss: 0.2529\n",
      "a: 0.0065\n",
      "step:  1015\n",
      "loss: 0.3133\n",
      "a: 0.0064\n",
      "step:  1016\n",
      "loss: 0.1841\n",
      "a: 0.0063\n",
      "step:  1017\n",
      "loss: 0.1481\n",
      "a: 0.0062\n",
      "step:  1018\n",
      "loss: 0.2028\n",
      "a: 0.0061\n",
      "step:  1019\n",
      "loss: 0.1102\n",
      "a: 0.0060\n",
      "step:  1020\n",
      "loss: 0.3476\n",
      "a: 0.0059\n",
      "step:  1021\n",
      "loss: 0.1244\n",
      "a: 0.0058\n",
      "step:  1022\n",
      "loss: 0.1806\n",
      "a: 0.0057\n",
      "step:  1023\n",
      "loss: 0.2596\n",
      "a: 0.0056\n",
      "step:  1024\n",
      "loss: 0.2115\n",
      "a: 0.0055\n",
      "step:  1025\n",
      "loss: 0.2030\n",
      "a: 0.0054\n",
      "step:  1026\n",
      "loss: 0.4751\n",
      "a: 0.0054\n",
      "step:  1027\n",
      "loss: 0.2597\n",
      "a: 0.0053\n",
      "step:  1028\n",
      "loss: 0.2641\n",
      "a: 0.0052\n",
      "step:  1029\n",
      "loss: 0.1412\n",
      "a: 0.0051\n",
      "step:  1030\n",
      "loss: 0.1362\n",
      "a: 0.0050\n",
      "step:  1031\n",
      "loss: 0.3646\n",
      "a: 0.0049\n",
      "step:  1032\n",
      "loss: 0.1100\n",
      "a: 0.0048\n",
      "step:  1033\n",
      "loss: 0.2753\n",
      "a: 0.0047\n",
      "step:  1034\n",
      "loss: 0.2140\n",
      "a: 0.0046\n",
      "step:  1035\n",
      "loss: 0.5572\n",
      "a: 0.0044\n",
      "step:  1036\n",
      "loss: 0.1857\n",
      "a: 0.0043\n",
      "step:  1037\n",
      "loss: 0.3929\n",
      "a: 0.0042\n",
      "step:  1038\n",
      "loss: 0.4525\n",
      "a: 0.0041\n",
      "step:  1039\n",
      "loss: 0.2491\n",
      "a: 0.0040\n",
      "step:  1040\n",
      "loss: 0.1314\n",
      "a: 0.0039\n",
      "step:  1041\n",
      "loss: 0.2402\n",
      "a: 0.0039\n",
      "step:  1042\n",
      "loss: 0.0766\n",
      "a: 0.0038\n",
      "step:  1043\n",
      "loss: 0.0371\n",
      "a: 0.0037\n",
      "step:  1044\n",
      "loss: 0.1093\n",
      "a: 0.0036\n",
      "step:  1045\n",
      "loss: 0.3386\n",
      "a: 0.0036\n",
      "step:  1046\n",
      "loss: 0.3393\n",
      "a: 0.0035\n",
      "step:  1047\n",
      "loss: 0.0251\n",
      "a: 0.0034\n",
      "step:  1048\n",
      "loss: 0.0849\n",
      "a: 0.0034\n",
      "step:  1049\n",
      "loss: 0.2635\n",
      "a: 0.0033\n",
      "step:  1050\n",
      "loss: 0.2052\n",
      "a: 0.0033\n",
      "step:  1051\n",
      "loss: 0.2885\n",
      "a: 0.0032\n",
      "step:  1052\n",
      "loss: 0.0840\n",
      "a: 0.0032\n",
      "step:  1053\n",
      "loss: 0.2052\n",
      "a: 0.0032\n",
      "step:  1054\n",
      "loss: 0.1258\n",
      "a: 0.0031\n",
      "step:  1055\n",
      "loss: 0.4759\n",
      "a: 0.0031\n",
      "step:  1056\n",
      "loss: 0.0840\n",
      "a: 0.0031\n",
      "step:  1057\n",
      "loss: 0.2974\n",
      "a: 0.0031\n",
      "step:  1058\n",
      "loss: 0.1584\n",
      "a: 0.0031\n",
      "step:  1059\n",
      "loss: 0.4537\n",
      "a: 0.0030\n",
      "step:  1060\n",
      "loss: 0.3684\n",
      "a: 0.0030\n",
      "step:  1061\n",
      "loss: 0.0556\n",
      "a: 0.0029\n",
      "step:  1062\n",
      "loss: 0.0498\n",
      "a: 0.0028\n",
      "step:  1063\n",
      "loss: 0.0939\n",
      "a: 0.0027\n",
      "step:  1064\n",
      "loss: 0.3290\n",
      "a: 0.0026\n",
      "step:  1065\n",
      "loss: 0.5482\n",
      "a: 0.0026\n",
      "step:  1066\n",
      "loss: 0.2029\n",
      "a: 0.0025\n",
      "step:  1067\n",
      "loss: 0.5927\n",
      "a: 0.0024\n",
      "step:  1068\n",
      "loss: 0.0582\n",
      "a: 0.0023\n",
      "step:  1069\n",
      "loss: 0.1758\n",
      "a: 0.0022\n",
      "step:  1070\n",
      "loss: 0.3521\n",
      "a: 0.0021\n",
      "step:  1071\n",
      "loss: 0.2193\n",
      "a: 0.0020\n",
      "step:  1072\n",
      "loss: 0.1499\n",
      "a: 0.0019\n",
      "step:  1073\n",
      "loss: 0.2529\n",
      "a: 0.0019\n",
      "step:  1074\n",
      "loss: 0.2686\n",
      "a: 0.0018\n",
      "step:  1075\n",
      "loss: 0.1792\n",
      "a: 0.0017\n",
      "step:  1076\n",
      "loss: 0.0643\n",
      "a: 0.0016\n",
      "step:  1077\n",
      "loss: 0.1976\n",
      "a: 0.0015\n",
      "step:  1078\n",
      "loss: 0.2703\n",
      "a: 0.0013\n",
      "step:  1079\n",
      "loss: 0.0476\n",
      "a: 0.0012\n",
      "step:  1080\n",
      "loss: 0.1200\n",
      "a: 0.0011\n",
      "step:  1081\n",
      "loss: 0.2330\n",
      "a: 0.0010\n",
      "step:  1082\n",
      "loss: 0.0751\n",
      "a: 0.0009\n",
      "step:  1083\n",
      "loss: 0.1303\n",
      "a: 0.0008\n",
      "step:  1084\n",
      "loss: 0.1989\n",
      "a: 0.0007\n",
      "step:  1085\n",
      "loss: 0.2603\n",
      "a: 0.0006\n",
      "step:  1086\n",
      "loss: 0.0799\n",
      "a: 0.0006\n",
      "step:  1087\n",
      "loss: 0.2442\n",
      "a: 0.0005\n",
      "step:  1088\n",
      "loss: 0.2748\n",
      "a: 0.0005\n",
      "step:  1089\n",
      "loss: 0.1508\n",
      "a: 0.0005\n",
      "step:  1090\n",
      "loss: 0.0592\n",
      "a: 0.0005\n",
      "step:  1091\n",
      "loss: 0.2983\n",
      "a: 0.0006\n",
      "step:  1092\n",
      "loss: 0.1937\n",
      "a: 0.0006\n",
      "step:  1093\n",
      "loss: 0.3631\n",
      "a: 0.0006\n",
      "step:  1094\n",
      "loss: 0.0376\n",
      "a: 0.0006\n",
      "step:  1095\n",
      "loss: 0.1010\n",
      "a: 0.0007\n",
      "step:  1096\n",
      "loss: 0.2077\n",
      "a: 0.0007\n",
      "step:  1097\n",
      "loss: 0.1538\n",
      "a: 0.0007\n",
      "step:  1098\n",
      "loss: 0.3993\n",
      "a: 0.0007\n",
      "step:  1099\n",
      "loss: 0.2069\n",
      "a: 0.0006\n",
      "step:  1100\n",
      "loss: 0.1347\n",
      "a: 0.0006\n",
      "step:  1101\n",
      "loss: 0.2274\n",
      "a: 0.0006\n",
      "step:  1102\n",
      "loss: 0.3401\n",
      "a: 0.0005\n",
      "step:  1103\n",
      "loss: 0.1027\n",
      "a: 0.0005\n",
      "step:  1104\n",
      "loss: 0.2125\n",
      "a: 0.0004\n",
      "step:  1105\n",
      "loss: 0.2053\n",
      "a: 0.0003\n",
      "step:  1106\n",
      "loss: 0.3643\n",
      "a: 0.0003\n",
      "step:  1107\n",
      "loss: 0.2246\n",
      "a: 0.0003\n",
      "step:  1108\n",
      "loss: 0.1721\n",
      "a: 0.0003\n",
      "step:  1109\n",
      "loss: 0.2626\n",
      "a: 0.0003\n",
      "step:  1110\n",
      "loss: 0.3323\n",
      "a: 0.0003\n",
      "step:  1111\n",
      "loss: 0.1232\n",
      "a: 0.0003\n",
      "step:  1112\n",
      "loss: 0.2195\n",
      "a: 0.0003\n",
      "step:  1113\n",
      "loss: 0.3283\n",
      "a: 0.0002\n",
      "step:  1114\n",
      "loss: 0.1457\n",
      "a: 0.0001\n",
      "step:  1115\n",
      "loss: 0.3355\n",
      "a: -0.0001\n",
      "step:  1116\n",
      "loss: 0.5547\n",
      "a: -0.0003\n",
      "step:  1117\n",
      "loss: 0.1205\n",
      "a: -0.0005\n",
      "step:  1118\n",
      "loss: 0.3945\n",
      "a: -0.0006\n",
      "step:  1119\n",
      "loss: 0.4714\n",
      "a: -0.0007\n",
      "step:  1120\n",
      "loss: 0.1986\n",
      "a: -0.0007\n",
      "step:  1121\n",
      "loss: 0.2766\n",
      "a: -0.0008\n",
      "step:  1122\n",
      "loss: 0.4062\n",
      "a: -0.0009\n",
      "step:  1123\n",
      "loss: 0.1875\n",
      "a: -0.0009\n",
      "step:  1124\n",
      "loss: 0.2256\n",
      "a: -0.0008\n",
      "step:  1125\n",
      "loss: 0.4112\n",
      "a: -0.0009\n",
      "step:  1126\n",
      "loss: 0.1140\n",
      "a: -0.0009\n",
      "step:  1127\n",
      "loss: 0.1643\n",
      "a: -0.0010\n",
      "step:  1128\n",
      "loss: 0.2558\n",
      "a: -0.0010\n",
      "step:  1129\n",
      "loss: 0.3697\n",
      "a: -0.0010\n",
      "step:  1130\n",
      "loss: 0.2493\n",
      "a: -0.0010\n",
      "step:  1131\n",
      "loss: 0.1927\n",
      "a: -0.0008\n",
      "step:  1132\n",
      "loss: 0.2758\n",
      "a: -0.0006\n",
      "step:  1133\n",
      "loss: 0.5277\n",
      "a: -0.0005\n",
      "step:  1134\n",
      "loss: 0.2549\n",
      "a: -0.0004\n",
      "step:  1135\n",
      "loss: 0.1992\n",
      "a: -0.0004\n",
      "step:  1136\n",
      "loss: 0.1315\n",
      "a: -0.0004\n",
      "step:  1137\n",
      "loss: 0.1676\n",
      "a: -0.0004\n",
      "step:  1138\n",
      "loss: 0.1700\n",
      "a: -0.0004\n",
      "step:  1139\n",
      "loss: 0.1278\n",
      "a: -0.0004\n",
      "step:  1140\n",
      "loss: 0.2076\n",
      "a: -0.0004\n",
      "step:  1141\n",
      "loss: 0.1772\n",
      "a: -0.0004\n",
      "step:  1142\n",
      "loss: 0.3892\n",
      "a: -0.0004\n",
      "step:  1143\n",
      "loss: 0.1216\n",
      "a: -0.0003\n",
      "step:  1144\n",
      "loss: 0.2656\n",
      "a: -0.0002\n",
      "step:  1145\n",
      "loss: 0.1167\n",
      "a: -0.0002\n",
      "step:  1146\n",
      "loss: 0.1726\n",
      "a: -0.0003\n",
      "step:  1147\n",
      "loss: 0.3747\n",
      "a: -0.0004\n",
      "step:  1148\n",
      "loss: 0.2323\n",
      "a: -0.0005\n",
      "step:  1149\n",
      "loss: 0.1668\n",
      "a: -0.0006\n",
      "step:  1150\n",
      "loss: 0.0814\n",
      "a: -0.0006\n",
      "step:  1151\n",
      "loss: 0.2217\n",
      "a: -0.0005\n",
      "step:  1152\n",
      "loss: 0.2887\n",
      "a: -0.0006\n",
      "step:  1153\n",
      "loss: 0.4795\n",
      "a: -0.0005\n",
      "step:  1154\n",
      "loss: 0.1015\n",
      "a: -0.0003\n",
      "step:  1155\n",
      "loss: 0.1647\n",
      "a: -0.0002\n",
      "step:  1156\n",
      "loss: 0.1625\n",
      "a: -0.0002\n",
      "step:  1157\n",
      "loss: 0.2139\n",
      "a: -0.0001\n",
      "step:  1158\n",
      "loss: 0.2813\n",
      "a: -0.0000\n",
      "step:  1159\n",
      "loss: 0.2370\n",
      "a: 0.0001\n",
      "step:  1160\n",
      "loss: 0.2770\n",
      "a: 0.0003\n",
      "step:  1161\n",
      "loss: 0.5554\n",
      "a: 0.0004\n",
      "step:  1162\n",
      "loss: 0.1726\n",
      "a: 0.0006\n",
      "step:  1163\n",
      "loss: 0.0978\n",
      "a: 0.0008\n",
      "step:  1164\n",
      "loss: 0.1746\n",
      "a: 0.0009\n",
      "step:  1165\n",
      "loss: 0.1169\n",
      "a: 0.0011\n",
      "step:  1166\n",
      "loss: 0.2203\n",
      "a: 0.0012\n",
      "step:  1167\n",
      "loss: 0.2370\n",
      "a: 0.0014\n",
      "step:  1168\n",
      "loss: 0.2100\n",
      "a: 0.0015\n",
      "step:  1169\n",
      "loss: 0.2643\n",
      "a: 0.0015\n",
      "step:  1170\n",
      "loss: 0.4769\n",
      "a: 0.0016\n",
      "step:  1171\n",
      "loss: 0.5372\n",
      "a: 0.0015\n",
      "step:  1172\n",
      "loss: 0.2226\n",
      "a: 0.0013\n",
      "step:  1173\n",
      "loss: 0.2461\n",
      "a: 0.0011\n",
      "step:  1174\n",
      "loss: 0.2003\n",
      "a: 0.0009\n",
      "step:  1175\n",
      "loss: 0.2331\n",
      "a: 0.0007\n",
      "step:  1176\n",
      "loss: 0.2749\n",
      "a: 0.0006\n",
      "step:  1177\n",
      "loss: 0.2239\n",
      "a: 0.0005\n",
      "step:  1178\n",
      "loss: 0.4199\n",
      "a: 0.0004\n",
      "step:  1179\n",
      "loss: 0.2620\n",
      "a: 0.0003\n",
      "step:  1180\n",
      "loss: 0.3168\n",
      "a: 0.0002\n",
      "step:  1181\n",
      "loss: 0.3618\n",
      "a: 0.0002\n",
      "step:  1182\n",
      "loss: 0.3694\n",
      "a: 0.0003\n",
      "step:  1183\n",
      "loss: 0.3198\n",
      "a: 0.0004\n",
      "step:  1184\n",
      "loss: 0.1235\n",
      "a: 0.0006\n",
      "step:  1185\n",
      "loss: 0.3199\n",
      "a: 0.0007\n",
      "step:  1186\n",
      "loss: 0.3062\n",
      "a: 0.0009\n",
      "step:  1187\n",
      "loss: 0.1645\n",
      "a: 0.0010\n",
      "step:  1188\n",
      "loss: 0.3473\n",
      "a: 0.0012\n",
      "step:  1189\n",
      "loss: 0.2078\n",
      "a: 0.0013\n",
      "step:  1190\n",
      "loss: 0.2594\n",
      "a: 0.0015\n",
      "step:  1191\n",
      "loss: 0.1719\n",
      "a: 0.0016\n",
      "step:  1192\n",
      "loss: 0.4136\n",
      "a: 0.0017\n",
      "step:  1193\n",
      "loss: 0.0926\n",
      "a: 0.0018\n",
      "step:  1194\n",
      "loss: 0.2717\n",
      "a: 0.0020\n",
      "step:  1195\n",
      "loss: 0.0340\n",
      "a: 0.0021\n",
      "step:  1196\n",
      "loss: 0.2470\n",
      "a: 0.0021\n",
      "step:  1197\n",
      "loss: 0.2837\n",
      "a: 0.0022\n",
      "step:  1198\n",
      "loss: 0.3044\n",
      "a: 0.0023\n",
      "step:  1199\n",
      "loss: 0.2589\n",
      "a: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model_prelu(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu parameter ```a``` is optimized in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code snippets to build the convolutional network:\n",
    "\n",
    "```python\n",
    "    from torch . nn . functional import conv2d , max_pool2d\n",
    "    convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "    subsampleing_layer = max_pool_2d ( convolutional_layer , (2 , 2) ) # reduces window 2x2 to 1 pixel\n",
    "    out_layer = dropout ( subsample_layer , p_drop_input )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  128\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "# modify for more speed\n",
    "#f1, f2, f3 = 20, 40, 80\n",
    "#pic_in1, pic_in2, pic_in3 = 1, 20, 40\n",
    "#k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "#k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        X = conv_layer(X, w_conv3, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, w_conv3, p_drop_input)\n",
    "    return Y.size()[1]\n",
    "\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, w_conv3, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([0.01], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 1., 1.)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_loader, test_loader, num_epochs, log_interval):\n",
    "    # run training\n",
    "    t1 = time.time()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, epoch, log_interval)\n",
    "        test(test_loader)       \n",
    "    t2 = time.time()\n",
    "    print('Training took {:.1f} seconds (or {:.1f} minutes)'.format(t2 - t1, (t2 - t1) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 14.3096\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 4.7954\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.2498\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.1114\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.1827\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.0474\n",
      "\n",
      "Test set: Average loss: 0.0410, Accuracy: 4479/10000 (44%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.0793\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.9478\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.8250\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.7619\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.4983\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.3147\n",
      "\n",
      "Test set: Average loss: 0.0196, Accuracy: 7359/10000 (73%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.3836\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.2182\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.1300\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.2226\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.9618\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.0778\n",
      "\n",
      "Test set: Average loss: 0.0122, Accuracy: 8130/10000 (81%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.9581\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.7349\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.0241\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.0109\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.7357\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.7037\n",
      "\n",
      "Test set: Average loss: 0.0101, Accuracy: 8420/10000 (84%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.7853\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.6162\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.5395\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.6564\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.7798\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.6899\n",
      "\n",
      "Test set: Average loss: 0.0078, Accuracy: 8853/10000 (88%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.5919\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.3602\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.8884\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.2573\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.6516\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.7503\n",
      "\n",
      "Test set: Average loss: 0.0066, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.6476\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.6218\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.6142\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.5746\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.5298\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.4838\n",
      "\n",
      "Test set: Average loss: 0.0059, Accuracy: 9134/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.4862\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.4531\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.4854\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.0832\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.4984\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.5572\n",
      "\n",
      "Test set: Average loss: 0.0053, Accuracy: 9228/10000 (92%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.4061\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.3986\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.6656\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.2778\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.2583\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.5161\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 9277/10000 (92%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.3773\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.5952\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.5757\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.5133\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.5487\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.3628\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 9327/10000 (93%)\n",
      "\n",
      "Training took 760.0 seconds (or 12.7 minutes)\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 10\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-03 *\n",
      "       [ 5.2599])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing a to the screen we see that it is indeed modified during training (initial value: a = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues:\n",
    " - find nice hyperparameters (learning rate, dropout, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4.2 Application of Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we sketch the network architecure in order to determine the number of neurons in the last layer. For this purpose we have to take into account the following:<br>\n",
    "<ul>\n",
    "<li>Convolving an (n x n) image with a (5 x 5) filter without padding produces an output of dimension (n-4 x n-4) since we lose 2 pixels on each side of the image.</li>\n",
    "<li>Applying a (2 x 2) max pooling layer on  a (n x n) image gives an (n/2 x n/2) output.</li>\n",
    "<li>Convolving an (n x n) image with a (2 x 2) filter witout padding produces an output of dimension (n-1 x n-1).</li>\n",
    "<li>In the last step, we apply a (2 x 2) pooling operation on a (3 x 3) image where the dimensions of filter and image do not really match. Nevertheless we obtain a (1 x 1) output, but maybe, it would be more convenient to apply a (3 x 3) filter in the previous layer such that we could apply the (2 x 2) pooling to a (2 x 2) image?</li>\n",
    "</ul>\n",
    "Taking all this into account, we obtain the architecture displayed below and can read off that the last convolutional layer contains (128 x 1 x 1) = 128 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 2546.5, 782.5, -0.5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACHUAAAKvCAYAAADu/WGZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAgIAEdQAAAAAAAAAABPQ/8KjfCGKKCQcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e795f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "architecture = plt.imread('architecture.png')\n",
    "fig, ax = plt.subplots(figsize=(40,12))\n",
    "ax.imshow(architecture)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize one image of the test set, its convolution with 3 filters of the first convolutional layer and the corresponding filter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x111e76be0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBXeMlEQP+sZNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e08940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 11\n",
    "selected_image = testset[index]\n",
    "plt.imshow(selected_image[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 3, 24, 24])\n",
      "torch.Size([32, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "filters_of_interest = torch.tensor([4,5,0])\n",
    "print(selected_image[0].size())\n",
    "feature_maps = conv2d(selected_image[0].unsqueeze(0), w_conv1[filters_of_interest,:,:,:])\n",
    "print(feature_maps.size())\n",
    "feature_maps = feature_maps.detach()  # detach from comp. graph\n",
    "filters = w_conv1.detach()\n",
    "print(filters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAJCCAYAAAA2rjfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngQEAAACM91/otFPTT9Z+vAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e76b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(2,3, figsize=(20,10))\n",
    "for i in range(3):\n",
    "    ax1[0,i].imshow(feature_maps[0,i,:,:], cmap='gray')\n",
    "    ax1[1,i].imshow(filters[filters_of_interest[i]][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upper row we show the feature maps which have been obtained by convolving the input image with the three filters shown in the lower row.<br>\n",
    "We can see that the left filter detects vertical edges wheread the central filter detects horizontal edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For comparison: feature maps and filters with no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAJCCAYAAAA2rjfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFqnCitt8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e6bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "filters_of_interest = torch.tensor([1,2,3])\n",
    "feature_maps = conv2d(selected_image[0].unsqueeze(0), w_conv1[filters_of_interest,:,:,:])\n",
    "feature_maps = feature_maps.detach()  # detach from comp. graph\n",
    "filters = w_conv1.detach()\n",
    "\n",
    "fig1, ax1 = plt.subplots(2,3, figsize=(20,10))\n",
    "for i in range(3):\n",
    "    ax1[0,i].imshow(feature_maps[0,i,:,:], cmap='gray')\n",
    "    ax1[1,i].imshow(filters[filters_of_interest[i]][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different network architecture\n",
    "\n",
    "### 1. add additional convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  1024\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2 = 32, 64\n",
    "pic_in1, pic_in2 = 1, 32 \n",
    "k_x1, k_x2 = 5, 5\n",
    "k_y1, k_y2 = 5, 5\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, p_drop_input)\n",
    "    return Y.size()[1]*Y.size()[2]*Y.size()[3]\n",
    "\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([0.01], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_h2, w_o, 0.8, 0.7)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_h2, w_o, 1., 1.)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 13.7446\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.8366\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.2868\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.6374\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.5670\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.6007\n",
      "\n",
      "Test set: Average loss: 0.0067, Accuracy: 8983/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.4190\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.6261\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.3500\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.2013\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.5761\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.4281\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 9474/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.3171\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.4084\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.2153\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.4661\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.2715\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.7973\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 9623/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.2966\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.1816\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.1604\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.2050\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.0995\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.2186\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 9676/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.1994\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.0950\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.4090\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.0982\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.0651\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.1303\n",
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.0861\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.1328\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.0729\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.1938\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.4371\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.2887\n",
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.3352\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.2176\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.0285\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.1659\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.1811\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.0990\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 9766/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.0986\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.1420\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.5085\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.1130\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.2220\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.4156\n",
      "\n",
      "Test set: Average loss: 0.0016, Accuracy: 9788/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.0742\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.1139\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.0885\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.2431\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.4356\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.0153\n",
      "\n",
      "Test set: Average loss: 0.0015, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.4173\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.1510\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.3108\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.0357\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.0318\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.1899\n",
      "\n",
      "Test set: Average loss: 0.0015, Accuracy: 9796/10000 (97%)\n",
      "\n",
      "Training took 628.6 seconds (or 10.5 minutes)\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 10\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we observe, what we believe to be faster fitting due to less layers. The original CNN should perform better with significantly more than 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

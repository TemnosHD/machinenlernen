{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 50 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset in trainset and testset. The trainset consists of 60000 images, the testset of 10000 imgs.\n",
    "\n",
    "trainset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "testset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classnames = [str(i) for i in range(10)]\n",
    "\n",
    "def imshow(img, title=\"\", cmap = \"Greys_r\"): #convert tensor to image\n",
    "    plt.title(title)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    numpyimg = img.numpy()[0]\n",
    "    plt.imshow(numpyimg, cmap = cmap)\n",
    "    \n",
    "\n",
    "def display_10_images_from_dataset(dataset, class_names):\n",
    "    \"\"\"\n",
    "    plots 10 randomly chosen images from a given dataset\n",
    "    \"\"\"\n",
    "    display = [] #holds tuples of image and respective label\n",
    "    for _ in range(10):\n",
    "        index = np.random.randint(0,len(dataset)+1)\n",
    "        display.append(dataset[index])\n",
    "    \n",
    "    nr = 1\n",
    "    fig, axes = plt.subplots(2,5,sharex='col',sharey='row', figsize = (14,9))\n",
    "    for image, label in display:\n",
    "        axes[(nr-1)//5][(nr-1)%5] = plt.subplot(2,5,nr)\n",
    "        plt.title(class_names[label], fontsize = 16)\n",
    "        imshow(image, title = class_names[label])\n",
    "        nr+=1\n",
    "    fig.subplots_adjust(hspace=-0.4)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[0:5]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in fig.axes[1:5]+fig.axes[6:]], visible=False)\n",
    "    plt.show()\n",
    "    plt.savefig(\"previewMNIST.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAFtCAYAAADccl8mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe13058e208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0cc1f9cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_10_images_from_dataset(testset, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4616\n",
      "Loss: 2.2623\n",
      "Loss: 2.2687\n",
      "Loss: 2.3062\n",
      "Loss: 1.9459\n",
      "Loss: 1.8585\n",
      "Loss: 1.7999\n",
      "Loss: 1.8672\n",
      "Loss: 1.5682\n",
      "Loss: 1.6160\n",
      "Loss: 1.5095\n",
      "Loss: 1.4167\n",
      "Loss: 1.3357\n",
      "Loss: 1.2428\n",
      "Loss: 1.2929\n",
      "Loss: 1.2513\n",
      "Loss: 1.0737\n",
      "Loss: 1.0431\n",
      "Loss: 0.9244\n",
      "Loss: 0.8819\n",
      "Loss: 0.8752\n",
      "Loss: 0.9034\n",
      "Loss: 0.9561\n",
      "Loss: 0.9420\n",
      "Loss: 0.6420\n",
      "Loss: 0.8193\n",
      "Loss: 0.9044\n",
      "Loss: 0.8192\n",
      "Loss: 0.6635\n",
      "Loss: 0.9786\n",
      "Loss: 0.8285\n",
      "Loss: 0.8810\n",
      "Loss: 0.7102\n",
      "Loss: 0.8722\n",
      "Loss: 0.7622\n",
      "Loss: 0.9330\n",
      "Loss: 0.8628\n",
      "Loss: 0.5893\n",
      "Loss: 0.8791\n",
      "Loss: 0.8498\n",
      "Loss: 0.6238\n",
      "Loss: 0.8118\n",
      "Loss: 0.6374\n",
      "Loss: 0.6403\n",
      "Loss: 0.5777\n",
      "Loss: 0.6154\n",
      "Loss: 0.8124\n",
      "Loss: 0.7551\n",
      "Loss: 0.4743\n",
      "Loss: 0.9171\n",
      "Loss: 0.6680\n",
      "Loss: 0.4804\n",
      "Loss: 0.4202\n",
      "Loss: 0.7785\n",
      "Loss: 0.7798\n",
      "Loss: 0.6439\n",
      "Loss: 0.7925\n",
      "Loss: 0.5243\n",
      "Loss: 0.8232\n",
      "Loss: 1.1416\n",
      "Loss: 0.9142\n",
      "Loss: 0.5445\n",
      "Loss: 0.5664\n",
      "Loss: 0.5694\n",
      "Loss: 0.7296\n",
      "Loss: 0.7483\n",
      "Loss: 0.6519\n",
      "Loss: 0.6084\n",
      "Loss: 0.5755\n",
      "Loss: 0.6159\n",
      "Loss: 0.7813\n",
      "Loss: 0.8101\n",
      "Loss: 0.8068\n",
      "Loss: 1.1041\n",
      "Loss: 0.6524\n",
      "Loss: 0.7702\n",
      "Loss: 0.5993\n",
      "Loss: 0.5065\n",
      "Loss: 0.6113\n",
      "Loss: 0.6083\n",
      "Loss: 0.7676\n",
      "Loss: 0.5566\n",
      "Loss: 0.8223\n",
      "Loss: 0.8087\n",
      "Loss: 0.4615\n",
      "Loss: 0.7688\n",
      "Loss: 0.8530\n",
      "Loss: 0.6136\n",
      "Loss: 0.3724\n",
      "Loss: 0.5497\n",
      "Loss: 0.6407\n",
      "Loss: 0.4824\n",
      "Loss: 0.5681\n",
      "Loss: 0.4612\n",
      "Loss: 0.4049\n",
      "Loss: 0.5048\n",
      "Loss: 0.5059\n",
      "Loss: 0.3631\n",
      "Loss: 0.4935\n",
      "Loss: 0.4644\n",
      "Loss: 0.4627\n",
      "Loss: 0.5506\n",
      "Loss: 0.3552\n",
      "Loss: 0.3457\n",
      "Loss: 0.4223\n",
      "Loss: 0.3638\n",
      "Loss: 0.4555\n",
      "Loss: 0.5683\n",
      "Loss: 0.5084\n",
      "Loss: 0.6139\n",
      "Loss: 0.3561\n",
      "Loss: 0.3619\n",
      "Loss: 0.5608\n",
      "Loss: 0.4616\n",
      "Loss: 0.3960\n",
      "Loss: 0.5241\n",
      "Loss: 0.6519\n",
      "Loss: 0.4764\n",
      "Loss: 0.4469\n",
      "Loss: 0.3596\n",
      "Loss: 0.5653\n",
      "Loss: 0.5156\n",
      "Loss: 0.7987\n",
      "Loss: 0.6775\n",
      "Loss: 0.8012\n",
      "Loss: 0.4932\n",
      "Loss: 0.5695\n",
      "Loss: 0.5412\n",
      "Loss: 0.3161\n",
      "Loss: 0.4107\n",
      "Loss: 0.3143\n",
      "Loss: 0.6117\n",
      "Loss: 0.5519\n",
      "Loss: 0.4310\n",
      "Loss: 0.4528\n",
      "Loss: 0.6208\n",
      "Loss: 0.6337\n",
      "Loss: 0.5806\n",
      "Loss: 0.6459\n",
      "Loss: 0.5501\n",
      "Loss: 1.0889\n",
      "Loss: 0.6188\n",
      "Loss: 0.3678\n",
      "Loss: 0.6154\n",
      "Loss: 0.5195\n",
      "Loss: 0.6008\n",
      "Loss: 0.5994\n",
      "Loss: 0.4818\n",
      "Loss: 0.3397\n",
      "Loss: 0.5009\n",
      "Loss: 0.5956\n",
      "Loss: 0.6631\n",
      "Loss: 0.3033\n",
      "Loss: 0.3443\n",
      "Loss: 0.3661\n",
      "Loss: 0.5735\n",
      "Loss: 0.8253\n",
      "Loss: 0.5399\n",
      "Loss: 0.2460\n",
      "Loss: 0.6103\n",
      "Loss: 0.5278\n",
      "Loss: 0.3016\n",
      "Loss: 0.7616\n",
      "Loss: 0.3581\n",
      "Loss: 0.6241\n",
      "Loss: 0.6071\n",
      "Loss: 0.2660\n",
      "Loss: 0.5084\n",
      "Loss: 0.6929\n",
      "Loss: 0.6686\n",
      "Loss: 0.5319\n",
      "Loss: 0.5198\n",
      "Loss: 0.4971\n",
      "Loss: 0.3130\n",
      "Loss: 0.3810\n",
      "Loss: 0.4776\n",
      "Loss: 0.3751\n",
      "Loss: 0.5262\n",
      "Loss: 0.5847\n",
      "Loss: 0.4147\n",
      "Loss: 1.0081\n",
      "Loss: 0.6562\n",
      "Loss: 0.7714\n",
      "Loss: 0.4018\n",
      "Loss: 0.3387\n",
      "Loss: 0.5060\n",
      "Loss: 0.6864\n",
      "Loss: 0.3463\n",
      "Loss: 0.9258\n",
      "Loss: 0.5689\n",
      "Loss: 0.5029\n",
      "Loss: 0.2791\n",
      "Loss: 0.5064\n",
      "Loss: 0.7162\n",
      "Loss: 0.9503\n",
      "Loss: 0.7706\n",
      "Loss: 0.5334\n",
      "Loss: 0.7142\n",
      "Loss: 0.5468\n",
      "Loss: 0.4680\n",
      "Loss: 0.5346\n",
      "Loss: 0.3577\n",
      "Loss: 0.4539\n",
      "Loss: 0.5988\n",
      "Loss: 0.8486\n",
      "Loss: 0.5251\n",
      "Loss: 0.3513\n",
      "Loss: 0.8071\n",
      "Loss: 0.3385\n",
      "Loss: 0.3458\n",
      "Loss: 0.7672\n",
      "Loss: 0.5986\n",
      "Loss: 0.3748\n",
      "Loss: 0.5346\n",
      "Loss: 0.6651\n",
      "Loss: 0.7060\n",
      "Loss: 0.3089\n",
      "Loss: 0.3123\n",
      "Loss: 0.4823\n",
      "Loss: 0.5656\n",
      "Loss: 0.5826\n",
      "Loss: 0.6617\n",
      "Loss: 0.7189\n",
      "Loss: 0.5287\n",
      "Loss: 1.0457\n",
      "Loss: 0.5443\n",
      "Loss: 0.5592\n",
      "Loss: 0.4268\n",
      "Loss: 0.5496\n",
      "Loss: 0.8281\n",
      "Loss: 0.2701\n",
      "Loss: 0.4742\n",
      "Loss: 0.7734\n",
      "Loss: 0.5355\n",
      "Loss: 0.5746\n",
      "Loss: 0.5162\n",
      "Loss: 0.5794\n",
      "Loss: 0.4154\n",
      "Loss: 0.5395\n",
      "Loss: 0.4341\n",
      "Loss: 0.3951\n",
      "Loss: 0.7457\n",
      "Loss: 0.5920\n",
      "Loss: 0.5922\n",
      "Loss: 0.5333\n",
      "Loss: 0.3329\n",
      "Loss: 0.9013\n",
      "Loss: 0.3802\n",
      "Loss: 0.5442\n",
      "Loss: 0.4862\n",
      "Loss: 0.4633\n",
      "Loss: 0.5156\n",
      "Loss: 0.7939\n",
      "Loss: 0.7227\n",
      "Loss: 0.4730\n",
      "Loss: 0.3706\n",
      "Loss: 0.6891\n",
      "Loss: 0.4992\n",
      "Loss: 0.5920\n",
      "Loss: 0.4109\n",
      "Loss: 0.5846\n",
      "Loss: 0.4111\n",
      "Loss: 0.4905\n",
      "Loss: 0.4662\n",
      "Loss: 0.4490\n",
      "Loss: 0.3806\n",
      "Loss: 0.3043\n",
      "Loss: 0.2153\n",
      "Loss: 0.4967\n",
      "Loss: 0.6346\n",
      "Loss: 0.4570\n",
      "Loss: 0.6542\n",
      "Loss: 0.4453\n",
      "Loss: 0.4115\n",
      "Loss: 0.3210\n",
      "Loss: 0.3969\n",
      "Loss: 0.4671\n",
      "Loss: 0.4463\n",
      "Loss: 0.2722\n",
      "Loss: 0.6704\n",
      "Loss: 0.5603\n",
      "Loss: 0.4002\n",
      "Loss: 0.6814\n",
      "Loss: 0.3246\n",
      "Loss: 0.3658\n",
      "Loss: 0.4499\n",
      "Loss: 0.6866\n",
      "Loss: 0.5176\n",
      "Loss: 0.5008\n",
      "Loss: 0.3147\n",
      "Loss: 0.5580\n",
      "Loss: 0.5748\n",
      "Loss: 0.3781\n",
      "Loss: 0.2691\n",
      "Loss: 0.5608\n",
      "Loss: 0.4585\n",
      "Loss: 0.6194\n",
      "Loss: 0.3689\n",
      "Loss: 0.4253\n",
      "Loss: 0.3593\n",
      "Loss: 0.6162\n",
      "Loss: 0.3170\n",
      "Loss: 0.5060\n",
      "Loss: 0.4654\n",
      "Loss: 0.4376\n",
      "Loss: 0.8769\n",
      "Loss: 0.3793\n",
      "Loss: 0.3859\n",
      "Loss: 0.5717\n",
      "Loss: 0.3544\n",
      "Loss: 0.6459\n",
      "Loss: 0.4938\n",
      "Loss: 0.2960\n",
      "Loss: 0.4165\n",
      "Loss: 0.3877\n",
      "Loss: 0.3235\n",
      "Loss: 0.5462\n",
      "Loss: 0.5003\n",
      "Loss: 0.3738\n",
      "Loss: 0.4763\n",
      "Loss: 0.3834\n",
      "Loss: 0.5366\n",
      "Loss: 0.4890\n",
      "Loss: 0.5048\n",
      "Loss: 0.3665\n",
      "Loss: 0.5615\n",
      "Loss: 0.4951\n",
      "Loss: 0.3332\n",
      "Loss: 0.5571\n",
      "Loss: 0.3276\n",
      "Loss: 0.4268\n",
      "Loss: 0.5432\n",
      "Loss: 0.3775\n",
      "Loss: 0.4099\n",
      "Loss: 0.5498\n",
      "Loss: 0.5800\n",
      "Loss: 0.5582\n",
      "Loss: 0.3626\n",
      "Loss: 0.5827\n",
      "Loss: 0.3552\n",
      "Loss: 0.7910\n",
      "Loss: 0.4607\n",
      "Loss: 0.4372\n",
      "Loss: 0.2572\n",
      "Loss: 0.6701\n",
      "Loss: 0.5336\n",
      "Loss: 0.6299\n",
      "Loss: 0.2954\n",
      "Loss: 0.4850\n",
      "Loss: 0.4365\n",
      "Loss: 0.3882\n",
      "Loss: 0.4287\n",
      "Loss: 0.5315\n",
      "Loss: 0.6359\n",
      "Loss: 0.4263\n",
      "Loss: 0.4065\n",
      "Loss: 0.3427\n",
      "Loss: 0.5349\n",
      "Loss: 0.3398\n",
      "Loss: 0.4635\n",
      "Loss: 0.5358\n",
      "Loss: 0.4481\n",
      "Loss: 0.3800\n",
      "Loss: 0.5976\n",
      "Loss: 0.4899\n",
      "Loss: 0.4744\n",
      "Loss: 0.3063\n",
      "Loss: 0.5163\n",
      "Loss: 0.4555\n",
      "Loss: 0.5230\n",
      "Loss: 0.3794\n",
      "Loss: 0.7661\n",
      "Loss: 0.4034\n",
      "Loss: 0.4500\n",
      "Loss: 0.4440\n",
      "Loss: 0.4394\n",
      "Loss: 0.4078\n",
      "Loss: 0.5002\n",
      "Loss: 0.3511\n",
      "Loss: 0.4337\n",
      "Loss: 0.2991\n",
      "Loss: 0.7771\n",
      "Loss: 0.3169\n",
      "Loss: 0.2788\n",
      "Loss: 0.7220\n",
      "Loss: 0.2496\n",
      "Loss: 0.7759\n",
      "Loss: 0.4751\n",
      "Loss: 0.4411\n",
      "Loss: 0.4566\n",
      "Loss: 0.4143\n",
      "Loss: 0.4321\n",
      "Loss: 0.4727\n",
      "Loss: 0.5029\n",
      "Loss: 0.5534\n",
      "Loss: 0.3229\n",
      "Loss: 0.5087\n",
      "Loss: 0.4032\n",
      "Loss: 0.3273\n",
      "Loss: 0.3290\n",
      "Loss: 0.3601\n",
      "Loss: 0.4091\n",
      "Loss: 0.3301\n",
      "Loss: 0.5504\n",
      "Loss: 0.3214\n",
      "Loss: 0.4117\n",
      "Loss: 0.4683\n",
      "Loss: 0.5451\n",
      "Loss: 0.5662\n",
      "Loss: 0.4219\n",
      "Loss: 0.4620\n",
      "Loss: 0.4889\n",
      "Loss: 0.4099\n",
      "Loss: 0.3905\n",
      "Loss: 0.2928\n",
      "Loss: 0.2827\n",
      "Loss: 0.4515\n",
      "Loss: 0.3770\n",
      "Loss: 0.7293\n",
      "Loss: 0.6723\n",
      "Loss: 0.5357\n",
      "Loss: 0.4687\n",
      "Loss: 0.7209\n",
      "Loss: 0.4211\n",
      "Loss: 0.5916\n",
      "Loss: 0.3764\n",
      "Loss: 0.6307\n",
      "Loss: 0.4605\n",
      "Loss: 0.5263\n",
      "Loss: 0.3403\n",
      "Loss: 0.5405\n",
      "Loss: 0.2212\n",
      "Loss: 0.2955\n",
      "Loss: 0.5630\n",
      "Loss: 0.5846\n",
      "Loss: 0.4816\n",
      "Loss: 0.8059\n",
      "Loss: 0.4175\n",
      "Loss: 0.4718\n",
      "Loss: 0.4672\n",
      "Loss: 0.3800\n",
      "Loss: 0.5999\n",
      "Loss: 0.5009\n",
      "Loss: 0.4884\n",
      "Loss: 0.4238\n",
      "Loss: 0.3324\n",
      "Loss: 0.2835\n",
      "Loss: 0.2854\n",
      "Loss: 0.3697\n",
      "Loss: 0.2945\n",
      "Loss: 0.4416\n",
      "Loss: 0.3408\n",
      "Loss: 0.2571\n",
      "Loss: 0.3184\n",
      "Loss: 0.5421\n",
      "Loss: 0.3616\n",
      "Loss: 0.4156\n",
      "Loss: 0.3264\n",
      "Loss: 0.5469\n",
      "Loss: 0.3640\n",
      "Loss: 0.1283\n",
      "Loss: 0.2844\n",
      "Loss: 0.3389\n",
      "Loss: 0.6840\n",
      "Loss: 0.3857\n",
      "Loss: 0.3060\n",
      "Loss: 0.4610\n",
      "Loss: 0.4309\n",
      "Loss: 0.3358\n",
      "Loss: 0.3594\n",
      "Loss: 0.2930\n",
      "Loss: 0.2050\n",
      "Loss: 0.6327\n",
      "Loss: 0.3841\n",
      "Loss: 0.3991\n",
      "Loss: 0.6249\n",
      "Loss: 0.7920\n",
      "Loss: 0.8226\n",
      "Loss: 0.2881\n",
      "Loss: 0.4108\n",
      "Loss: 0.2964\n",
      "Loss: 0.4324\n",
      "Loss: 0.7763\n",
      "Loss: 0.5533\n",
      "Loss: 0.5627\n",
      "Loss: 0.2879\n",
      "Loss: 0.3243\n",
      "Loss: 0.3827\n",
      "Loss: 0.2166\n",
      "Loss: 0.2598\n",
      "Loss: 0.3612\n",
      "Loss: 0.4857\n",
      "Loss: 0.6551\n",
      "Loss: 0.3499\n",
      "Loss: 0.3770\n",
      "Loss: 0.4011\n",
      "Loss: 0.3225\n",
      "Loss: 0.6388\n",
      "Loss: 0.2935\n",
      "Loss: 0.2533\n",
      "Loss: 0.5639\n",
      "Loss: 0.6246\n",
      "Loss: 0.5892\n",
      "Loss: 0.4082\n",
      "Loss: 0.7108\n",
      "Loss: 0.4996\n",
      "Loss: 0.4141\n",
      "Loss: 0.3437\n",
      "Loss: 0.3003\n",
      "Loss: 0.2630\n",
      "Loss: 0.2989\n",
      "Loss: 0.4879\n",
      "Loss: 0.4748\n",
      "Loss: 0.2661\n",
      "Loss: 0.2179\n",
      "Loss: 0.5489\n",
      "Loss: 0.6702\n",
      "Loss: 0.4912\n",
      "Loss: 0.5852\n",
      "Loss: 0.5248\n",
      "Loss: 0.4798\n",
      "Loss: 0.3958\n",
      "Loss: 0.4397\n",
      "Loss: 0.2556\n",
      "Loss: 0.6624\n",
      "Loss: 0.3231\n",
      "Loss: 0.3485\n",
      "Loss: 0.4423\n",
      "Loss: 0.3493\n",
      "Loss: 0.3847\n",
      "Loss: 0.5707\n",
      "Loss: 0.4374\n",
      "Loss: 0.6499\n",
      "Loss: 0.4231\n",
      "Loss: 0.3916\n",
      "Loss: 0.2718\n",
      "Loss: 0.3710\n",
      "Loss: 0.3987\n",
      "Loss: 0.6048\n",
      "Loss: 0.2581\n",
      "Loss: 0.3456\n",
      "Loss: 0.3289\n",
      "Loss: 0.3561\n",
      "Loss: 0.3654\n",
      "Loss: 0.5201\n",
      "Loss: 0.2678\n",
      "Loss: 0.3805\n",
      "Loss: 0.1776\n",
      "Loss: 0.5063\n",
      "Loss: 0.3214\n",
      "Loss: 0.3035\n",
      "Loss: 0.4356\n",
      "Loss: 0.6392\n",
      "Loss: 0.5581\n",
      "Loss: 0.4488\n",
      "Loss: 0.3308\n",
      "Loss: 0.5436\n",
      "Loss: 0.4916\n",
      "Loss: 0.8371\n",
      "Loss: 0.4399\n",
      "Loss: 0.7166\n",
      "Loss: 0.3481\n",
      "Loss: 0.5592\n",
      "Loss: 0.1957\n",
      "Loss: 0.6562\n",
      "Loss: 0.2271\n",
      "Loss: 0.3201\n",
      "Loss: 0.4854\n",
      "Loss: 0.5390\n",
      "Loss: 0.3106\n",
      "Loss: 0.6640\n",
      "Loss: 0.4056\n",
      "Loss: 0.4467\n",
      "Loss: 0.4425\n",
      "Loss: 0.3170\n",
      "Loss: 0.4059\n",
      "Loss: 0.2320\n",
      "Loss: 0.3860\n",
      "Loss: 0.4788\n",
      "Loss: 0.4850\n",
      "Loss: 0.3389\n",
      "Loss: 0.4528\n",
      "Loss: 0.4506\n",
      "Loss: 0.4980\n",
      "Loss: 0.3779\n",
      "Loss: 0.7441\n",
      "Loss: 0.6545\n",
      "Loss: 0.3650\n",
      "Loss: 0.2083\n",
      "Loss: 0.5279\n",
      "Loss: 0.5429\n",
      "Loss: 0.2747\n",
      "Loss: 0.5620\n",
      "Loss: 0.4168\n",
      "Loss: 0.4494\n",
      "Loss: 0.4217\n",
      "Loss: 0.2789\n",
      "Loss: 0.3679\n",
      "Loss: 0.4033\n",
      "Loss: 0.3569\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print('Loss: %.4f' % cost)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.235685110092163\n",
      "Loss: 2.6122305393218994\n",
      "Loss: 2.633882761001587\n",
      "Loss: 2.463536262512207\n",
      "Loss: 2.253995418548584\n",
      "Loss: 2.227691173553467\n",
      "Loss: 2.2037668228149414\n",
      "Loss: 2.2376036643981934\n",
      "Loss: 2.2737526893615723\n",
      "Loss: 2.1525075435638428\n",
      "Loss: 2.1420462131500244\n",
      "Loss: 2.104735851287842\n",
      "Loss: 2.2754225730895996\n",
      "Loss: 2.1633718013763428\n",
      "Loss: 2.124685525894165\n",
      "Loss: 2.085428237915039\n",
      "Loss: 2.080772876739502\n",
      "Loss: 1.9635969400405884\n",
      "Loss: 1.9688693284988403\n",
      "Loss: 2.071608543395996\n",
      "Loss: 1.9973366260528564\n",
      "Loss: 1.8948277235031128\n",
      "Loss: 1.8804954290390015\n",
      "Loss: 1.899318814277649\n",
      "Loss: 1.8611576557159424\n",
      "Loss: 1.9064974784851074\n",
      "Loss: 1.904478907585144\n",
      "Loss: 1.9299267530441284\n",
      "Loss: 1.7353285551071167\n",
      "Loss: 1.746910572052002\n",
      "Loss: 1.9866044521331787\n",
      "Loss: 1.872605323791504\n",
      "Loss: 1.9907395839691162\n",
      "Loss: 1.8297711610794067\n",
      "Loss: 1.760430932044983\n",
      "Loss: 1.9318454265594482\n",
      "Loss: 1.8175140619277954\n",
      "Loss: 1.8376332521438599\n",
      "Loss: 1.6989854574203491\n",
      "Loss: 1.582572340965271\n",
      "Loss: 1.8445755243301392\n",
      "Loss: 1.7279452085494995\n",
      "Loss: 1.8538498878479004\n",
      "Loss: 1.6030325889587402\n",
      "Loss: 1.6972392797470093\n",
      "Loss: 1.833166480064392\n",
      "Loss: 1.6762057542800903\n",
      "Loss: 1.7540795803070068\n",
      "Loss: 1.7157599925994873\n",
      "Loss: 1.5768641233444214\n",
      "Loss: 1.4103089570999146\n",
      "Loss: 1.5442253351211548\n",
      "Loss: 1.577795386314392\n",
      "Loss: 1.6203781366348267\n",
      "Loss: 1.6437221765518188\n",
      "Loss: 1.670914888381958\n",
      "Loss: 1.6315300464630127\n",
      "Loss: 1.7126332521438599\n",
      "Loss: 1.8169559240341187\n",
      "Loss: 1.4232901334762573\n",
      "Loss: 1.5889582633972168\n",
      "Loss: 1.4107666015625\n",
      "Loss: 1.5896573066711426\n",
      "Loss: 1.4864424467086792\n",
      "Loss: 1.431215524673462\n",
      "Loss: 1.6170637607574463\n",
      "Loss: 1.5263373851776123\n",
      "Loss: 1.5201152563095093\n",
      "Loss: 1.66840398311615\n",
      "Loss: 1.4270063638687134\n",
      "Loss: 1.4221171140670776\n",
      "Loss: 1.619748830795288\n",
      "Loss: 1.5797163248062134\n",
      "Loss: 1.518632411956787\n",
      "Loss: 1.5847445726394653\n",
      "Loss: 1.669315218925476\n",
      "Loss: 1.4403001070022583\n",
      "Loss: 1.5390825271606445\n",
      "Loss: 1.648163914680481\n",
      "Loss: 1.5826245546340942\n",
      "Loss: 1.3668900728225708\n",
      "Loss: 1.497296929359436\n",
      "Loss: 1.6096783876419067\n",
      "Loss: 1.7891534566879272\n",
      "Loss: 1.590016484260559\n",
      "Loss: 1.3957737684249878\n",
      "Loss: 1.5452258586883545\n",
      "Loss: 1.5920830965042114\n",
      "Loss: 1.62259840965271\n",
      "Loss: 1.413475751876831\n",
      "Loss: 1.4523564577102661\n",
      "Loss: 1.3758866786956787\n",
      "Loss: 1.5771470069885254\n",
      "Loss: 1.4463021755218506\n",
      "Loss: 1.3915131092071533\n",
      "Loss: 1.313422679901123\n",
      "Loss: 1.5220309495925903\n",
      "Loss: 1.5391067266464233\n",
      "Loss: 1.5877798795700073\n",
      "Loss: 1.497165322303772\n",
      "Loss: 1.3383429050445557\n",
      "Loss: 1.379634976387024\n",
      "Loss: 1.3977731466293335\n",
      "Loss: 1.4110832214355469\n",
      "Loss: 1.4959138631820679\n",
      "Loss: 1.4388912916183472\n",
      "Loss: 1.1972342729568481\n",
      "Loss: 1.4660602807998657\n",
      "Loss: 1.4974255561828613\n",
      "Loss: 1.2635573148727417\n",
      "Loss: 1.6518816947937012\n",
      "Loss: 1.2052518129348755\n",
      "Loss: 1.3269137144088745\n",
      "Loss: 1.4603450298309326\n",
      "Loss: 1.7873073816299438\n",
      "Loss: 1.3309401273727417\n",
      "Loss: 1.2543457746505737\n",
      "Loss: 1.335268497467041\n",
      "Loss: 1.3518387079238892\n",
      "Loss: 1.397591233253479\n",
      "Loss: 1.6119184494018555\n",
      "Loss: 1.6606438159942627\n",
      "Loss: 1.3041276931762695\n",
      "Loss: 1.3731377124786377\n",
      "Loss: 1.409706711769104\n",
      "Loss: 1.4399464130401611\n",
      "Loss: 1.3342857360839844\n",
      "Loss: 1.457720160484314\n",
      "Loss: 1.2296181917190552\n",
      "Loss: 1.4223198890686035\n",
      "Loss: 1.383384108543396\n",
      "Loss: 1.2598140239715576\n",
      "Loss: 1.2187786102294922\n",
      "Loss: 1.2607779502868652\n",
      "Loss: 1.0820035934448242\n",
      "Loss: 1.6282519102096558\n",
      "Loss: 1.3300622701644897\n",
      "Loss: 1.098014235496521\n",
      "Loss: 1.4603869915008545\n",
      "Loss: 1.431526780128479\n",
      "Loss: 1.1109938621520996\n",
      "Loss: 1.2632489204406738\n",
      "Loss: 1.4098953008651733\n",
      "Loss: 1.1122719049453735\n",
      "Loss: 1.2248601913452148\n",
      "Loss: 1.6237895488739014\n",
      "Loss: 1.763085126876831\n",
      "Loss: 1.632572889328003\n",
      "Loss: 1.3014280796051025\n",
      "Loss: 1.3012524843215942\n",
      "Loss: 1.250795841217041\n",
      "Loss: 1.4027079343795776\n",
      "Loss: 1.3468444347381592\n",
      "Loss: 1.5457476377487183\n",
      "Loss: 1.277178168296814\n",
      "Loss: 1.196873664855957\n",
      "Loss: 1.2334718704223633\n",
      "Loss: 1.0899207592010498\n",
      "Loss: 1.5114836692810059\n",
      "Loss: 1.340662956237793\n",
      "Loss: 1.253896951675415\n",
      "Loss: 1.2866239547729492\n",
      "Loss: 1.342124581336975\n",
      "Loss: 1.324912428855896\n",
      "Loss: 1.2508654594421387\n",
      "Loss: 1.2259007692337036\n",
      "Loss: 1.2639505863189697\n",
      "Loss: 1.392605185508728\n",
      "Loss: 1.2682194709777832\n",
      "Loss: 1.2202367782592773\n",
      "Loss: 1.1189669370651245\n",
      "Loss: 1.4634824991226196\n",
      "Loss: 1.1182035207748413\n",
      "Loss: 1.617924451828003\n",
      "Loss: 1.312849998474121\n",
      "Loss: 1.7567520141601562\n",
      "Loss: 1.2642279863357544\n",
      "Loss: 1.0016878843307495\n",
      "Loss: 1.278499722480774\n",
      "Loss: 1.3395122289657593\n",
      "Loss: 1.0473235845565796\n",
      "Loss: 1.303757667541504\n",
      "Loss: 1.0364866256713867\n",
      "Loss: 1.3158003091812134\n",
      "Loss: 1.0334320068359375\n",
      "Loss: 1.3454898595809937\n",
      "Loss: 1.610535740852356\n",
      "Loss: 1.117316484451294\n",
      "Loss: 1.3290739059448242\n",
      "Loss: 1.1589869260787964\n",
      "Loss: 1.2751556634902954\n",
      "Loss: 1.118372917175293\n",
      "Loss: 1.0789942741394043\n",
      "Loss: 1.2052178382873535\n",
      "Loss: 1.25667142868042\n",
      "Loss: 1.087365746498108\n",
      "Loss: 1.26752507686615\n",
      "Loss: 1.3001240491867065\n",
      "Loss: 1.2131319046020508\n",
      "Loss: 1.383064866065979\n",
      "Loss: 1.0269824266433716\n",
      "Loss: 1.267236351966858\n",
      "Loss: 1.2051447629928589\n",
      "Loss: 1.1581027507781982\n",
      "Loss: 1.2143549919128418\n",
      "Loss: 1.4053155183792114\n",
      "Loss: 1.2736320495605469\n",
      "Loss: 1.4560455083847046\n",
      "Loss: 1.3637226819992065\n",
      "Loss: 1.280386209487915\n",
      "Loss: 0.9875839352607727\n",
      "Loss: 1.3149687051773071\n",
      "Loss: 1.1509945392608643\n",
      "Loss: 1.4508002996444702\n",
      "Loss: 1.1669307947158813\n",
      "Loss: 1.1835060119628906\n",
      "Loss: 1.4548437595367432\n",
      "Loss: 1.2214583158493042\n",
      "Loss: 1.1926418542861938\n",
      "Loss: 1.3775122165679932\n",
      "Loss: 1.3205671310424805\n",
      "Loss: 1.2841286659240723\n",
      "Loss: 1.219007968902588\n",
      "Loss: 1.1335276365280151\n",
      "Loss: 1.2119208574295044\n",
      "Loss: 1.2257529497146606\n",
      "Loss: 1.2654953002929688\n",
      "Loss: 1.1596870422363281\n",
      "Loss: 0.9888041615486145\n",
      "Loss: 1.4068117141723633\n",
      "Loss: 1.2808972597122192\n",
      "Loss: 1.1286488771438599\n",
      "Loss: 1.3028886318206787\n",
      "Loss: 1.3703080415725708\n",
      "Loss: 1.261032223701477\n",
      "Loss: 1.4445573091506958\n",
      "Loss: 1.1982226371765137\n",
      "Loss: 1.2251136302947998\n",
      "Loss: 1.4952645301818848\n",
      "Loss: 0.9675335884094238\n",
      "Loss: 1.4960349798202515\n",
      "Loss: 1.3032640218734741\n",
      "Loss: 1.3615292310714722\n",
      "Loss: 1.1314189434051514\n",
      "Loss: 1.0800020694732666\n",
      "Loss: 1.1606308221817017\n",
      "Loss: 1.2875251770019531\n",
      "Loss: 0.9796814918518066\n",
      "Loss: 1.0468345880508423\n",
      "Loss: 1.3220371007919312\n",
      "Loss: 1.2171556949615479\n",
      "Loss: 1.4108604192733765\n",
      "Loss: 1.0667273998260498\n",
      "Loss: 1.2724387645721436\n",
      "Loss: 0.9733000993728638\n",
      "Loss: 1.1657841205596924\n",
      "Loss: 1.2085964679718018\n",
      "Loss: 1.4145327806472778\n",
      "Loss: 1.0316940546035767\n",
      "Loss: 1.2020612955093384\n",
      "Loss: 1.4044190645217896\n",
      "Loss: 1.2166486978530884\n",
      "Loss: 1.3140478134155273\n",
      "Loss: 1.1699028015136719\n",
      "Loss: 1.0640208721160889\n",
      "Loss: 1.1109665632247925\n",
      "Loss: 1.282559871673584\n",
      "Loss: 1.13416588306427\n",
      "Loss: 1.378733515739441\n",
      "Loss: 1.3009631633758545\n",
      "Loss: 1.1336040496826172\n",
      "Loss: 1.117401361465454\n",
      "Loss: 0.9894036054611206\n",
      "Loss: 1.2349382638931274\n",
      "Loss: 1.293837547302246\n",
      "Loss: 1.0066263675689697\n",
      "Loss: 1.144315481185913\n",
      "Loss: 1.2892123460769653\n",
      "Loss: 1.3642756938934326\n",
      "Loss: 1.1503819227218628\n",
      "Loss: 1.238848090171814\n",
      "Loss: 1.2947592735290527\n",
      "Loss: 1.2793198823928833\n",
      "Loss: 1.3867994546890259\n",
      "Loss: 1.1846293210983276\n",
      "Loss: 1.3020211458206177\n",
      "Loss: 1.2613575458526611\n",
      "Loss: 1.4629641771316528\n",
      "Loss: 1.1246036291122437\n",
      "Loss: 1.4205130338668823\n",
      "Loss: 1.4473727941513062\n",
      "Loss: 1.1376562118530273\n",
      "Loss: 1.229251503944397\n",
      "Loss: 1.0496493577957153\n",
      "Loss: 1.1700372695922852\n",
      "Loss: 1.2896748781204224\n",
      "Loss: 1.1218352317810059\n",
      "Loss: 1.2222869396209717\n",
      "Loss: 1.0251193046569824\n",
      "Loss: 1.4456483125686646\n",
      "Loss: 1.1176317930221558\n",
      "Loss: 1.35329270362854\n",
      "Loss: 1.3475489616394043\n",
      "Loss: 1.1959071159362793\n",
      "Loss: 1.2581703662872314\n",
      "Loss: 1.0993905067443848\n",
      "Loss: 0.9449941515922546\n",
      "Loss: 1.2014377117156982\n",
      "Loss: 1.462572455406189\n",
      "Loss: 1.3420929908752441\n",
      "Loss: 1.0339393615722656\n",
      "Loss: 1.534349799156189\n",
      "Loss: 1.1335688829421997\n",
      "Loss: 1.3005404472351074\n",
      "Loss: 1.113824486732483\n",
      "Loss: 1.1736772060394287\n",
      "Loss: 1.0841693878173828\n",
      "Loss: 1.008832573890686\n",
      "Loss: 1.2442015409469604\n",
      "Loss: 1.378468632698059\n",
      "Loss: 1.2424261569976807\n",
      "Loss: 1.1010956764221191\n",
      "Loss: 1.2528785467147827\n",
      "Loss: 1.4006870985031128\n",
      "Loss: 1.3451794385910034\n",
      "Loss: 1.1506346464157104\n",
      "Loss: 1.104378581047058\n",
      "Loss: 1.1331069469451904\n",
      "Loss: 1.5056166648864746\n",
      "Loss: 1.4190624952316284\n",
      "Loss: 1.3183910846710205\n",
      "Loss: 1.2393198013305664\n",
      "Loss: 1.3059238195419312\n",
      "Loss: 1.1074869632720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2563494443893433\n",
      "Loss: 1.403342843055725\n",
      "Loss: 1.1448407173156738\n",
      "Loss: 1.2700246572494507\n",
      "Loss: 1.197383999824524\n",
      "Loss: 1.1524964570999146\n",
      "Loss: 1.1833837032318115\n",
      "Loss: 1.2829405069351196\n",
      "Loss: 1.2575676441192627\n",
      "Loss: 1.3691112995147705\n",
      "Loss: 1.1953160762786865\n",
      "Loss: 0.9843840003013611\n",
      "Loss: 1.049376130104065\n",
      "Loss: 1.2238192558288574\n",
      "Loss: 1.0818769931793213\n",
      "Loss: 1.221265435218811\n",
      "Loss: 1.296981692314148\n",
      "Loss: 1.2105437517166138\n",
      "Loss: 1.3517882823944092\n",
      "Loss: 1.4353647232055664\n",
      "Loss: 1.157196044921875\n",
      "Loss: 1.3252780437469482\n",
      "Loss: 1.3122432231903076\n",
      "Loss: 1.1853066682815552\n",
      "Loss: 1.2519562244415283\n",
      "Loss: 1.3399854898452759\n",
      "Loss: 1.3301687240600586\n",
      "Loss: 1.2446739673614502\n",
      "Loss: 0.9673069715499878\n",
      "Loss: 1.179593563079834\n",
      "Loss: 1.198248028755188\n",
      "Loss: 1.2492250204086304\n",
      "Loss: 1.1838258504867554\n",
      "Loss: 1.1154186725616455\n",
      "Loss: 1.2435503005981445\n",
      "Loss: 1.1733181476593018\n",
      "Loss: 1.2816259860992432\n",
      "Loss: 1.4226664304733276\n",
      "Loss: 1.1587389707565308\n",
      "Loss: 1.1353973150253296\n",
      "Loss: 1.1618744134902954\n",
      "Loss: 1.3599145412445068\n",
      "Loss: 1.1938884258270264\n",
      "Loss: 1.4208449125289917\n",
      "Loss: 1.2440860271453857\n",
      "Loss: 1.2013260126113892\n",
      "Loss: 1.167707085609436\n",
      "Loss: 1.190779685974121\n",
      "Loss: 1.1260404586791992\n",
      "Loss: 1.0278937816619873\n",
      "Loss: 1.303391456604004\n",
      "Loss: 1.3113423585891724\n",
      "Loss: 1.2229841947555542\n",
      "Loss: 1.0783264636993408\n",
      "Loss: 1.2692532539367676\n",
      "Loss: 1.2664270401000977\n",
      "Loss: 1.1705702543258667\n",
      "Loss: 1.1029691696166992\n",
      "Loss: 1.2340068817138672\n",
      "Loss: 1.0087275505065918\n",
      "Loss: 1.2008581161499023\n",
      "Loss: 1.1323069334030151\n",
      "Loss: 1.0741431713104248\n",
      "Loss: 1.0341225862503052\n",
      "Loss: 1.0548293590545654\n",
      "Loss: 1.1281077861785889\n",
      "Loss: 1.0293493270874023\n",
      "Loss: 1.1280714273452759\n",
      "Loss: 1.2192203998565674\n",
      "Loss: 1.3690423965454102\n",
      "Loss: 1.1918632984161377\n",
      "Loss: 1.2687883377075195\n",
      "Loss: 1.3266428709030151\n",
      "Loss: 1.1690545082092285\n",
      "Loss: 1.0636903047561646\n",
      "Loss: 1.2885087728500366\n",
      "Loss: 1.456192970275879\n",
      "Loss: 1.339510440826416\n",
      "Loss: 1.3222934007644653\n",
      "Loss: 1.2157245874404907\n",
      "Loss: 1.2957513332366943\n",
      "Loss: 1.1628409624099731\n",
      "Loss: 0.9602407217025757\n",
      "Loss: 1.3418344259262085\n",
      "Loss: 1.2332618236541748\n",
      "Loss: 1.0941405296325684\n",
      "Loss: 1.273746132850647\n",
      "Loss: 1.234391450881958\n",
      "Loss: 1.2861526012420654\n",
      "Loss: 1.3526252508163452\n",
      "Loss: 1.0530059337615967\n",
      "Loss: 1.195702075958252\n",
      "Loss: 1.2838160991668701\n",
      "Loss: 1.321865200996399\n",
      "Loss: 1.2850500345230103\n",
      "Loss: 1.1863939762115479\n",
      "Loss: 1.2922964096069336\n",
      "Loss: 0.9848041534423828\n",
      "Loss: 1.1725280284881592\n",
      "Loss: 1.1318854093551636\n",
      "Loss: 1.270263433456421\n",
      "Loss: 1.2737932205200195\n",
      "Loss: 1.2582436800003052\n",
      "Loss: 1.2190213203430176\n",
      "Loss: 1.478855848312378\n",
      "Loss: 1.263303518295288\n",
      "Loss: 1.0482492446899414\n",
      "Loss: 1.1727783679962158\n",
      "Loss: 1.2216111421585083\n",
      "Loss: 0.8183339834213257\n",
      "Loss: 1.4877914190292358\n",
      "Loss: 1.1857587099075317\n",
      "Loss: 0.9796085357666016\n",
      "Loss: 1.1918102502822876\n",
      "Loss: 1.1566601991653442\n",
      "Loss: 1.1214625835418701\n",
      "Loss: 1.2897753715515137\n",
      "Loss: 1.393506646156311\n",
      "Loss: 1.1876639127731323\n",
      "Loss: 1.3913214206695557\n",
      "Loss: 1.1665016412734985\n",
      "Loss: 1.1178789138793945\n",
      "Loss: 1.231469988822937\n",
      "Loss: 1.1592686176300049\n",
      "Loss: 1.1318601369857788\n",
      "Loss: 1.157965898513794\n",
      "Loss: 1.1896733045578003\n",
      "Loss: 1.0742554664611816\n",
      "Loss: 1.121546983718872\n",
      "Loss: 1.0540724992752075\n",
      "Loss: 1.296531081199646\n",
      "Loss: 1.1609470844268799\n",
      "Loss: 1.66414213180542\n",
      "Loss: 1.3626017570495605\n",
      "Loss: 1.2246931791305542\n",
      "Loss: 1.3278210163116455\n",
      "Loss: 0.998862087726593\n",
      "Loss: 1.0043548345565796\n",
      "Loss: 1.2588485479354858\n",
      "Loss: 1.3776724338531494\n",
      "Loss: 1.0444905757904053\n",
      "Loss: 1.3926353454589844\n",
      "Loss: 1.283640742301941\n",
      "Loss: 1.3935601711273193\n",
      "Loss: 1.2700749635696411\n",
      "Loss: 1.4010580778121948\n",
      "Loss: 1.2938357591629028\n",
      "Loss: 1.2393853664398193\n",
      "Loss: 1.1966496706008911\n",
      "Loss: 1.2760347127914429\n",
      "Loss: 1.4307186603546143\n",
      "Loss: 1.2587332725524902\n",
      "Loss: 1.5049831867218018\n",
      "Loss: 1.1174768209457397\n",
      "Loss: 1.3139582872390747\n",
      "Loss: 1.2776484489440918\n",
      "Loss: 1.1688232421875\n",
      "Loss: 1.2090564966201782\n",
      "Loss: 1.3832415342330933\n",
      "Loss: 1.4675743579864502\n",
      "Loss: 1.2586028575897217\n",
      "Loss: 1.2986814975738525\n",
      "Loss: 1.0805147886276245\n",
      "Loss: 1.240971326828003\n",
      "Loss: 1.2335131168365479\n",
      "Loss: 1.1076582670211792\n",
      "Loss: 1.2298142910003662\n",
      "Loss: 1.3393030166625977\n",
      "Loss: 1.3650034666061401\n",
      "Loss: 1.1393321752548218\n",
      "Loss: 1.3616055250167847\n",
      "Loss: 1.1057307720184326\n",
      "Loss: 1.307349443435669\n",
      "Loss: 1.2807817459106445\n",
      "Loss: 1.2801460027694702\n",
      "Loss: 1.2532916069030762\n",
      "Loss: 1.2900947332382202\n",
      "Loss: 1.0855380296707153\n",
      "Loss: 1.379518747329712\n",
      "Loss: 1.1467593908309937\n",
      "Loss: 0.9852449893951416\n",
      "Loss: 1.4831522703170776\n",
      "Loss: 1.1293200254440308\n",
      "Loss: 1.4231446981430054\n",
      "Loss: 1.0936334133148193\n",
      "Loss: 1.4027410745620728\n",
      "Loss: 1.1791912317276\n",
      "Loss: 1.1081740856170654\n",
      "Loss: 1.1920348405838013\n",
      "Loss: 1.1149975061416626\n",
      "Loss: 1.2038017511367798\n",
      "Loss: 1.0106333494186401\n",
      "Loss: 1.4339001178741455\n",
      "Loss: 1.1490801572799683\n",
      "Loss: 1.4813343286514282\n",
      "Loss: 1.8436601161956787\n",
      "Loss: 0.992838442325592\n",
      "Loss: 1.2004950046539307\n",
      "Loss: 1.2700743675231934\n",
      "Loss: 1.0762827396392822\n",
      "Loss: 1.3768911361694336\n",
      "Loss: 1.4488389492034912\n",
      "Loss: 1.3022938966751099\n",
      "Loss: 1.3164743185043335\n",
      "Loss: 1.0845494270324707\n",
      "Loss: 1.231898307800293\n",
      "Loss: 1.4370489120483398\n",
      "Loss: 1.406612515449524\n",
      "Loss: 1.1785396337509155\n",
      "Loss: 1.197891354560852\n",
      "Loss: 1.1143451929092407\n",
      "Loss: 1.0594983100891113\n",
      "Loss: 1.1966118812561035\n",
      "Loss: 1.439035177230835\n",
      "Loss: 0.9877829551696777\n",
      "Loss: 1.245133638381958\n",
      "Loss: 1.376039981842041\n",
      "Loss: 1.07768976688385\n",
      "Loss: 1.2206538915634155\n",
      "Loss: 1.2591651678085327\n",
      "Loss: 1.369421124458313\n",
      "Loss: 1.1184288263320923\n",
      "Loss: 1.0475637912750244\n",
      "Loss: 0.930880069732666\n",
      "Loss: 1.2883648872375488\n",
      "Loss: 1.217452883720398\n",
      "Loss: 1.0205053091049194\n",
      "Loss: 1.0486727952957153\n",
      "Loss: 1.3300988674163818\n",
      "Loss: 1.0711673498153687\n",
      "Loss: 1.2951669692993164\n",
      "Loss: 1.0698494911193848\n",
      "Loss: 1.7003238201141357\n",
      "Loss: 1.296053409576416\n",
      "Loss: 1.2219146490097046\n",
      "Loss: 1.202470302581787\n",
      "Loss: 1.3263145685195923\n",
      "Loss: 1.121506929397583\n",
      "Loss: 1.2151283025741577\n",
      "Loss: 1.3277678489685059\n",
      "Loss: 1.4927781820297241\n",
      "Loss: 1.1611453294754028\n",
      "Loss: 1.3179692029953003\n",
      "Loss: 1.4605231285095215\n",
      "Loss: 1.1721575260162354\n",
      "Loss: 1.158571481704712\n",
      "Loss: 1.1376765966415405\n",
      "Loss: 1.0068988800048828\n",
      "Loss: 1.4054572582244873\n",
      "Loss: 1.2447326183319092\n",
      "Loss: 1.4498543739318848\n",
      "Loss: 1.3311924934387207\n",
      "Loss: 1.0564143657684326\n",
      "Loss: 1.1576443910598755\n",
      "Loss: 1.2988070249557495\n",
      "Loss: 1.1543501615524292\n",
      "Loss: 1.1974117755889893\n",
      "Loss: 1.5257419347763062\n",
      "Loss: 1.471794605255127\n",
      "Loss: 1.1776920557022095\n",
      "Loss: 1.3780800104141235\n",
      "Loss: 1.1459885835647583\n",
      "Loss: 1.1982214450836182\n",
      "Loss: 1.676869511604309\n",
      "Loss: 1.2468976974487305\n",
      "Loss: 1.1324021816253662\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 3.4085\n",
      "a: -0.1000\n",
      "step:  1\n",
      "loss: 2.5221\n",
      "a: -0.0968\n",
      "step:  2\n",
      "loss: 2.5012\n",
      "a: -0.0941\n",
      "step:  3\n",
      "loss: 2.6047\n",
      "a: -0.0919\n",
      "step:  4\n",
      "loss: 2.3506\n",
      "a: -0.0891\n",
      "step:  5\n",
      "loss: 2.4400\n",
      "a: -0.0866\n",
      "step:  6\n",
      "loss: 2.3091\n",
      "a: -0.0840\n",
      "step:  7\n",
      "loss: 2.3205\n",
      "a: -0.0816\n",
      "step:  8\n",
      "loss: 2.3555\n",
      "a: -0.0792\n",
      "step:  9\n",
      "loss: 2.3413\n",
      "a: -0.0770\n",
      "step:  10\n",
      "loss: 2.3310\n",
      "a: -0.0750\n",
      "step:  11\n",
      "loss: 2.4262\n",
      "a: -0.0729\n",
      "step:  12\n",
      "loss: 2.2930\n",
      "a: -0.0709\n",
      "step:  13\n",
      "loss: 2.3100\n",
      "a: -0.0690\n",
      "step:  14\n",
      "loss: 2.2143\n",
      "a: -0.0672\n",
      "step:  15\n",
      "loss: 2.2310\n",
      "a: -0.0655\n",
      "step:  16\n",
      "loss: 2.2795\n",
      "a: -0.0639\n",
      "step:  17\n",
      "loss: 2.3911\n",
      "a: -0.0623\n",
      "step:  18\n",
      "loss: 2.2091\n",
      "a: -0.0607\n",
      "step:  19\n",
      "loss: 2.1138\n",
      "a: -0.0591\n",
      "step:  20\n",
      "loss: 2.1172\n",
      "a: -0.0576\n",
      "step:  21\n",
      "loss: 2.2578\n",
      "a: -0.0561\n",
      "step:  22\n",
      "loss: 2.2086\n",
      "a: -0.0546\n",
      "step:  23\n",
      "loss: 2.1716\n",
      "a: -0.0532\n",
      "step:  24\n",
      "loss: 2.0940\n",
      "a: -0.0518\n",
      "step:  25\n",
      "loss: 2.1407\n",
      "a: -0.0503\n",
      "step:  26\n",
      "loss: 2.1106\n",
      "a: -0.0489\n",
      "step:  27\n",
      "loss: 2.0607\n",
      "a: -0.0474\n",
      "step:  28\n",
      "loss: 2.1161\n",
      "a: -0.0460\n",
      "step:  29\n",
      "loss: 1.9676\n",
      "a: -0.0447\n",
      "step:  30\n",
      "loss: 2.1163\n",
      "a: -0.0434\n",
      "step:  31\n",
      "loss: 1.9599\n",
      "a: -0.0420\n",
      "step:  32\n",
      "loss: 2.1428\n",
      "a: -0.0407\n",
      "step:  33\n",
      "loss: 2.0046\n",
      "a: -0.0394\n",
      "step:  34\n",
      "loss: 1.9258\n",
      "a: -0.0381\n",
      "step:  35\n",
      "loss: 1.9724\n",
      "a: -0.0368\n",
      "step:  36\n",
      "loss: 1.8967\n",
      "a: -0.0355\n",
      "step:  37\n",
      "loss: 1.9229\n",
      "a: -0.0342\n",
      "step:  38\n",
      "loss: 1.8829\n",
      "a: -0.0329\n",
      "step:  39\n",
      "loss: 1.8738\n",
      "a: -0.0316\n",
      "step:  40\n",
      "loss: 1.9295\n",
      "a: -0.0304\n",
      "step:  41\n",
      "loss: 1.7957\n",
      "a: -0.0292\n",
      "step:  42\n",
      "loss: 1.8951\n",
      "a: -0.0280\n",
      "step:  43\n",
      "loss: 1.7749\n",
      "a: -0.0268\n",
      "step:  44\n",
      "loss: 1.9031\n",
      "a: -0.0256\n",
      "step:  45\n",
      "loss: 1.7559\n",
      "a: -0.0244\n",
      "step:  46\n",
      "loss: 1.8441\n",
      "a: -0.0233\n",
      "step:  47\n",
      "loss: 1.7691\n",
      "a: -0.0222\n",
      "step:  48\n",
      "loss: 1.6500\n",
      "a: -0.0210\n",
      "step:  49\n",
      "loss: 1.8786\n",
      "a: -0.0198\n",
      "step:  50\n",
      "loss: 1.6281\n",
      "a: -0.0186\n",
      "step:  51\n",
      "loss: 1.9056\n",
      "a: -0.0174\n",
      "step:  52\n",
      "loss: 1.9222\n",
      "a: -0.0162\n",
      "step:  53\n",
      "loss: 1.5522\n",
      "a: -0.0150\n",
      "step:  54\n",
      "loss: 1.7388\n",
      "a: -0.0138\n",
      "step:  55\n",
      "loss: 1.7017\n",
      "a: -0.0127\n",
      "step:  56\n",
      "loss: 1.8135\n",
      "a: -0.0116\n",
      "step:  57\n",
      "loss: 1.7568\n",
      "a: -0.0105\n",
      "step:  58\n",
      "loss: 1.5650\n",
      "a: -0.0095\n",
      "step:  59\n",
      "loss: 1.6894\n",
      "a: -0.0084\n",
      "step:  60\n",
      "loss: 1.7822\n",
      "a: -0.0074\n",
      "step:  61\n",
      "loss: 1.8146\n",
      "a: -0.0064\n",
      "step:  62\n",
      "loss: 1.7770\n",
      "a: -0.0055\n",
      "step:  63\n",
      "loss: 1.7493\n",
      "a: -0.0045\n",
      "step:  64\n",
      "loss: 1.6809\n",
      "a: -0.0035\n",
      "step:  65\n",
      "loss: 1.5123\n",
      "a: -0.0026\n",
      "step:  66\n",
      "loss: 1.4301\n",
      "a: -0.0017\n",
      "step:  67\n",
      "loss: 1.6585\n",
      "a: -0.0008\n",
      "step:  68\n",
      "loss: 1.6485\n",
      "a: -0.0001\n",
      "step:  69\n",
      "loss: 1.8119\n",
      "a: 0.0007\n",
      "step:  70\n",
      "loss: 1.3545\n",
      "a: 0.0014\n",
      "step:  71\n",
      "loss: 1.7212\n",
      "a: 0.0021\n",
      "step:  72\n",
      "loss: 1.7668\n",
      "a: 0.0025\n",
      "step:  73\n",
      "loss: 1.5602\n",
      "a: 0.0028\n",
      "step:  74\n",
      "loss: 1.8980\n",
      "a: 0.0030\n",
      "step:  75\n",
      "loss: 1.6417\n",
      "a: 0.0031\n",
      "step:  76\n",
      "loss: 1.6261\n",
      "a: 0.0029\n",
      "step:  77\n",
      "loss: 1.8146\n",
      "a: 0.0026\n",
      "step:  78\n",
      "loss: 1.9130\n",
      "a: 0.0020\n",
      "step:  79\n",
      "loss: 1.6634\n",
      "a: 0.0011\n",
      "step:  80\n",
      "loss: 1.7140\n",
      "a: 0.0001\n",
      "step:  81\n",
      "loss: 1.6001\n",
      "a: -0.0009\n",
      "step:  82\n",
      "loss: 1.5459\n",
      "a: -0.0019\n",
      "step:  83\n",
      "loss: 1.7669\n",
      "a: -0.0030\n",
      "step:  84\n",
      "loss: 1.6140\n",
      "a: -0.0040\n",
      "step:  85\n",
      "loss: 1.5271\n",
      "a: -0.0051\n",
      "step:  86\n",
      "loss: 1.4989\n",
      "a: -0.0062\n",
      "step:  87\n",
      "loss: 1.5702\n",
      "a: -0.0072\n",
      "step:  88\n",
      "loss: 1.6632\n",
      "a: -0.0082\n",
      "step:  89\n",
      "loss: 1.7257\n",
      "a: -0.0090\n",
      "step:  90\n",
      "loss: 1.8331\n",
      "a: -0.0096\n",
      "step:  91\n",
      "loss: 1.7898\n",
      "a: -0.0101\n",
      "step:  92\n",
      "loss: 1.5673\n",
      "a: -0.0104\n",
      "step:  93\n",
      "loss: 1.6740\n",
      "a: -0.0103\n",
      "step:  94\n",
      "loss: 1.6874\n",
      "a: -0.0100\n",
      "step:  95\n",
      "loss: 1.5068\n",
      "a: -0.0093\n",
      "step:  96\n",
      "loss: 1.7248\n",
      "a: -0.0085\n",
      "step:  97\n",
      "loss: 1.6170\n",
      "a: -0.0075\n",
      "step:  98\n",
      "loss: 1.5882\n",
      "a: -0.0063\n",
      "step:  99\n",
      "loss: 1.5802\n",
      "a: -0.0052\n",
      "step:  100\n",
      "loss: 1.6569\n",
      "a: -0.0041\n",
      "step:  101\n",
      "loss: 1.7392\n",
      "a: -0.0029\n",
      "step:  102\n",
      "loss: 1.5889\n",
      "a: -0.0020\n",
      "step:  103\n",
      "loss: 1.6225\n",
      "a: -0.0010\n",
      "step:  104\n",
      "loss: 1.5066\n",
      "a: -0.0003\n",
      "step:  105\n",
      "loss: 1.8229\n",
      "a: 0.0004\n",
      "step:  106\n",
      "loss: 1.6268\n",
      "a: 0.0010\n",
      "step:  107\n",
      "loss: 1.4473\n",
      "a: 0.0012\n",
      "step:  108\n",
      "loss: 1.5546\n",
      "a: 0.0013\n",
      "step:  109\n",
      "loss: 1.5561\n",
      "a: 0.0010\n",
      "step:  110\n",
      "loss: 1.5924\n",
      "a: 0.0002\n",
      "step:  111\n",
      "loss: 1.2335\n",
      "a: -0.0006\n",
      "step:  112\n",
      "loss: 1.6826\n",
      "a: -0.0015\n",
      "step:  113\n",
      "loss: 1.6981\n",
      "a: -0.0025\n",
      "step:  114\n",
      "loss: 1.3215\n",
      "a: -0.0034\n",
      "step:  115\n",
      "loss: 1.4294\n",
      "a: -0.0038\n",
      "step:  116\n",
      "loss: 1.6396\n",
      "a: -0.0041\n",
      "step:  117\n",
      "loss: 1.7817\n",
      "a: -0.0046\n",
      "step:  118\n",
      "loss: 1.7511\n",
      "a: -0.0051\n",
      "step:  119\n",
      "loss: 1.7832\n",
      "a: -0.0050\n",
      "step:  120\n",
      "loss: 1.6281\n",
      "a: -0.0046\n",
      "step:  121\n",
      "loss: 1.5801\n",
      "a: -0.0036\n",
      "step:  122\n",
      "loss: 1.4008\n",
      "a: -0.0029\n",
      "step:  123\n",
      "loss: 1.4286\n",
      "a: -0.0028\n",
      "step:  124\n",
      "loss: 1.6410\n",
      "a: -0.0028\n",
      "step:  125\n",
      "loss: 1.3077\n",
      "a: -0.0030\n",
      "step:  126\n",
      "loss: 1.6435\n",
      "a: -0.0031\n",
      "step:  127\n",
      "loss: 1.4600\n",
      "a: -0.0035\n",
      "step:  128\n",
      "loss: 1.7056\n",
      "a: -0.0044\n",
      "step:  129\n",
      "loss: 1.7571\n",
      "a: -0.0057\n",
      "step:  130\n",
      "loss: 1.4880\n",
      "a: -0.0061\n",
      "step:  131\n",
      "loss: 1.5708\n",
      "a: -0.0063\n",
      "step:  132\n",
      "loss: 1.5376\n",
      "a: -0.0054\n",
      "step:  133\n",
      "loss: 1.4449\n",
      "a: -0.0041\n",
      "step:  134\n",
      "loss: 1.6267\n",
      "a: -0.0026\n",
      "step:  135\n",
      "loss: 1.4549\n",
      "a: -0.0010\n",
      "step:  136\n",
      "loss: 1.5073\n",
      "a: 0.0003\n",
      "step:  137\n",
      "loss: 1.7053\n",
      "a: 0.0006\n",
      "step:  138\n",
      "loss: 1.4918\n",
      "a: 0.0003\n",
      "step:  139\n",
      "loss: 1.6660\n",
      "a: 0.0000\n",
      "step:  140\n",
      "loss: 1.4423\n",
      "a: 0.0002\n",
      "step:  141\n",
      "loss: 1.5049\n",
      "a: 0.0009\n",
      "step:  142\n",
      "loss: 1.5169\n",
      "a: 0.0013\n",
      "step:  143\n",
      "loss: 1.5914\n",
      "a: 0.0005\n",
      "step:  144\n",
      "loss: 1.5522\n",
      "a: -0.0002\n",
      "step:  145\n",
      "loss: 1.5344\n",
      "a: -0.0012\n",
      "step:  146\n",
      "loss: 1.4068\n",
      "a: -0.0027\n",
      "step:  147\n",
      "loss: 1.6969\n",
      "a: -0.0042\n",
      "step:  148\n",
      "loss: 1.6349\n",
      "a: -0.0054\n",
      "step:  149\n",
      "loss: 1.5064\n",
      "a: -0.0052\n",
      "step:  150\n",
      "loss: 1.4180\n",
      "a: -0.0037\n",
      "step:  151\n",
      "loss: 1.5857\n",
      "a: -0.0020\n",
      "step:  152\n",
      "loss: 1.4785\n",
      "a: -0.0012\n",
      "step:  153\n",
      "loss: 1.4036\n",
      "a: -0.0012\n",
      "step:  154\n",
      "loss: 1.4994\n",
      "a: -0.0025\n",
      "step:  155\n",
      "loss: 1.4950\n",
      "a: -0.0034\n",
      "step:  156\n",
      "loss: 1.4671\n",
      "a: -0.0043\n",
      "step:  157\n",
      "loss: 1.3805\n",
      "a: -0.0048\n",
      "step:  158\n",
      "loss: 1.6333\n",
      "a: -0.0048\n",
      "step:  159\n",
      "loss: 1.6569\n",
      "a: -0.0030\n",
      "step:  160\n",
      "loss: 1.4015\n",
      "a: -0.0014\n",
      "step:  161\n",
      "loss: 1.4531\n",
      "a: -0.0007\n",
      "step:  162\n",
      "loss: 1.5482\n",
      "a: -0.0009\n",
      "step:  163\n",
      "loss: 1.5637\n",
      "a: -0.0024\n",
      "step:  164\n",
      "loss: 1.5310\n",
      "a: -0.0034\n",
      "step:  165\n",
      "loss: 1.5554\n",
      "a: -0.0045\n",
      "step:  166\n",
      "loss: 1.5648\n",
      "a: -0.0047\n",
      "step:  167\n",
      "loss: 1.4782\n",
      "a: -0.0043\n",
      "step:  168\n",
      "loss: 1.4542\n",
      "a: -0.0036\n",
      "step:  169\n",
      "loss: 1.5651\n",
      "a: -0.0028\n",
      "step:  170\n",
      "loss: 1.6215\n",
      "a: -0.0008\n",
      "step:  171\n",
      "loss: 1.9141\n",
      "a: 0.0011\n",
      "step:  172\n",
      "loss: 1.5872\n",
      "a: -0.0013\n",
      "step:  173\n",
      "loss: 1.6786\n",
      "a: -0.0034\n",
      "step:  174\n",
      "loss: 1.4823\n",
      "a: -0.0051\n",
      "step:  175\n",
      "loss: 1.4915\n",
      "a: -0.0064\n",
      "step:  176\n",
      "loss: 1.7009\n",
      "a: -0.0071\n",
      "step:  177\n",
      "loss: 1.7038\n",
      "a: -0.0071\n",
      "step:  178\n",
      "loss: 1.5468\n",
      "a: -0.0060\n",
      "step:  179\n",
      "loss: 1.4559\n",
      "a: -0.0043\n",
      "step:  180\n",
      "loss: 1.4176\n",
      "a: -0.0026\n",
      "step:  181\n",
      "loss: 1.8261\n",
      "a: -0.0012\n",
      "step:  182\n",
      "loss: 1.7024\n",
      "a: -0.0000\n",
      "step:  183\n",
      "loss: 1.4918\n",
      "a: 0.0004\n",
      "step:  184\n",
      "loss: 1.4905\n",
      "a: 0.0003\n",
      "step:  185\n",
      "loss: 1.3890\n",
      "a: -0.0000\n",
      "step:  186\n",
      "loss: 1.4765\n",
      "a: -0.0009\n",
      "step:  187\n",
      "loss: 1.5421\n",
      "a: -0.0017\n",
      "step:  188\n",
      "loss: 1.2771\n",
      "a: -0.0026\n",
      "step:  189\n",
      "loss: 1.4485\n",
      "a: -0.0035\n",
      "step:  190\n",
      "loss: 1.4907\n",
      "a: -0.0041\n",
      "step:  191\n",
      "loss: 1.1742\n",
      "a: -0.0045\n",
      "step:  192\n",
      "loss: 1.6574\n",
      "a: -0.0045\n",
      "step:  193\n",
      "loss: 1.2833\n",
      "a: -0.0037\n",
      "step:  194\n",
      "loss: 1.5668\n",
      "a: -0.0030\n",
      "step:  195\n",
      "loss: 1.5464\n",
      "a: -0.0023\n",
      "step:  196\n",
      "loss: 1.5504\n",
      "a: -0.0014\n",
      "step:  197\n",
      "loss: 1.5030\n",
      "a: -0.0005\n",
      "step:  198\n",
      "loss: 1.6402\n",
      "a: -0.0003\n",
      "step:  199\n",
      "loss: 1.4054\n",
      "a: -0.0010\n",
      "step:  200\n",
      "loss: 1.3490\n",
      "a: -0.0025\n",
      "step:  201\n",
      "loss: 1.5555\n",
      "a: -0.0039\n",
      "step:  202\n",
      "loss: 1.2723\n",
      "a: -0.0054\n",
      "step:  203\n",
      "loss: 1.2722\n",
      "a: -0.0063\n",
      "step:  204\n",
      "loss: 1.5342\n",
      "a: -0.0073\n",
      "step:  205\n",
      "loss: 1.4231\n",
      "a: -0.0076\n",
      "step:  206\n",
      "loss: 1.4120\n",
      "a: -0.0072\n",
      "step:  207\n",
      "loss: 1.7237\n",
      "a: -0.0063\n",
      "step:  208\n",
      "loss: 1.7755\n",
      "a: -0.0051\n",
      "step:  209\n",
      "loss: 1.3786\n",
      "a: -0.0033\n",
      "step:  210\n",
      "loss: 1.5147\n",
      "a: -0.0017\n",
      "step:  211\n",
      "loss: 1.7378\n",
      "a: -0.0004\n",
      "step:  212\n",
      "loss: 1.5177\n",
      "a: 0.0004\n",
      "step:  213\n",
      "loss: 1.4615\n",
      "a: -0.0005\n",
      "step:  214\n",
      "loss: 1.4202\n",
      "a: -0.0017\n",
      "step:  215\n",
      "loss: 1.3972\n",
      "a: -0.0033\n",
      "step:  216\n",
      "loss: 1.3857\n",
      "a: -0.0048\n",
      "step:  217\n",
      "loss: 1.6005\n",
      "a: -0.0062\n",
      "step:  218\n",
      "loss: 1.5054\n",
      "a: -0.0073\n",
      "step:  219\n",
      "loss: 1.8596\n",
      "a: -0.0082\n",
      "step:  220\n",
      "loss: 1.1927\n",
      "a: -0.0086\n",
      "step:  221\n",
      "loss: 1.6698\n",
      "a: -0.0087\n",
      "step:  222\n",
      "loss: 1.6810\n",
      "a: -0.0080\n",
      "step:  223\n",
      "loss: 1.3340\n",
      "a: -0.0071\n",
      "step:  224\n",
      "loss: 1.5324\n",
      "a: -0.0058\n",
      "step:  225\n",
      "loss: 1.3293\n",
      "a: -0.0046\n",
      "step:  226\n",
      "loss: 1.4565\n",
      "a: -0.0033\n",
      "step:  227\n",
      "loss: 1.5513\n",
      "a: -0.0021\n",
      "step:  228\n",
      "loss: 1.6032\n",
      "a: -0.0014\n",
      "step:  229\n",
      "loss: 1.5490\n",
      "a: -0.0013\n",
      "step:  230\n",
      "loss: 1.4824\n",
      "a: -0.0018\n",
      "step:  231\n",
      "loss: 1.5488\n",
      "a: -0.0027\n",
      "step:  232\n",
      "loss: 1.3808\n",
      "a: -0.0035\n",
      "step:  233\n",
      "loss: 1.5430\n",
      "a: -0.0044\n",
      "step:  234\n",
      "loss: 1.4282\n",
      "a: -0.0052\n",
      "step:  235\n",
      "loss: 1.6667\n",
      "a: -0.0056\n",
      "step:  236\n",
      "loss: 1.5187\n",
      "a: -0.0058\n",
      "step:  237\n",
      "loss: 1.3366\n",
      "a: -0.0056\n",
      "step:  238\n",
      "loss: 1.5825\n",
      "a: -0.0054\n",
      "step:  239\n",
      "loss: 1.5864\n",
      "a: -0.0046\n",
      "step:  240\n",
      "loss: 1.6622\n",
      "a: -0.0037\n",
      "step:  241\n",
      "loss: 1.4211\n",
      "a: -0.0033\n",
      "step:  242\n",
      "loss: 1.5797\n",
      "a: -0.0030\n",
      "step:  243\n",
      "loss: 1.6200\n",
      "a: -0.0025\n",
      "step:  244\n",
      "loss: 1.1813\n",
      "a: -0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  245\n",
      "loss: 1.5209\n",
      "a: -0.0017\n",
      "step:  246\n",
      "loss: 1.5846\n",
      "a: -0.0019\n",
      "step:  247\n",
      "loss: 1.3325\n",
      "a: -0.0028\n",
      "step:  248\n",
      "loss: 1.3627\n",
      "a: -0.0039\n",
      "step:  249\n",
      "loss: 1.6347\n",
      "a: -0.0049\n",
      "step:  250\n",
      "loss: 1.4698\n",
      "a: -0.0052\n",
      "step:  251\n",
      "loss: 1.5196\n",
      "a: -0.0057\n",
      "step:  252\n",
      "loss: 1.4727\n",
      "a: -0.0055\n",
      "step:  253\n",
      "loss: 1.4236\n",
      "a: -0.0046\n",
      "step:  254\n",
      "loss: 1.3833\n",
      "a: -0.0031\n",
      "step:  255\n",
      "loss: 1.2631\n",
      "a: -0.0018\n",
      "step:  256\n",
      "loss: 1.5503\n",
      "a: -0.0013\n",
      "step:  257\n",
      "loss: 1.7177\n",
      "a: -0.0021\n",
      "step:  258\n",
      "loss: 1.5474\n",
      "a: -0.0038\n",
      "step:  259\n",
      "loss: 1.6490\n",
      "a: -0.0051\n",
      "step:  260\n",
      "loss: 1.3808\n",
      "a: -0.0059\n",
      "step:  261\n",
      "loss: 1.5072\n",
      "a: -0.0061\n",
      "step:  262\n",
      "loss: 1.4202\n",
      "a: -0.0058\n",
      "step:  263\n",
      "loss: 1.4471\n",
      "a: -0.0049\n",
      "step:  264\n",
      "loss: 1.4630\n",
      "a: -0.0037\n",
      "step:  265\n",
      "loss: 1.2740\n",
      "a: -0.0028\n",
      "step:  266\n",
      "loss: 1.6468\n",
      "a: -0.0021\n",
      "step:  267\n",
      "loss: 1.3712\n",
      "a: -0.0020\n",
      "step:  268\n",
      "loss: 1.4291\n",
      "a: -0.0029\n",
      "step:  269\n",
      "loss: 1.7333\n",
      "a: -0.0041\n",
      "step:  270\n",
      "loss: 1.5332\n",
      "a: -0.0057\n",
      "step:  271\n",
      "loss: 1.3080\n",
      "a: -0.0071\n",
      "step:  272\n",
      "loss: 1.6563\n",
      "a: -0.0082\n",
      "step:  273\n",
      "loss: 1.3753\n",
      "a: -0.0089\n",
      "step:  274\n",
      "loss: 1.5225\n",
      "a: -0.0095\n",
      "step:  275\n",
      "loss: 1.7651\n",
      "a: -0.0097\n",
      "step:  276\n",
      "loss: 1.4318\n",
      "a: -0.0091\n",
      "step:  277\n",
      "loss: 1.5283\n",
      "a: -0.0082\n",
      "step:  278\n",
      "loss: 1.3391\n",
      "a: -0.0069\n",
      "step:  279\n",
      "loss: 1.4175\n",
      "a: -0.0056\n",
      "step:  280\n",
      "loss: 1.2237\n",
      "a: -0.0042\n",
      "step:  281\n",
      "loss: 1.4786\n",
      "a: -0.0030\n",
      "step:  282\n",
      "loss: 1.6658\n",
      "a: -0.0023\n",
      "step:  283\n",
      "loss: 1.6725\n",
      "a: -0.0022\n",
      "step:  284\n",
      "loss: 1.4878\n",
      "a: -0.0030\n",
      "step:  285\n",
      "loss: 1.3781\n",
      "a: -0.0043\n",
      "step:  286\n",
      "loss: 1.3871\n",
      "a: -0.0055\n",
      "step:  287\n",
      "loss: 1.6144\n",
      "a: -0.0066\n",
      "step:  288\n",
      "loss: 1.4349\n",
      "a: -0.0078\n",
      "step:  289\n",
      "loss: 1.6048\n",
      "a: -0.0087\n",
      "step:  290\n",
      "loss: 1.4849\n",
      "a: -0.0093\n",
      "step:  291\n",
      "loss: 1.3999\n",
      "a: -0.0093\n",
      "step:  292\n",
      "loss: 1.5296\n",
      "a: -0.0088\n",
      "step:  293\n",
      "loss: 1.5687\n",
      "a: -0.0079\n",
      "step:  294\n",
      "loss: 1.3354\n",
      "a: -0.0065\n",
      "step:  295\n",
      "loss: 1.3680\n",
      "a: -0.0051\n",
      "step:  296\n",
      "loss: 1.2096\n",
      "a: -0.0038\n",
      "step:  297\n",
      "loss: 1.4351\n",
      "a: -0.0026\n",
      "step:  298\n",
      "loss: 1.5171\n",
      "a: -0.0015\n",
      "step:  299\n",
      "loss: 1.5058\n",
      "a: -0.0008\n",
      "step:  300\n",
      "loss: 1.6863\n",
      "a: -0.0019\n",
      "step:  301\n",
      "loss: 1.4714\n",
      "a: -0.0039\n",
      "step:  302\n",
      "loss: 1.3872\n",
      "a: -0.0059\n",
      "step:  303\n",
      "loss: 1.4142\n",
      "a: -0.0077\n",
      "step:  304\n",
      "loss: 1.5576\n",
      "a: -0.0091\n",
      "step:  305\n",
      "loss: 1.4400\n",
      "a: -0.0102\n",
      "step:  306\n",
      "loss: 1.7764\n",
      "a: -0.0110\n",
      "step:  307\n",
      "loss: 1.7344\n",
      "a: -0.0115\n",
      "step:  308\n",
      "loss: 1.6417\n",
      "a: -0.0113\n",
      "step:  309\n",
      "loss: 1.5149\n",
      "a: -0.0106\n",
      "step:  310\n",
      "loss: 1.4317\n",
      "a: -0.0094\n",
      "step:  311\n",
      "loss: 1.4205\n",
      "a: -0.0081\n",
      "step:  312\n",
      "loss: 1.4711\n",
      "a: -0.0067\n",
      "step:  313\n",
      "loss: 1.3685\n",
      "a: -0.0054\n",
      "step:  314\n",
      "loss: 1.3144\n",
      "a: -0.0041\n",
      "step:  315\n",
      "loss: 1.4795\n",
      "a: -0.0029\n",
      "step:  316\n",
      "loss: 1.4846\n",
      "a: -0.0020\n",
      "step:  317\n",
      "loss: 1.3948\n",
      "a: -0.0013\n",
      "step:  318\n",
      "loss: 1.5317\n",
      "a: -0.0010\n",
      "step:  319\n",
      "loss: 1.4584\n",
      "a: -0.0018\n",
      "step:  320\n",
      "loss: 1.3177\n",
      "a: -0.0030\n",
      "step:  321\n",
      "loss: 1.5132\n",
      "a: -0.0043\n",
      "step:  322\n",
      "loss: 1.4429\n",
      "a: -0.0054\n",
      "step:  323\n",
      "loss: 1.3838\n",
      "a: -0.0065\n",
      "step:  324\n",
      "loss: 1.3738\n",
      "a: -0.0073\n",
      "step:  325\n",
      "loss: 1.3955\n",
      "a: -0.0079\n",
      "step:  326\n",
      "loss: 1.5592\n",
      "a: -0.0084\n",
      "step:  327\n",
      "loss: 1.6626\n",
      "a: -0.0087\n",
      "step:  328\n",
      "loss: 1.6361\n",
      "a: -0.0089\n",
      "step:  329\n",
      "loss: 1.3335\n",
      "a: -0.0086\n",
      "step:  330\n",
      "loss: 1.4324\n",
      "a: -0.0081\n",
      "step:  331\n",
      "loss: 1.3349\n",
      "a: -0.0073\n",
      "step:  332\n",
      "loss: 1.3686\n",
      "a: -0.0063\n",
      "step:  333\n",
      "loss: 1.4735\n",
      "a: -0.0050\n",
      "step:  334\n",
      "loss: 1.2658\n",
      "a: -0.0036\n",
      "step:  335\n",
      "loss: 1.7588\n",
      "a: -0.0023\n",
      "step:  336\n",
      "loss: 1.3266\n",
      "a: -0.0017\n",
      "step:  337\n",
      "loss: 1.5655\n",
      "a: -0.0017\n",
      "step:  338\n",
      "loss: 1.3494\n",
      "a: -0.0024\n",
      "step:  339\n",
      "loss: 1.4804\n",
      "a: -0.0035\n",
      "step:  340\n",
      "loss: 1.5241\n",
      "a: -0.0045\n",
      "step:  341\n",
      "loss: 1.5804\n",
      "a: -0.0054\n",
      "step:  342\n",
      "loss: 1.3673\n",
      "a: -0.0061\n",
      "step:  343\n",
      "loss: 1.5484\n",
      "a: -0.0064\n",
      "step:  344\n",
      "loss: 1.5041\n",
      "a: -0.0066\n",
      "step:  345\n",
      "loss: 1.3817\n",
      "a: -0.0064\n",
      "step:  346\n",
      "loss: 1.3506\n",
      "a: -0.0059\n",
      "step:  347\n",
      "loss: 1.4246\n",
      "a: -0.0050\n",
      "step:  348\n",
      "loss: 1.3054\n",
      "a: -0.0038\n",
      "step:  349\n",
      "loss: 1.3737\n",
      "a: -0.0029\n",
      "step:  350\n",
      "loss: 1.5774\n",
      "a: -0.0025\n",
      "step:  351\n",
      "loss: 1.4159\n",
      "a: -0.0028\n",
      "step:  352\n",
      "loss: 1.4766\n",
      "a: -0.0029\n",
      "step:  353\n",
      "loss: 1.5666\n",
      "a: -0.0026\n",
      "step:  354\n",
      "loss: 1.5964\n",
      "a: -0.0029\n",
      "step:  355\n",
      "loss: 1.5003\n",
      "a: -0.0032\n",
      "step:  356\n",
      "loss: 1.2437\n",
      "a: -0.0035\n",
      "step:  357\n",
      "loss: 1.2803\n",
      "a: -0.0039\n",
      "step:  358\n",
      "loss: 1.5494\n",
      "a: -0.0038\n",
      "step:  359\n",
      "loss: 1.2628\n",
      "a: -0.0041\n",
      "step:  360\n",
      "loss: 1.4170\n",
      "a: -0.0046\n",
      "step:  361\n",
      "loss: 1.4638\n",
      "a: -0.0054\n",
      "step:  362\n",
      "loss: 1.6000\n",
      "a: -0.0060\n",
      "step:  363\n",
      "loss: 1.4271\n",
      "a: -0.0066\n",
      "step:  364\n",
      "loss: 1.3582\n",
      "a: -0.0062\n",
      "step:  365\n",
      "loss: 1.4264\n",
      "a: -0.0050\n",
      "step:  366\n",
      "loss: 1.5278\n",
      "a: -0.0033\n",
      "step:  367\n",
      "loss: 1.3411\n",
      "a: -0.0019\n",
      "step:  368\n",
      "loss: 1.7706\n",
      "a: -0.0013\n",
      "step:  369\n",
      "loss: 1.4430\n",
      "a: -0.0034\n",
      "step:  370\n",
      "loss: 1.4483\n",
      "a: -0.0055\n",
      "step:  371\n",
      "loss: 1.5394\n",
      "a: -0.0072\n",
      "step:  372\n",
      "loss: 1.6347\n",
      "a: -0.0081\n",
      "step:  373\n",
      "loss: 1.4695\n",
      "a: -0.0083\n",
      "step:  374\n",
      "loss: 1.5499\n",
      "a: -0.0080\n",
      "step:  375\n",
      "loss: 1.6720\n",
      "a: -0.0067\n",
      "step:  376\n",
      "loss: 1.1936\n",
      "a: -0.0049\n",
      "step:  377\n",
      "loss: 1.4614\n",
      "a: -0.0032\n",
      "step:  378\n",
      "loss: 1.3751\n",
      "a: -0.0015\n",
      "step:  379\n",
      "loss: 1.8183\n",
      "a: 0.0001\n",
      "step:  380\n",
      "loss: 1.1914\n",
      "a: -0.0024\n",
      "step:  381\n",
      "loss: 1.3997\n",
      "a: -0.0045\n",
      "step:  382\n",
      "loss: 1.3573\n",
      "a: -0.0063\n",
      "step:  383\n",
      "loss: 1.7872\n",
      "a: -0.0079\n",
      "step:  384\n",
      "loss: 1.4352\n",
      "a: -0.0093\n",
      "step:  385\n",
      "loss: 1.5285\n",
      "a: -0.0105\n",
      "step:  386\n",
      "loss: 1.7676\n",
      "a: -0.0115\n",
      "step:  387\n",
      "loss: 1.4668\n",
      "a: -0.0122\n",
      "step:  388\n",
      "loss: 1.8264\n",
      "a: -0.0127\n",
      "step:  389\n",
      "loss: 1.7786\n",
      "a: -0.0129\n",
      "step:  390\n",
      "loss: 1.8431\n",
      "a: -0.0128\n",
      "step:  391\n",
      "loss: 1.8258\n",
      "a: -0.0124\n",
      "step:  392\n",
      "loss: 1.7676\n",
      "a: -0.0116\n",
      "step:  393\n",
      "loss: 1.2490\n",
      "a: -0.0104\n",
      "step:  394\n",
      "loss: 1.5124\n",
      "a: -0.0092\n",
      "step:  395\n",
      "loss: 1.4598\n",
      "a: -0.0079\n",
      "step:  396\n",
      "loss: 1.4803\n",
      "a: -0.0067\n",
      "step:  397\n",
      "loss: 1.5367\n",
      "a: -0.0055\n",
      "step:  398\n",
      "loss: 1.7900\n",
      "a: -0.0044\n",
      "step:  399\n",
      "loss: 1.4096\n",
      "a: -0.0035\n",
      "step:  400\n",
      "loss: 1.3900\n",
      "a: -0.0027\n",
      "step:  401\n",
      "loss: 1.7690\n",
      "a: -0.0022\n",
      "step:  402\n",
      "loss: 1.6105\n",
      "a: -0.0020\n",
      "step:  403\n",
      "loss: 1.6909\n",
      "a: -0.0021\n",
      "step:  404\n",
      "loss: 1.6312\n",
      "a: -0.0027\n",
      "step:  405\n",
      "loss: 1.7770\n",
      "a: -0.0035\n",
      "step:  406\n",
      "loss: 1.5975\n",
      "a: -0.0045\n",
      "step:  407\n",
      "loss: 1.5194\n",
      "a: -0.0056\n",
      "step:  408\n",
      "loss: 1.6813\n",
      "a: -0.0068\n",
      "step:  409\n",
      "loss: 1.4421\n",
      "a: -0.0079\n",
      "step:  410\n",
      "loss: 1.5364\n",
      "a: -0.0090\n",
      "step:  411\n",
      "loss: 1.6065\n",
      "a: -0.0099\n",
      "step:  412\n",
      "loss: 1.4862\n",
      "a: -0.0107\n",
      "step:  413\n",
      "loss: 1.4724\n",
      "a: -0.0114\n",
      "step:  414\n",
      "loss: 1.6989\n",
      "a: -0.0118\n",
      "step:  415\n",
      "loss: 1.8300\n",
      "a: -0.0118\n",
      "step:  416\n",
      "loss: 1.5716\n",
      "a: -0.0115\n",
      "step:  417\n",
      "loss: 1.3857\n",
      "a: -0.0110\n",
      "step:  418\n",
      "loss: 1.6650\n",
      "a: -0.0102\n",
      "step:  419\n",
      "loss: 1.6840\n",
      "a: -0.0092\n",
      "step:  420\n",
      "loss: 1.5038\n",
      "a: -0.0081\n",
      "step:  421\n",
      "loss: 1.6525\n",
      "a: -0.0069\n",
      "step:  422\n",
      "loss: 1.4078\n",
      "a: -0.0056\n",
      "step:  423\n",
      "loss: 1.5590\n",
      "a: -0.0043\n",
      "step:  424\n",
      "loss: 1.3243\n",
      "a: -0.0031\n",
      "step:  425\n",
      "loss: 1.3945\n",
      "a: -0.0018\n",
      "step:  426\n",
      "loss: 1.6448\n",
      "a: -0.0008\n",
      "step:  427\n",
      "loss: 1.9456\n",
      "a: 0.0001\n",
      "step:  428\n",
      "loss: 1.5604\n",
      "a: 0.0003\n",
      "step:  429\n",
      "loss: 1.3839\n",
      "a: -0.0024\n",
      "step:  430\n",
      "loss: 1.5211\n",
      "a: -0.0045\n",
      "step:  431\n",
      "loss: 1.7385\n",
      "a: -0.0064\n",
      "step:  432\n",
      "loss: 1.3746\n",
      "a: -0.0080\n",
      "step:  433\n",
      "loss: 1.5746\n",
      "a: -0.0095\n",
      "step:  434\n",
      "loss: 1.8084\n",
      "a: -0.0109\n",
      "step:  435\n",
      "loss: 1.8677\n",
      "a: -0.0121\n",
      "step:  436\n",
      "loss: 1.7727\n",
      "a: -0.0132\n",
      "step:  437\n",
      "loss: 1.9030\n",
      "a: -0.0141\n",
      "step:  438\n",
      "loss: 2.2071\n",
      "a: -0.0150\n",
      "step:  439\n",
      "loss: 1.9243\n",
      "a: -0.0157\n",
      "step:  440\n",
      "loss: 2.1869\n",
      "a: -0.0162\n",
      "step:  441\n",
      "loss: 2.0041\n",
      "a: -0.0165\n",
      "step:  442\n",
      "loss: 2.3599\n",
      "a: -0.0167\n",
      "step:  443\n",
      "loss: 1.9475\n",
      "a: -0.0167\n",
      "step:  444\n",
      "loss: 1.9053\n",
      "a: -0.0165\n",
      "step:  445\n",
      "loss: 1.7523\n",
      "a: -0.0161\n",
      "step:  446\n",
      "loss: 2.0476\n",
      "a: -0.0155\n",
      "step:  447\n",
      "loss: 2.0135\n",
      "a: -0.0147\n",
      "step:  448\n",
      "loss: 1.8224\n",
      "a: -0.0137\n",
      "step:  449\n",
      "loss: 1.9502\n",
      "a: -0.0126\n",
      "step:  450\n",
      "loss: 1.6249\n",
      "a: -0.0115\n",
      "step:  451\n",
      "loss: 1.4393\n",
      "a: -0.0104\n",
      "step:  452\n",
      "loss: 1.7461\n",
      "a: -0.0092\n",
      "step:  453\n",
      "loss: 1.6359\n",
      "a: -0.0081\n",
      "step:  454\n",
      "loss: 1.6725\n",
      "a: -0.0069\n",
      "step:  455\n",
      "loss: 1.4999\n",
      "a: -0.0058\n",
      "step:  456\n",
      "loss: 1.6318\n",
      "a: -0.0047\n",
      "step:  457\n",
      "loss: 1.3595\n",
      "a: -0.0036\n",
      "step:  458\n",
      "loss: 1.4765\n",
      "a: -0.0026\n",
      "step:  459\n",
      "loss: 1.8408\n",
      "a: -0.0016\n",
      "step:  460\n",
      "loss: 1.6302\n",
      "a: -0.0006\n",
      "step:  461\n",
      "loss: 1.6242\n",
      "a: 0.0002\n",
      "step:  462\n",
      "loss: 1.9497\n",
      "a: 0.0004\n",
      "step:  463\n",
      "loss: 1.5056\n",
      "a: -0.0012\n",
      "step:  464\n",
      "loss: 1.4580\n",
      "a: -0.0025\n",
      "step:  465\n",
      "loss: 1.6800\n",
      "a: -0.0039\n",
      "step:  466\n",
      "loss: 1.5558\n",
      "a: -0.0052\n",
      "step:  467\n",
      "loss: 1.5933\n",
      "a: -0.0065\n",
      "step:  468\n",
      "loss: 1.5844\n",
      "a: -0.0077\n",
      "step:  469\n",
      "loss: 1.5516\n",
      "a: -0.0089\n",
      "step:  470\n",
      "loss: 1.4709\n",
      "a: -0.0100\n",
      "step:  471\n",
      "loss: 1.5399\n",
      "a: -0.0111\n",
      "step:  472\n",
      "loss: 1.5777\n",
      "a: -0.0121\n",
      "step:  473\n",
      "loss: 1.8503\n",
      "a: -0.0130\n",
      "step:  474\n",
      "loss: 1.8718\n",
      "a: -0.0139\n",
      "step:  475\n",
      "loss: 1.9781\n",
      "a: -0.0147\n",
      "step:  476\n",
      "loss: 1.9116\n",
      "a: -0.0153\n",
      "step:  477\n",
      "loss: 2.1566\n",
      "a: -0.0158\n",
      "step:  478\n",
      "loss: 2.1755\n",
      "a: -0.0161\n",
      "step:  479\n",
      "loss: 1.7344\n",
      "a: -0.0162\n",
      "step:  480\n",
      "loss: 2.3249\n",
      "a: -0.0162\n",
      "step:  481\n",
      "loss: 2.1301\n",
      "a: -0.0160\n",
      "step:  482\n",
      "loss: 1.9009\n",
      "a: -0.0156\n",
      "step:  483\n",
      "loss: 2.2466\n",
      "a: -0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  484\n",
      "loss: 1.8223\n",
      "a: -0.0142\n",
      "step:  485\n",
      "loss: 1.6479\n",
      "a: -0.0133\n",
      "step:  486\n",
      "loss: 1.8781\n",
      "a: -0.0122\n",
      "step:  487\n",
      "loss: 1.7053\n",
      "a: -0.0110\n",
      "step:  488\n",
      "loss: 1.6862\n",
      "a: -0.0098\n",
      "step:  489\n",
      "loss: 1.5665\n",
      "a: -0.0085\n",
      "step:  490\n",
      "loss: 1.5441\n",
      "a: -0.0072\n",
      "step:  491\n",
      "loss: 1.5239\n",
      "a: -0.0060\n",
      "step:  492\n",
      "loss: 1.6709\n",
      "a: -0.0048\n",
      "step:  493\n",
      "loss: 1.4837\n",
      "a: -0.0037\n",
      "step:  494\n",
      "loss: 1.5473\n",
      "a: -0.0026\n",
      "step:  495\n",
      "loss: 1.5519\n",
      "a: -0.0015\n",
      "step:  496\n",
      "loss: 1.8529\n",
      "a: -0.0005\n",
      "step:  497\n",
      "loss: 1.7404\n",
      "a: 0.0002\n",
      "step:  498\n",
      "loss: 1.6038\n",
      "a: -0.0001\n",
      "step:  499\n",
      "loss: 1.6841\n",
      "a: -0.0006\n",
      "step:  500\n",
      "loss: 1.5672\n",
      "a: -0.0014\n",
      "step:  501\n",
      "loss: 1.6632\n",
      "a: -0.0022\n",
      "step:  502\n",
      "loss: 1.6321\n",
      "a: -0.0032\n",
      "step:  503\n",
      "loss: 1.4383\n",
      "a: -0.0041\n",
      "step:  504\n",
      "loss: 1.6570\n",
      "a: -0.0051\n",
      "step:  505\n",
      "loss: 1.4947\n",
      "a: -0.0061\n",
      "step:  506\n",
      "loss: 1.6434\n",
      "a: -0.0070\n",
      "step:  507\n",
      "loss: 1.5852\n",
      "a: -0.0079\n",
      "step:  508\n",
      "loss: 1.6863\n",
      "a: -0.0088\n",
      "step:  509\n",
      "loss: 1.5260\n",
      "a: -0.0097\n",
      "step:  510\n",
      "loss: 1.6556\n",
      "a: -0.0104\n",
      "step:  511\n",
      "loss: 1.5281\n",
      "a: -0.0112\n",
      "step:  512\n",
      "loss: 1.5763\n",
      "a: -0.0118\n",
      "step:  513\n",
      "loss: 1.9250\n",
      "a: -0.0124\n",
      "step:  514\n",
      "loss: 1.7211\n",
      "a: -0.0128\n",
      "step:  515\n",
      "loss: 1.5164\n",
      "a: -0.0132\n",
      "step:  516\n",
      "loss: 1.9687\n",
      "a: -0.0133\n",
      "step:  517\n",
      "loss: 1.7745\n",
      "a: -0.0133\n",
      "step:  518\n",
      "loss: 1.6445\n",
      "a: -0.0129\n",
      "step:  519\n",
      "loss: 1.7095\n",
      "a: -0.0125\n",
      "step:  520\n",
      "loss: 1.7216\n",
      "a: -0.0118\n",
      "step:  521\n",
      "loss: 1.7386\n",
      "a: -0.0111\n",
      "step:  522\n",
      "loss: 1.8622\n",
      "a: -0.0101\n",
      "step:  523\n",
      "loss: 1.6694\n",
      "a: -0.0091\n",
      "step:  524\n",
      "loss: 1.6177\n",
      "a: -0.0080\n",
      "step:  525\n",
      "loss: 1.5338\n",
      "a: -0.0069\n",
      "step:  526\n",
      "loss: 1.5866\n",
      "a: -0.0057\n",
      "step:  527\n",
      "loss: 1.6463\n",
      "a: -0.0044\n",
      "step:  528\n",
      "loss: 1.5078\n",
      "a: -0.0032\n",
      "step:  529\n",
      "loss: 1.8282\n",
      "a: -0.0021\n",
      "step:  530\n",
      "loss: 1.5575\n",
      "a: -0.0011\n",
      "step:  531\n",
      "loss: 1.6924\n",
      "a: -0.0002\n",
      "step:  532\n",
      "loss: 2.4011\n",
      "a: 0.0006\n",
      "step:  533\n",
      "loss: 1.6422\n",
      "a: -0.0020\n",
      "step:  534\n",
      "loss: 1.4662\n",
      "a: -0.0041\n",
      "step:  535\n",
      "loss: 1.2930\n",
      "a: -0.0060\n",
      "step:  536\n",
      "loss: 1.5241\n",
      "a: -0.0076\n",
      "step:  537\n",
      "loss: 1.6975\n",
      "a: -0.0091\n",
      "step:  538\n",
      "loss: 1.4229\n",
      "a: -0.0104\n",
      "step:  539\n",
      "loss: 1.7580\n",
      "a: -0.0117\n",
      "step:  540\n",
      "loss: 1.7183\n",
      "a: -0.0129\n",
      "step:  541\n",
      "loss: 2.0004\n",
      "a: -0.0141\n",
      "step:  542\n",
      "loss: 2.1851\n",
      "a: -0.0152\n",
      "step:  543\n",
      "loss: 2.1807\n",
      "a: -0.0162\n",
      "step:  544\n",
      "loss: 2.2652\n",
      "a: -0.0171\n",
      "step:  545\n",
      "loss: 2.1195\n",
      "a: -0.0179\n",
      "step:  546\n",
      "loss: 2.0966\n",
      "a: -0.0187\n",
      "step:  547\n",
      "loss: 2.4934\n",
      "a: -0.0193\n",
      "step:  548\n",
      "loss: 2.6251\n",
      "a: -0.0199\n",
      "step:  549\n",
      "loss: 2.4641\n",
      "a: -0.0203\n",
      "step:  550\n",
      "loss: 2.4123\n",
      "a: -0.0205\n",
      "step:  551\n",
      "loss: 2.4635\n",
      "a: -0.0207\n",
      "step:  552\n",
      "loss: 2.4450\n",
      "a: -0.0207\n",
      "step:  553\n",
      "loss: 2.2607\n",
      "a: -0.0205\n",
      "step:  554\n",
      "loss: 2.2623\n",
      "a: -0.0201\n",
      "step:  555\n",
      "loss: 2.1236\n",
      "a: -0.0196\n",
      "step:  556\n",
      "loss: 1.9308\n",
      "a: -0.0189\n",
      "step:  557\n",
      "loss: 2.1605\n",
      "a: -0.0180\n",
      "step:  558\n",
      "loss: 2.3417\n",
      "a: -0.0170\n",
      "step:  559\n",
      "loss: 1.7416\n",
      "a: -0.0158\n",
      "step:  560\n",
      "loss: 1.9004\n",
      "a: -0.0146\n",
      "step:  561\n",
      "loss: 1.7424\n",
      "a: -0.0133\n",
      "step:  562\n",
      "loss: 1.6182\n",
      "a: -0.0120\n",
      "step:  563\n",
      "loss: 1.4844\n",
      "a: -0.0108\n",
      "step:  564\n",
      "loss: 1.7367\n",
      "a: -0.0095\n",
      "step:  565\n",
      "loss: 1.6736\n",
      "a: -0.0083\n",
      "step:  566\n",
      "loss: 1.8476\n",
      "a: -0.0071\n",
      "step:  567\n",
      "loss: 1.8394\n",
      "a: -0.0060\n",
      "step:  568\n",
      "loss: 1.6672\n",
      "a: -0.0048\n",
      "step:  569\n",
      "loss: 1.8599\n",
      "a: -0.0038\n",
      "step:  570\n",
      "loss: 2.3641\n",
      "a: -0.0028\n",
      "step:  571\n",
      "loss: 1.8664\n",
      "a: -0.0019\n",
      "step:  572\n",
      "loss: 1.6074\n",
      "a: -0.0010\n",
      "step:  573\n",
      "loss: 1.3696\n",
      "a: -0.0002\n",
      "step:  574\n",
      "loss: 2.2987\n",
      "a: 0.0005\n",
      "step:  575\n",
      "loss: 2.0533\n",
      "a: 0.0003\n",
      "step:  576\n",
      "loss: 1.4549\n",
      "a: -0.0008\n",
      "step:  577\n",
      "loss: 1.7564\n",
      "a: -0.0020\n",
      "step:  578\n",
      "loss: 1.6598\n",
      "a: -0.0032\n",
      "step:  579\n",
      "loss: 1.5558\n",
      "a: -0.0044\n",
      "step:  580\n",
      "loss: 1.6511\n",
      "a: -0.0056\n",
      "step:  581\n",
      "loss: 1.5704\n",
      "a: -0.0068\n",
      "step:  582\n",
      "loss: 1.4829\n",
      "a: -0.0080\n",
      "step:  583\n",
      "loss: 1.5268\n",
      "a: -0.0090\n",
      "step:  584\n",
      "loss: 1.5883\n",
      "a: -0.0101\n",
      "step:  585\n",
      "loss: 1.7334\n",
      "a: -0.0111\n",
      "step:  586\n",
      "loss: 2.0150\n",
      "a: -0.0121\n",
      "step:  587\n",
      "loss: 1.7514\n",
      "a: -0.0130\n",
      "step:  588\n",
      "loss: 2.1706\n",
      "a: -0.0139\n",
      "step:  589\n",
      "loss: 1.9335\n",
      "a: -0.0147\n",
      "step:  590\n",
      "loss: 2.0530\n",
      "a: -0.0154\n",
      "step:  591\n",
      "loss: 2.1806\n",
      "a: -0.0160\n",
      "step:  592\n",
      "loss: 2.2779\n",
      "a: -0.0165\n",
      "step:  593\n",
      "loss: 2.2316\n",
      "a: -0.0169\n",
      "step:  594\n",
      "loss: 2.4358\n",
      "a: -0.0172\n",
      "step:  595\n",
      "loss: 2.4354\n",
      "a: -0.0173\n",
      "step:  596\n",
      "loss: 2.0021\n",
      "a: -0.0173\n",
      "step:  597\n",
      "loss: 2.2460\n",
      "a: -0.0171\n",
      "step:  598\n",
      "loss: 1.9036\n",
      "a: -0.0167\n",
      "step:  599\n",
      "loss: 2.0858\n",
      "a: -0.0162\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code snippets to build the convolutional network:\n",
    "\n",
    "```python\n",
    "    from torch . nn . functional import conv2d , max_pool2d\n",
    "    convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "    subsampleing_layer = max_pool_2d ( convolutional_layer , (2 , 2) ) # reduces window 2x2 to 1 pixel\n",
    "    out_layer = dropout ( subsample_layer , p_drop_input )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  80\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "# modify for more speed\n",
    "f1, f2, f3 = 20, 40, 80\n",
    "pic_in1, pic_in2, pic_in3 = 1, 20, 40\n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        X = conv_layer(X, w_conv3, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, w_conv3, p_drop_input)\n",
    "    return Y.size()[1]\n",
    "\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, w_conv3, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([-0.1], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 1., 1.)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(train_loader, test_loader, num_epochs, log_interval):\n",
    "    # run training\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, epoch, log_interval)\n",
    "        test(test_loader)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 14.3353\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 5.7286\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 3.9902\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.5469\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.4593\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.2992\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 2139/10000 (21%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.3846\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.3861\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.2285\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.3094\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.2173\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.1853\n",
      "\n",
      "Test set: Average loss: 0.0422, Accuracy: 3939/10000 (39%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.1400\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.1945\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.2235\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.1165\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.0370\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.7808\n",
      "\n",
      "Test set: Average loss: 0.0328, Accuracy: 5204/10000 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.7241\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.7751\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.5230\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.5417\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.7022\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.6868\n",
      "\n",
      "Test set: Average loss: 0.0231, Accuracy: 6442/10000 (64%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.7241\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.5158\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.1980\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.2714\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.2278\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.2465\n",
      "\n",
      "Test set: Average loss: 0.0196, Accuracy: 7112/10000 (71%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.2803\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.5279\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.2915\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.2678\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.4546\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.4255\n",
      "\n",
      "Test set: Average loss: 0.0174, Accuracy: 7431/10000 (74%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.2571\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.3855\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.4883\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.0600\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.1393\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.0253\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 7802/10000 (78%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.1873\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.2203\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.1394\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.9901\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.0515\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.2335\n",
      "\n",
      "Test set: Average loss: 0.0136, Accuracy: 7960/10000 (79%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.2073\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.8506\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.0890\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.2862\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.8735\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.1940\n",
      "\n",
      "Test set: Average loss: 0.0126, Accuracy: 8145/10000 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.4672\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.0076\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.8663\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.9583\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.9335\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.8568\n",
      "\n",
      "Test set: Average loss: 0.0119, Accuracy: 8216/10000 (82%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.0536\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 1.4370\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.7200\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 0.8593\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.8146\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 0.9874\n",
      "\n",
      "Test set: Average loss: 0.0114, Accuracy: 8430/10000 (84%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.1211\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 1.1441\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.9761\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 1.0502\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.9552\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 1.2073\n",
      "\n",
      "Test set: Average loss: 0.0104, Accuracy: 8529/10000 (85%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.0332\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.6147\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.7401\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 0.7157\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 1.0178\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 0.7342\n",
      "\n",
      "Test set: Average loss: 0.0100, Accuracy: 8586/10000 (85%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.3142\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.9034\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 1.0357\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 1.3284\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.7008\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.9023\n",
      "\n",
      "Test set: Average loss: 0.0096, Accuracy: 8700/10000 (87%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.6240\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 0.7279\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.3792\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 0.8366\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.5875\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 0.6138\n",
      "\n",
      "Test set: Average loss: 0.0093, Accuracy: 8708/10000 (87%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.2615\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 1.0892\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.9182\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 1.2488\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.5473\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 0.6965\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 8797/10000 (87%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.7241\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 0.4234\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 1.0197\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 0.7008\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.8870\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 1.2393\n",
      "\n",
      "Test set: Average loss: 0.0080, Accuracy: 8824/10000 (88%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.6342\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 0.6034\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.8594\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 1.6223\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.7971\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 0.5926\n",
      "\n",
      "Test set: Average loss: 0.0079, Accuracy: 8943/10000 (89%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.6929\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 1.0187\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.9102\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 0.5795\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.6754\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 0.8375\n",
      "\n",
      "Test set: Average loss: 0.0080, Accuracy: 8990/10000 (89%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.6356\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 1.1222\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 1.0202\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 0.7481\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.5144\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 0.9285\n",
      "\n",
      "Test set: Average loss: 0.0078, Accuracy: 8970/10000 (89%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.5285\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 0.5933\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 0.6815\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 0.9500\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.7467\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 0.9347\n",
      "\n",
      "Test set: Average loss: 0.0078, Accuracy: 9001/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.4677\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 0.7598\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 0.8418\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 1.0657\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 0.7221\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 0.7118\n",
      "\n",
      "Test set: Average loss: 0.0078, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.7638\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 0.8783\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 0.7013\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 0.4921\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 0.4476\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 0.8664\n",
      "\n",
      "Test set: Average loss: 0.0072, Accuracy: 9051/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.9389\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 1.5864\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 0.6501\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 0.9876\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 0.8890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 0.7465\n",
      "\n",
      "Test set: Average loss: 0.0075, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.8741\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 0.6144\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 0.6375\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 1.1881\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 0.9930\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 0.5682\n",
      "\n",
      "Test set: Average loss: 0.0073, Accuracy: 9074/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 1.0793\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 0.7046\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 0.7334\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 0.6571\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 0.8038\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 0.6694\n",
      "\n",
      "Test set: Average loss: 0.0073, Accuracy: 9080/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.0358\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 0.5347\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 1.0430\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 1.1212\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 0.9181\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 1.2209\n",
      "\n",
      "Test set: Average loss: 0.0073, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.9705\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 1.2817\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 0.8020\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 1.0346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-115:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 161, in normalize\n",
      "    t.sub_(m).div_(s)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/mnist.py\", line 76, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 118, in __call__\n",
      "    return F.normalize(tensor, self.mean, self.std)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-649f2faeaf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-3f284de02341>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(train_loader, test_loader, num_epochs, log_interval)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3c012f2b7d03>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, log_interval)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# print to screen every log_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpre_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#output = softmax(pre_softmax)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# note: torch.nn.functional.cross_entropy applies log_softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-8705e152917c>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_output_pixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-8705e152917c>\u001b[0m in \u001b[0;36mconv_layer\u001b[0;34m(X, weightvector, p_drop)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrectify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reduces window 2x2 to 1 pixel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmax_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \"\"\"\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_epochs = 30\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-03 *\n",
      "       [ 4.2836])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues:\n",
    " - find nice hyperparameters (learning rate, dropout, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.543125867843628\n",
      "Loss: 3.167893409729004\n",
      "Loss: 2.11494517326355\n",
      "Loss: 2.336005449295044\n",
      "Loss: 2.2041916847229004\n",
      "Loss: 2.149024724960327\n",
      "Loss: 1.8707449436187744\n",
      "Loss: 1.8855136632919312\n",
      "Loss: 2.0242385864257812\n",
      "Loss: 1.8084663152694702\n",
      "Loss: 1.807221531867981\n",
      "Loss: 1.6832900047302246\n",
      "Loss: 1.727016806602478\n",
      "Loss: 1.5345144271850586\n",
      "Loss: 1.6410164833068848\n",
      "Loss: 1.5259240865707397\n",
      "Loss: 1.4974876642227173\n",
      "Loss: 1.4708524942398071\n",
      "Loss: 1.3217053413391113\n",
      "Loss: 1.3835594654083252\n",
      "Loss: 1.2267502546310425\n",
      "Loss: 1.1142263412475586\n",
      "Loss: 1.1801670789718628\n",
      "Loss: 1.141950249671936\n",
      "Loss: 1.1050403118133545\n",
      "Loss: 1.2678269147872925\n",
      "Loss: 1.0788801908493042\n",
      "Loss: 1.094488501548767\n",
      "Loss: 0.9235233068466187\n",
      "Loss: 0.971934974193573\n",
      "Loss: 0.7941033840179443\n",
      "Loss: 0.7810167670249939\n",
      "Loss: 0.805601179599762\n",
      "Loss: 1.1381202936172485\n",
      "Loss: 0.82025146484375\n",
      "Loss: 0.9093353748321533\n",
      "Loss: 0.8725736141204834\n",
      "Loss: 0.760343611240387\n",
      "Loss: 0.9493373036384583\n",
      "Loss: 0.8345934152603149\n",
      "Loss: 0.8497349619865417\n",
      "Loss: 0.7667115926742554\n",
      "Loss: 0.7867527008056641\n",
      "Loss: 0.7349132299423218\n",
      "Loss: 0.7115955352783203\n",
      "Loss: 0.7291639447212219\n",
      "Loss: 0.6665917038917542\n",
      "Loss: 0.7486509084701538\n",
      "Loss: 0.7161068916320801\n",
      "Loss: 0.7960557341575623\n",
      "Loss: 0.6506683826446533\n",
      "Loss: 0.5860702395439148\n",
      "Loss: 0.6972334384918213\n",
      "Loss: 0.9605365991592407\n",
      "Loss: 0.7969971299171448\n",
      "Loss: 0.6626360416412354\n",
      "Loss: 0.5306394100189209\n",
      "Loss: 0.5508484840393066\n",
      "Loss: 0.6004781126976013\n",
      "Loss: 0.6941526532173157\n",
      "Loss: 0.772136926651001\n",
      "Loss: 0.906826913356781\n",
      "Loss: 0.7131541967391968\n",
      "Loss: 0.5927572846412659\n",
      "Loss: 0.7892910838127136\n",
      "Loss: 0.6536308526992798\n",
      "Loss: 0.5257377624511719\n",
      "Loss: 0.7337332963943481\n",
      "Loss: 0.6729161739349365\n",
      "Loss: 0.445376992225647\n",
      "Loss: 0.6493065357208252\n",
      "Loss: 0.6299341320991516\n",
      "Loss: 0.5822784900665283\n",
      "Loss: 0.6469131708145142\n",
      "Loss: 0.7829668521881104\n",
      "Loss: 0.44263336062431335\n",
      "Loss: 0.5075831413269043\n",
      "Loss: 0.40777674317359924\n",
      "Loss: 0.836925208568573\n",
      "Loss: 1.058295726776123\n",
      "Loss: 0.6382925510406494\n",
      "Loss: 0.617149829864502\n",
      "Loss: 0.7824652791023254\n",
      "Loss: 0.5925714373588562\n",
      "Loss: 0.873542070388794\n",
      "Loss: 0.6565977334976196\n",
      "Loss: 0.9873127937316895\n",
      "Loss: 0.7949115037918091\n",
      "Loss: 0.6609464287757874\n",
      "Loss: 0.5788174271583557\n",
      "Loss: 0.7516103982925415\n",
      "Loss: 0.6509892344474792\n",
      "Loss: 0.5958733558654785\n",
      "Loss: 0.6490173935890198\n",
      "Loss: 0.7616950869560242\n",
      "Loss: 0.5780243873596191\n",
      "Loss: 0.74455726146698\n",
      "Loss: 0.6937625408172607\n",
      "Loss: 0.7276437878608704\n",
      "Loss: 0.43361666798591614\n",
      "Loss: 0.2607033848762512\n",
      "Loss: 0.5549665093421936\n",
      "Loss: 0.3741886019706726\n",
      "Loss: 0.7597572803497314\n",
      "Loss: 0.671815037727356\n",
      "Loss: 0.8120179176330566\n",
      "Loss: 1.0782651901245117\n",
      "Loss: 1.0359294414520264\n",
      "Loss: 0.711712658405304\n",
      "Loss: 0.3487284183502197\n",
      "Loss: 0.7569056749343872\n",
      "Loss: 0.8694841265678406\n",
      "Loss: 0.7849181294441223\n",
      "Loss: 0.6283538937568665\n",
      "Loss: 0.8279719352722168\n",
      "Loss: 0.5074806809425354\n",
      "Loss: 0.6219611167907715\n",
      "Loss: 0.5130611658096313\n",
      "Loss: 0.6035650372505188\n",
      "Loss: 1.4705297946929932\n",
      "Loss: 0.5992090702056885\n",
      "Loss: 0.5464145541191101\n",
      "Loss: 0.6296575665473938\n",
      "Loss: 0.7436283230781555\n",
      "Loss: 0.4980117678642273\n",
      "Loss: 0.4760343134403229\n",
      "Loss: 0.4389892816543579\n",
      "Loss: 0.471032053232193\n",
      "Loss: 0.3582862615585327\n",
      "Loss: 0.7368035912513733\n",
      "Loss: 0.5076072216033936\n",
      "Loss: 0.4085736572742462\n",
      "Loss: 0.7104473114013672\n",
      "Loss: 0.5750009417533875\n",
      "Loss: 0.5983902215957642\n",
      "Loss: 0.39349204301834106\n",
      "Loss: 0.35809722542762756\n",
      "Loss: 0.6244493722915649\n",
      "Loss: 0.5181422233581543\n",
      "Loss: 0.5897938013076782\n",
      "Loss: 0.32791393995285034\n",
      "Loss: 0.6553816199302673\n",
      "Loss: 0.44899070262908936\n",
      "Loss: 0.43928036093711853\n",
      "Loss: 0.6859478950500488\n",
      "Loss: 0.48522841930389404\n",
      "Loss: 0.4430506229400635\n",
      "Loss: 0.3694561719894409\n",
      "Loss: 0.8837478756904602\n",
      "Loss: 0.37564659118652344\n",
      "Loss: 0.4703044593334198\n",
      "Loss: 0.3360729515552521\n",
      "Loss: 0.4552886486053467\n",
      "Loss: 0.36390212178230286\n",
      "Loss: 0.4628835916519165\n",
      "Loss: 0.6630728244781494\n",
      "Loss: 0.6735960245132446\n",
      "Loss: 0.3399253189563751\n",
      "Loss: 0.5440161228179932\n",
      "Loss: 0.46644240617752075\n",
      "Loss: 0.561577320098877\n",
      "Loss: 0.465920090675354\n",
      "Loss: 0.5755780935287476\n",
      "Loss: 0.6009624600410461\n",
      "Loss: 0.6733734011650085\n",
      "Loss: 0.5053571462631226\n",
      "Loss: 0.33450084924697876\n",
      "Loss: 0.25926992297172546\n",
      "Loss: 0.837674617767334\n",
      "Loss: 0.4861740469932556\n",
      "Loss: 0.6308353543281555\n",
      "Loss: 0.3977712392807007\n",
      "Loss: 0.5380299687385559\n",
      "Loss: 0.4561898112297058\n",
      "Loss: 0.6074825525283813\n",
      "Loss: 0.43578895926475525\n",
      "Loss: 0.7293288707733154\n",
      "Loss: 0.6179559826850891\n",
      "Loss: 0.34304478764533997\n",
      "Loss: 0.6665535569190979\n",
      "Loss: 0.49446114897727966\n",
      "Loss: 0.5785182118415833\n",
      "Loss: 0.609864354133606\n",
      "Loss: 0.5001081228256226\n",
      "Loss: 0.446624219417572\n",
      "Loss: 0.47131606936454773\n",
      "Loss: 0.48182037472724915\n",
      "Loss: 0.3373611867427826\n",
      "Loss: 0.5514293909072876\n",
      "Loss: 0.8270677924156189\n",
      "Loss: 0.608237624168396\n",
      "Loss: 0.4844095706939697\n",
      "Loss: 0.7138293385505676\n",
      "Loss: 0.6245492100715637\n",
      "Loss: 0.638790488243103\n",
      "Loss: 0.5265518426895142\n",
      "Loss: 0.6650659441947937\n",
      "Loss: 0.4940197467803955\n",
      "Loss: 0.5758970975875854\n",
      "Loss: 0.5855565071105957\n",
      "Loss: 0.44776371121406555\n",
      "Loss: 0.6618592739105225\n",
      "Loss: 0.5572037100791931\n",
      "Loss: 0.6597018241882324\n",
      "Loss: 0.4858906865119934\n",
      "Loss: 0.854220986366272\n",
      "Loss: 0.5877754092216492\n",
      "Loss: 0.5692876577377319\n",
      "Loss: 0.4962370991706848\n",
      "Loss: 0.5346415042877197\n",
      "Loss: 0.39474526047706604\n",
      "Loss: 0.5360918045043945\n",
      "Loss: 0.7923516631126404\n",
      "Loss: 0.7262030243873596\n",
      "Loss: 0.7140786647796631\n",
      "Loss: 0.4813186228275299\n",
      "Loss: 0.4384559988975525\n",
      "Loss: 0.4726206958293915\n",
      "Loss: 0.43824493885040283\n",
      "Loss: 0.5471162796020508\n",
      "Loss: 0.8525763750076294\n",
      "Loss: 0.3496195077896118\n",
      "Loss: 0.478585809469223\n",
      "Loss: 0.6664895415306091\n",
      "Loss: 0.46948978304862976\n",
      "Loss: 0.4036857485771179\n",
      "Loss: 0.46715113520622253\n",
      "Loss: 0.5079823136329651\n",
      "Loss: 0.55478835105896\n",
      "Loss: 0.39636820554733276\n",
      "Loss: 0.39287489652633667\n",
      "Loss: 0.44255006313323975\n",
      "Loss: 0.3436432182788849\n",
      "Loss: 0.39213457703590393\n",
      "Loss: 0.5985120534896851\n",
      "Loss: 0.4715498387813568\n",
      "Loss: 0.3267037570476532\n",
      "Loss: 0.6487202644348145\n",
      "Loss: 0.5418894290924072\n",
      "Loss: 0.6577427387237549\n",
      "Loss: 0.7137593030929565\n",
      "Loss: 0.674607515335083\n",
      "Loss: 0.823448896408081\n",
      "Loss: 0.660854160785675\n",
      "Loss: 0.4341704249382019\n",
      "Loss: 0.5297477841377258\n",
      "Loss: 0.4468463659286499\n",
      "Loss: 0.6200408935546875\n",
      "Loss: 0.4344649910926819\n",
      "Loss: 0.4718482196331024\n",
      "Loss: 0.6005867123603821\n",
      "Loss: 0.5697112679481506\n",
      "Loss: 0.48942339420318604\n",
      "Loss: 0.6486670970916748\n",
      "Loss: 0.4464380145072937\n",
      "Loss: 0.6461766958236694\n",
      "Loss: 0.4885598421096802\n",
      "Loss: 0.5741355419158936\n",
      "Loss: 0.3620513081550598\n",
      "Loss: 0.41392460465431213\n",
      "Loss: 0.4353838860988617\n",
      "Loss: 0.9805538654327393\n",
      "Loss: 0.3558250665664673\n",
      "Loss: 0.5500062108039856\n",
      "Loss: 0.35159939527511597\n",
      "Loss: 0.5796972513198853\n",
      "Loss: 0.54791259765625\n",
      "Loss: 0.5569069385528564\n",
      "Loss: 0.5466462969779968\n",
      "Loss: 0.3468053340911865\n",
      "Loss: 0.31213974952697754\n",
      "Loss: 0.5815581679344177\n",
      "Loss: 0.3948398530483246\n",
      "Loss: 0.5069096088409424\n",
      "Loss: 0.3681986927986145\n",
      "Loss: 0.533917248249054\n",
      "Loss: 0.6358335614204407\n",
      "Loss: 0.6369436383247375\n",
      "Loss: 0.5357253551483154\n",
      "Loss: 0.35858914256095886\n",
      "Loss: 0.764359712600708\n",
      "Loss: 0.4662341773509979\n",
      "Loss: 0.2850874960422516\n",
      "Loss: 0.4108167290687561\n",
      "Loss: 0.5675909519195557\n",
      "Loss: 0.38694941997528076\n",
      "Loss: 0.5507446527481079\n",
      "Loss: 0.42973318696022034\n",
      "Loss: 0.4129799008369446\n",
      "Loss: 0.4323199391365051\n",
      "Loss: 0.4936731457710266\n",
      "Loss: 0.39434733986854553\n",
      "Loss: 0.5653142333030701\n",
      "Loss: 0.508191704750061\n",
      "Loss: 0.5653679370880127\n",
      "Loss: 0.5446804761886597\n",
      "Loss: 0.447426974773407\n",
      "Loss: 0.5488207340240479\n",
      "Loss: 0.3153967559337616\n",
      "Loss: 0.44894665479660034\n",
      "Loss: 0.4487065374851227\n",
      "Loss: 0.41771233081817627\n",
      "Loss: 0.27999892830848694\n",
      "Loss: 0.5052406787872314\n",
      "Loss: 0.4066275358200073\n",
      "Loss: 0.4443751573562622\n",
      "Loss: 0.5174857974052429\n",
      "Loss: 0.5538647770881653\n",
      "Loss: 0.5304341316223145\n",
      "Loss: 0.4380297064781189\n",
      "Loss: 0.49605098366737366\n",
      "Loss: 0.5264216661453247\n",
      "Loss: 0.5240545868873596\n",
      "Loss: 0.528285562992096\n",
      "Loss: 0.6059668064117432\n",
      "Loss: 0.4012751281261444\n",
      "Loss: 0.317913293838501\n",
      "Loss: 0.4873128831386566\n",
      "Loss: 0.5257595181465149\n",
      "Loss: 0.6081820726394653\n",
      "Loss: 0.507291316986084\n",
      "Loss: 0.5615146160125732\n",
      "Loss: 0.5087234377861023\n",
      "Loss: 0.24231939017772675\n",
      "Loss: 0.4001011252403259\n",
      "Loss: 0.38139981031417847\n",
      "Loss: 0.29400625824928284\n",
      "Loss: 0.3064061403274536\n",
      "Loss: 0.5177963972091675\n",
      "Loss: 0.26238369941711426\n",
      "Loss: 0.42519742250442505\n",
      "Loss: 0.6458446383476257\n",
      "Loss: 0.3672717213630676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.43049556016921997\n",
      "Loss: 0.4028354287147522\n",
      "Loss: 0.7192733287811279\n",
      "Loss: 0.42686694860458374\n",
      "Loss: 0.2545320391654968\n",
      "Loss: 0.45023685693740845\n",
      "Loss: 0.2101462483406067\n",
      "Loss: 0.9105786681175232\n",
      "Loss: 0.5288601517677307\n",
      "Loss: 0.7229608297348022\n",
      "Loss: 0.4575176537036896\n",
      "Loss: 0.43677496910095215\n",
      "Loss: 0.30618512630462646\n",
      "Loss: 0.43474656343460083\n",
      "Loss: 0.39928972721099854\n",
      "Loss: 0.5100942254066467\n",
      "Loss: 0.8629547357559204\n",
      "Loss: 0.5943202972412109\n",
      "Loss: 0.38674426078796387\n",
      "Loss: 0.4581432640552521\n",
      "Loss: 0.5663266777992249\n",
      "Loss: 0.8519665002822876\n",
      "Loss: 0.33807235956192017\n",
      "Loss: 0.7346905469894409\n",
      "Loss: 0.3195611834526062\n",
      "Loss: 0.40302401781082153\n",
      "Loss: 0.4184427559375763\n",
      "Loss: 0.32671600580215454\n",
      "Loss: 0.19888153672218323\n",
      "Loss: 0.6668321490287781\n",
      "Loss: 0.48521482944488525\n",
      "Loss: 0.3711341917514801\n",
      "Loss: 0.3347781002521515\n",
      "Loss: 0.5677977204322815\n",
      "Loss: 0.43925610184669495\n",
      "Loss: 0.5576645135879517\n",
      "Loss: 0.5453698635101318\n",
      "Loss: 0.21910996735095978\n",
      "Loss: 0.3059163987636566\n",
      "Loss: 0.3567739725112915\n",
      "Loss: 0.7064210772514343\n",
      "Loss: 0.4125763773918152\n",
      "Loss: 0.3560177981853485\n",
      "Loss: 0.3039160966873169\n",
      "Loss: 0.43911534547805786\n",
      "Loss: 0.26969432830810547\n",
      "Loss: 0.46823593974113464\n",
      "Loss: 0.34951090812683105\n",
      "Loss: 0.5085801482200623\n",
      "Loss: 0.46331265568733215\n",
      "Loss: 0.21625390648841858\n",
      "Loss: 0.6479178667068481\n",
      "Loss: 0.3986327648162842\n",
      "Loss: 0.31819969415664673\n",
      "Loss: 0.5055038332939148\n",
      "Loss: 0.33162787556648254\n",
      "Loss: 0.6196340322494507\n",
      "Loss: 0.3940775394439697\n",
      "Loss: 0.2827240824699402\n",
      "Loss: 0.38914766907691956\n",
      "Loss: 0.3659714162349701\n",
      "Loss: 0.3604919910430908\n",
      "Loss: 0.5153769850730896\n",
      "Loss: 0.38144686818122864\n",
      "Loss: 0.4502048194408417\n",
      "Loss: 0.7005632519721985\n",
      "Loss: 0.24960985779762268\n",
      "Loss: 0.5323253273963928\n",
      "Loss: 0.579677402973175\n",
      "Loss: 0.24801547825336456\n",
      "Loss: 0.3780307471752167\n",
      "Loss: 0.35899803042411804\n",
      "Loss: 0.5597414970397949\n",
      "Loss: 0.4855033755302429\n",
      "Loss: 0.49954450130462646\n",
      "Loss: 0.4351375102996826\n",
      "Loss: 0.23116163909435272\n",
      "Loss: 0.35116657614707947\n",
      "Loss: 0.5770280957221985\n",
      "Loss: 0.4861333966255188\n",
      "Loss: 0.42661407589912415\n",
      "Loss: 0.4557521939277649\n",
      "Loss: 0.3466983437538147\n",
      "Loss: 0.28088533878326416\n",
      "Loss: 0.33288630843162537\n",
      "Loss: 0.204742431640625\n",
      "Loss: 0.4876135587692261\n",
      "Loss: 0.2959033250808716\n",
      "Loss: 0.4720515012741089\n",
      "Loss: 0.38074928522109985\n",
      "Loss: 0.3906065821647644\n",
      "Loss: 0.6500738263130188\n",
      "Loss: 0.3894750475883484\n",
      "Loss: 0.4279818832874298\n",
      "Loss: 0.490749329328537\n",
      "Loss: 0.5881235599517822\n",
      "Loss: 0.6651848554611206\n",
      "Loss: 0.4370537996292114\n",
      "Loss: 0.33022353053092957\n",
      "Loss: 0.2586171627044678\n",
      "Loss: 0.35689499974250793\n",
      "Loss: 0.4076162278652191\n",
      "Loss: 0.38750120997428894\n",
      "Loss: 0.4581831991672516\n",
      "Loss: 0.314550518989563\n",
      "Loss: 0.4606264531612396\n",
      "Loss: 0.8533730506896973\n",
      "Loss: 0.481294721364975\n",
      "Loss: 0.4669733941555023\n",
      "Loss: 0.3704048991203308\n",
      "Loss: 0.30095744132995605\n",
      "Loss: 0.36283546686172485\n",
      "Loss: 0.4916638433933258\n",
      "Loss: 0.46317288279533386\n",
      "Loss: 0.5540562868118286\n",
      "Loss: 0.31050166487693787\n",
      "Loss: 0.33536961674690247\n",
      "Loss: 0.39138373732566833\n",
      "Loss: 0.3652373254299164\n",
      "Loss: 0.583441972732544\n",
      "Loss: 0.5821521878242493\n",
      "Loss: 0.40729737281799316\n",
      "Loss: 0.7052429914474487\n",
      "Loss: 0.49537262320518494\n",
      "Loss: 0.3460251986980438\n",
      "Loss: 0.4377860128879547\n",
      "Loss: 0.48024094104766846\n",
      "Loss: 0.46096205711364746\n",
      "Loss: 0.3840861916542053\n",
      "Loss: 0.3241346776485443\n",
      "Loss: 0.4443424344062805\n",
      "Loss: 0.43607470393180847\n",
      "Loss: 0.35018524527549744\n",
      "Loss: 0.46802619099617004\n",
      "Loss: 0.21762889623641968\n",
      "Loss: 0.4341937303543091\n",
      "Loss: 0.15469904243946075\n",
      "Loss: 0.47557076811790466\n",
      "Loss: 0.3566398620605469\n",
      "Loss: 0.4755840599536896\n",
      "Loss: 0.5555479526519775\n",
      "Loss: 0.5324215292930603\n",
      "Loss: 0.4952806532382965\n",
      "Loss: 0.46807661652565\n",
      "Loss: 0.36300647258758545\n",
      "Loss: 0.45424097776412964\n",
      "Loss: 0.4020027220249176\n",
      "Loss: 0.28290215134620667\n",
      "Loss: 0.42405804991722107\n",
      "Loss: 0.7872605323791504\n",
      "Loss: 0.6391544938087463\n",
      "Loss: 0.32940739393234253\n",
      "Loss: 0.45038098096847534\n",
      "Loss: 0.41112515330314636\n",
      "Loss: 0.21813362836837769\n",
      "Loss: 0.22409771382808685\n",
      "Loss: 0.5152426362037659\n",
      "Loss: 0.4176637530326843\n",
      "Loss: 0.4961283504962921\n",
      "Loss: 0.4463369846343994\n",
      "Loss: 0.4127667546272278\n",
      "Loss: 0.3025690019130707\n",
      "Loss: 0.3591056168079376\n",
      "Loss: 0.4588145315647125\n",
      "Loss: 0.4000878930091858\n",
      "Loss: 0.41823360323905945\n",
      "Loss: 0.14257292449474335\n",
      "Loss: 0.45560091733932495\n",
      "Loss: 0.5759565830230713\n",
      "Loss: 0.40462982654571533\n",
      "Loss: 0.49299418926239014\n",
      "Loss: 0.8363317847251892\n",
      "Loss: 0.5635003447532654\n",
      "Loss: 0.36012694239616394\n",
      "Loss: 0.22347904741764069\n",
      "Loss: 0.3778175115585327\n",
      "Loss: 0.44259968400001526\n",
      "Loss: 0.41489359736442566\n",
      "Loss: 0.5061401724815369\n",
      "Loss: 0.5065372586250305\n",
      "Loss: 0.31395500898361206\n",
      "Loss: 0.4765271246433258\n",
      "Loss: 0.44612956047058105\n",
      "Loss: 0.43489205837249756\n",
      "Loss: 0.31854987144470215\n",
      "Loss: 0.4451560080051422\n",
      "Loss: 0.44402435421943665\n",
      "Loss: 0.5684199333190918\n",
      "Loss: 0.7279978394508362\n",
      "Loss: 0.40266314148902893\n",
      "Loss: 0.6069982051849365\n",
      "Loss: 0.31023022532463074\n",
      "Loss: 0.40791964530944824\n",
      "Loss: 0.29429274797439575\n",
      "Loss: 0.4697592258453369\n",
      "Loss: 0.3212491273880005\n",
      "Loss: 0.45503121614456177\n",
      "Loss: 0.24367734789848328\n",
      "Loss: 0.3623313009738922\n",
      "Loss: 0.3668537437915802\n",
      "Loss: 0.2794131338596344\n",
      "Loss: 0.2078074961900711\n",
      "Loss: 0.36818721890449524\n",
      "Loss: 0.47803306579589844\n",
      "Loss: 0.4220079481601715\n",
      "Loss: 0.4078079164028168\n",
      "Loss: 0.31788045167922974\n",
      "Loss: 0.5075189471244812\n",
      "Loss: 0.3837253153324127\n",
      "Loss: 0.5985199809074402\n",
      "Loss: 0.5425974130630493\n",
      "Loss: 0.5137032866477966\n",
      "Loss: 0.4663788080215454\n",
      "Loss: 0.4231932461261749\n",
      "Loss: 0.46041786670684814\n",
      "Loss: 0.36570122838020325\n",
      "Loss: 0.3595444858074188\n",
      "Loss: 0.33302637934684753\n",
      "Loss: 0.509028971195221\n",
      "Loss: 0.5322522521018982\n",
      "Loss: 0.3436441421508789\n",
      "Loss: 0.7603696584701538\n",
      "Loss: 0.3694688379764557\n",
      "Loss: 0.27263888716697693\n",
      "Loss: 0.3454197347164154\n",
      "Loss: 0.25673729181289673\n",
      "Loss: 0.28418347239494324\n",
      "Loss: 0.25695544481277466\n",
      "Loss: 0.5461351871490479\n",
      "Loss: 0.4447903037071228\n",
      "Loss: 0.42090240120887756\n",
      "Loss: 0.2972171902656555\n",
      "Loss: 0.23321667313575745\n",
      "Loss: 0.701139509677887\n",
      "Loss: 0.37998902797698975\n",
      "Loss: 0.5297912359237671\n",
      "Loss: 0.39316990971565247\n",
      "Loss: 0.4575173854827881\n",
      "Loss: 0.2835749685764313\n",
      "Loss: 0.3286356031894684\n",
      "Loss: 0.3849337697029114\n",
      "Loss: 0.31750771403312683\n",
      "Loss: 0.4634997844696045\n",
      "Loss: 0.42445141077041626\n",
      "Loss: 0.6384460926055908\n",
      "Loss: 0.4599432647228241\n",
      "Loss: 0.28528454899787903\n",
      "Loss: 0.5997771620750427\n",
      "Loss: 0.37725529074668884\n",
      "Loss: 0.5259813070297241\n",
      "Loss: 0.2998313009738922\n",
      "Loss: 0.5831009149551392\n",
      "Loss: 0.34491631388664246\n",
      "Loss: 0.4582120478153229\n",
      "Loss: 0.32299891114234924\n",
      "Loss: 0.281192809343338\n",
      "Loss: 0.3482423424720764\n",
      "Loss: 0.5898606181144714\n",
      "Loss: 0.3433243930339813\n",
      "Loss: 0.4581802785396576\n",
      "Loss: 0.3954424560070038\n",
      "Loss: 0.6072186827659607\n",
      "Loss: 0.36341574788093567\n",
      "Loss: 0.3566247522830963\n",
      "Loss: 0.37467625737190247\n",
      "Loss: 0.46117526292800903\n",
      "Loss: 0.731900155544281\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9777066707611084\n",
      "Loss: 2.792482376098633\n",
      "Loss: 2.452669620513916\n",
      "Loss: 2.656148910522461\n",
      "Loss: 2.522226095199585\n",
      "Loss: 2.391789197921753\n",
      "Loss: 2.2834012508392334\n",
      "Loss: 2.2822084426879883\n",
      "Loss: 2.2742981910705566\n",
      "Loss: 2.2216081619262695\n",
      "Loss: 2.1191587448120117\n",
      "Loss: 2.122025966644287\n",
      "Loss: 2.1486589908599854\n",
      "Loss: 2.210784912109375\n",
      "Loss: 2.3145430088043213\n",
      "Loss: 2.047504186630249\n",
      "Loss: 2.0910446643829346\n",
      "Loss: 2.0555498600006104\n",
      "Loss: 2.127573013305664\n",
      "Loss: 2.1899008750915527\n",
      "Loss: 2.011915922164917\n",
      "Loss: 2.0761187076568604\n",
      "Loss: 2.0185554027557373\n",
      "Loss: 1.9213013648986816\n",
      "Loss: 1.9938369989395142\n",
      "Loss: 1.8620680570602417\n",
      "Loss: 1.8784730434417725\n",
      "Loss: 1.9026424884796143\n",
      "Loss: 1.8876065015792847\n",
      "Loss: 1.908701777458191\n",
      "Loss: 1.9440159797668457\n",
      "Loss: 1.9917231798171997\n",
      "Loss: 1.813230037689209\n",
      "Loss: 1.7259515523910522\n",
      "Loss: 1.8802047967910767\n",
      "Loss: 1.721096396446228\n",
      "Loss: 1.8942500352859497\n",
      "Loss: 1.8478379249572754\n",
      "Loss: 1.7724220752716064\n",
      "Loss: 1.7721017599105835\n",
      "Loss: 1.763889193534851\n",
      "Loss: 1.7913693189620972\n",
      "Loss: 1.8921754360198975\n",
      "Loss: 1.6092100143432617\n",
      "Loss: 1.672441005706787\n",
      "Loss: 1.6246594190597534\n",
      "Loss: 1.905788540840149\n",
      "Loss: 1.7649933099746704\n",
      "Loss: 1.513826608657837\n",
      "Loss: 1.5371901988983154\n",
      "Loss: 1.5667766332626343\n",
      "Loss: 1.7239964008331299\n",
      "Loss: 1.4956119060516357\n",
      "Loss: 1.5505257844924927\n",
      "Loss: 1.6743320226669312\n",
      "Loss: 1.671308994293213\n",
      "Loss: 1.5102424621582031\n",
      "Loss: 1.5410401821136475\n",
      "Loss: 1.5079872608184814\n",
      "Loss: 1.6494189500808716\n",
      "Loss: 1.6465572118759155\n",
      "Loss: 1.4080318212509155\n",
      "Loss: 1.5470843315124512\n",
      "Loss: 1.586023211479187\n",
      "Loss: 1.506582498550415\n",
      "Loss: 1.5698703527450562\n",
      "Loss: 1.602696180343628\n",
      "Loss: 1.5818618535995483\n",
      "Loss: 1.7322152853012085\n",
      "Loss: 1.6592618227005005\n",
      "Loss: 1.4106143712997437\n",
      "Loss: 1.3768500089645386\n",
      "Loss: 1.5263845920562744\n",
      "Loss: 1.4351261854171753\n",
      "Loss: 1.4914768934249878\n",
      "Loss: 1.6848349571228027\n",
      "Loss: 1.490472674369812\n",
      "Loss: 1.2736470699310303\n",
      "Loss: 1.377182960510254\n",
      "Loss: 1.5163780450820923\n",
      "Loss: 1.3069912195205688\n",
      "Loss: 1.5533726215362549\n",
      "Loss: 1.345947265625\n",
      "Loss: 1.4906213283538818\n",
      "Loss: 1.226448655128479\n",
      "Loss: 1.5427923202514648\n",
      "Loss: 1.479610562324524\n",
      "Loss: 1.594264030456543\n",
      "Loss: 1.5756199359893799\n",
      "Loss: 1.6344375610351562\n",
      "Loss: 1.5122390985488892\n",
      "Loss: 1.4261868000030518\n",
      "Loss: 1.4059187173843384\n",
      "Loss: 1.383071780204773\n",
      "Loss: 1.4615103006362915\n",
      "Loss: 1.480456829071045\n",
      "Loss: 1.5614169836044312\n",
      "Loss: 1.4787036180496216\n",
      "Loss: 1.283258080482483\n",
      "Loss: 1.3173484802246094\n",
      "Loss: 1.500016689300537\n",
      "Loss: 1.39546799659729\n",
      "Loss: 1.504520297050476\n",
      "Loss: 1.2990052700042725\n",
      "Loss: 1.142398476600647\n",
      "Loss: 1.4673590660095215\n",
      "Loss: 1.4923577308654785\n",
      "Loss: 1.3027993440628052\n",
      "Loss: 1.369747519493103\n",
      "Loss: 1.284807562828064\n",
      "Loss: 1.244421362876892\n",
      "Loss: 1.5391901731491089\n",
      "Loss: 1.4973210096359253\n",
      "Loss: 1.3980294466018677\n",
      "Loss: 1.3989732265472412\n",
      "Loss: 1.190851092338562\n",
      "Loss: 1.2329394817352295\n",
      "Loss: 1.3500032424926758\n",
      "Loss: 1.5845621824264526\n",
      "Loss: 1.467763066291809\n",
      "Loss: 1.4934418201446533\n",
      "Loss: 1.284417748451233\n",
      "Loss: 1.3946425914764404\n",
      "Loss: 1.3212298154830933\n",
      "Loss: 1.1861244440078735\n",
      "Loss: 1.375133991241455\n",
      "Loss: 1.2996766567230225\n",
      "Loss: 1.2575547695159912\n",
      "Loss: 1.3138972520828247\n",
      "Loss: 1.1928951740264893\n",
      "Loss: 1.367690086364746\n",
      "Loss: 1.4920105934143066\n",
      "Loss: 1.383026123046875\n",
      "Loss: 1.321761965751648\n",
      "Loss: 1.3048242330551147\n",
      "Loss: 1.5252200365066528\n",
      "Loss: 1.3681482076644897\n",
      "Loss: 1.3463447093963623\n",
      "Loss: 1.3846174478530884\n",
      "Loss: 1.11713445186615\n",
      "Loss: 1.7433059215545654\n",
      "Loss: 1.4039617776870728\n",
      "Loss: 1.1370381116867065\n",
      "Loss: 1.4438129663467407\n",
      "Loss: 1.2952640056610107\n",
      "Loss: 1.2865197658538818\n",
      "Loss: 1.125109314918518\n",
      "Loss: 1.4547080993652344\n",
      "Loss: 1.3300634622573853\n",
      "Loss: 1.3268812894821167\n",
      "Loss: 1.4891406297683716\n",
      "Loss: 1.192428469657898\n",
      "Loss: 1.3590705394744873\n",
      "Loss: 1.169802188873291\n",
      "Loss: 1.284949541091919\n",
      "Loss: 1.053000807762146\n",
      "Loss: 1.1742310523986816\n",
      "Loss: 1.4645620584487915\n",
      "Loss: 1.2132368087768555\n",
      "Loss: 1.2680771350860596\n",
      "Loss: 1.2319884300231934\n",
      "Loss: 1.3235336542129517\n",
      "Loss: 1.472190499305725\n",
      "Loss: 1.5366085767745972\n",
      "Loss: 1.1798900365829468\n",
      "Loss: 1.6151602268218994\n",
      "Loss: 1.3864210844039917\n",
      "Loss: 1.3162280321121216\n",
      "Loss: 1.4310896396636963\n",
      "Loss: 1.5093597173690796\n",
      "Loss: 1.1923805475234985\n",
      "Loss: 1.4701042175292969\n",
      "Loss: 1.2093640565872192\n",
      "Loss: 1.401336669921875\n",
      "Loss: 1.3496001958847046\n",
      "Loss: 1.1959196329116821\n",
      "Loss: 1.4253458976745605\n",
      "Loss: 1.329757809638977\n",
      "Loss: 1.546545386314392\n",
      "Loss: 1.4575603008270264\n",
      "Loss: 1.0115894079208374\n",
      "Loss: 1.1805363893508911\n",
      "Loss: 1.3564540147781372\n",
      "Loss: 1.322853446006775\n",
      "Loss: 1.4418120384216309\n",
      "Loss: 1.0780967473983765\n",
      "Loss: 1.3639689683914185\n",
      "Loss: 1.4778248071670532\n",
      "Loss: 1.3967318534851074\n",
      "Loss: 1.1938436031341553\n",
      "Loss: 1.174276351928711\n",
      "Loss: 1.4125332832336426\n",
      "Loss: 1.383103609085083\n",
      "Loss: 1.1556776762008667\n",
      "Loss: 1.425693154335022\n",
      "Loss: 1.2942711114883423\n",
      "Loss: 1.2977967262268066\n",
      "Loss: 1.260299801826477\n",
      "Loss: 1.2725249528884888\n",
      "Loss: 1.3726657629013062\n",
      "Loss: 1.2069993019104004\n",
      "Loss: 1.6385785341262817\n",
      "Loss: 1.3659179210662842\n",
      "Loss: 1.6117219924926758\n",
      "Loss: 1.5347533226013184\n",
      "Loss: 1.2430988550186157\n",
      "Loss: 1.6300166845321655\n",
      "Loss: 1.1958962678909302\n",
      "Loss: 1.3015354871749878\n",
      "Loss: 1.3590283393859863\n",
      "Loss: 1.5241763591766357\n",
      "Loss: 1.3120418787002563\n",
      "Loss: 1.4353070259094238\n",
      "Loss: 1.3558056354522705\n",
      "Loss: 1.143592119216919\n",
      "Loss: 1.3256709575653076\n",
      "Loss: 1.4289394617080688\n",
      "Loss: 1.3452562093734741\n",
      "Loss: 1.376848578453064\n",
      "Loss: 1.346071481704712\n",
      "Loss: 1.2059367895126343\n",
      "Loss: 1.0461225509643555\n",
      "Loss: 1.2613142728805542\n",
      "Loss: 1.4210761785507202\n",
      "Loss: 1.2931941747665405\n",
      "Loss: 1.3775396347045898\n",
      "Loss: 1.5169494152069092\n",
      "Loss: 1.0337631702423096\n",
      "Loss: 1.4698632955551147\n",
      "Loss: 1.2326656579971313\n",
      "Loss: 1.2038911581039429\n",
      "Loss: 1.3491076231002808\n",
      "Loss: 1.1815544366836548\n",
      "Loss: 1.392398476600647\n",
      "Loss: 1.3445184230804443\n",
      "Loss: 1.2390271425247192\n",
      "Loss: 1.3487359285354614\n",
      "Loss: 1.2648017406463623\n",
      "Loss: 1.3474541902542114\n",
      "Loss: 1.316311240196228\n",
      "Loss: 1.2907838821411133\n",
      "Loss: 1.202938437461853\n",
      "Loss: 1.3316541910171509\n",
      "Loss: 1.4120323657989502\n",
      "Loss: 1.2462317943572998\n",
      "Loss: 1.321657657623291\n",
      "Loss: 1.4090598821640015\n",
      "Loss: 1.5524638891220093\n",
      "Loss: 1.1544315814971924\n",
      "Loss: 1.1503679752349854\n",
      "Loss: 1.3442634344100952\n",
      "Loss: 1.375566005706787\n",
      "Loss: 1.654871940612793\n",
      "Loss: 1.4196581840515137\n",
      "Loss: 1.5351811647415161\n",
      "Loss: 1.561364769935608\n",
      "Loss: 1.3327168226242065\n",
      "Loss: 1.3606398105621338\n",
      "Loss: 1.5475540161132812\n",
      "Loss: 1.375380039215088\n",
      "Loss: 1.4188042879104614\n",
      "Loss: 1.3110889196395874\n",
      "Loss: 1.474971890449524\n",
      "Loss: 1.1877789497375488\n",
      "Loss: 1.084984302520752\n",
      "Loss: 1.2557001113891602\n",
      "Loss: 1.6124435663223267\n",
      "Loss: 1.2789599895477295\n",
      "Loss: 1.3130453824996948\n",
      "Loss: 1.281510829925537\n",
      "Loss: 1.3291897773742676\n",
      "Loss: 1.1496561765670776\n",
      "Loss: 1.4229434728622437\n",
      "Loss: 1.1757216453552246\n",
      "Loss: 1.4181667566299438\n",
      "Loss: 1.4734246730804443\n",
      "Loss: 1.1844478845596313\n",
      "Loss: 1.243757724761963\n",
      "Loss: 1.1059019565582275\n",
      "Loss: 1.282831072807312\n",
      "Loss: 1.4542739391326904\n",
      "Loss: 1.4975110292434692\n",
      "Loss: 1.3163707256317139\n",
      "Loss: 1.1259303092956543\n",
      "Loss: 1.1389633417129517\n",
      "Loss: 1.526685357093811\n",
      "Loss: 1.11582350730896\n",
      "Loss: 1.3695847988128662\n",
      "Loss: 1.3834114074707031\n",
      "Loss: 1.3122433423995972\n",
      "Loss: 1.1435401439666748\n",
      "Loss: 1.3509702682495117\n",
      "Loss: 1.1574305295944214\n",
      "Loss: 1.3780195713043213\n",
      "Loss: 1.487282156944275\n",
      "Loss: 1.296942114830017\n",
      "Loss: 1.2412563562393188\n",
      "Loss: 1.3791584968566895\n",
      "Loss: 1.1887133121490479\n",
      "Loss: 1.2746477127075195\n",
      "Loss: 1.2294033765792847\n",
      "Loss: 1.4070430994033813\n",
      "Loss: 1.0676500797271729\n",
      "Loss: 1.2913345098495483\n",
      "Loss: 1.5489729642868042\n",
      "Loss: 1.286277174949646\n",
      "Loss: 1.2044144868850708\n",
      "Loss: 1.236783504486084\n",
      "Loss: 1.283492922782898\n",
      "Loss: 1.4145814180374146\n",
      "Loss: 1.0412325859069824\n",
      "Loss: 1.4474172592163086\n",
      "Loss: 1.2728192806243896\n",
      "Loss: 1.344260811805725\n",
      "Loss: 1.5947340726852417\n",
      "Loss: 1.409796118736267\n",
      "Loss: 1.1565942764282227\n",
      "Loss: 1.2183367013931274\n",
      "Loss: 1.2246075868606567\n",
      "Loss: 1.2131609916687012\n",
      "Loss: 1.4766106605529785\n",
      "Loss: 1.1332614421844482\n",
      "Loss: 1.1492823362350464\n",
      "Loss: 1.4906631708145142\n",
      "Loss: 1.4659647941589355\n",
      "Loss: 1.1974838972091675\n",
      "Loss: 1.14130437374115\n",
      "Loss: 1.2504616975784302\n",
      "Loss: 1.3173584938049316\n",
      "Loss: 1.3804091215133667\n",
      "Loss: 1.2964494228363037\n",
      "Loss: 1.1225850582122803\n",
      "Loss: 1.275998592376709\n",
      "Loss: 1.295884132385254\n",
      "Loss: 1.2129226922988892\n",
      "Loss: 1.7100621461868286\n",
      "Loss: 1.4293311834335327\n",
      "Loss: 1.3966255187988281\n",
      "Loss: 1.3614283800125122\n",
      "Loss: 1.1738547086715698\n",
      "Loss: 1.2026150226593018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2464197874069214\n",
      "Loss: 1.5268865823745728\n",
      "Loss: 1.0447982549667358\n",
      "Loss: 1.1179317235946655\n",
      "Loss: 1.5214200019836426\n",
      "Loss: 1.4846481084823608\n",
      "Loss: 1.2078003883361816\n",
      "Loss: 1.3247840404510498\n",
      "Loss: 1.2816874980926514\n",
      "Loss: 1.2159488201141357\n",
      "Loss: 1.3111481666564941\n",
      "Loss: 1.355787992477417\n",
      "Loss: 1.2426345348358154\n",
      "Loss: 1.4561281204223633\n",
      "Loss: 1.4086788892745972\n",
      "Loss: 1.1907093524932861\n",
      "Loss: 1.4026079177856445\n",
      "Loss: 1.1233417987823486\n",
      "Loss: 1.271918773651123\n",
      "Loss: 1.2502374649047852\n",
      "Loss: 1.3481303453445435\n",
      "Loss: 1.5370103120803833\n",
      "Loss: 1.1817386150360107\n",
      "Loss: 1.5656694173812866\n",
      "Loss: 1.1446104049682617\n",
      "Loss: 1.1795508861541748\n",
      "Loss: 1.3387999534606934\n",
      "Loss: 1.0111130475997925\n",
      "Loss: 1.149440884590149\n",
      "Loss: 1.4036558866500854\n",
      "Loss: 1.3407385349273682\n",
      "Loss: 1.4758350849151611\n",
      "Loss: 1.2943012714385986\n",
      "Loss: 1.3095871210098267\n",
      "Loss: 1.2404696941375732\n",
      "Loss: 1.2576706409454346\n",
      "Loss: 1.2606594562530518\n",
      "Loss: 1.2611839771270752\n",
      "Loss: 1.2416951656341553\n",
      "Loss: 1.3786442279815674\n",
      "Loss: 1.2249088287353516\n",
      "Loss: 1.693422555923462\n",
      "Loss: 1.2537957429885864\n",
      "Loss: 1.0774976015090942\n",
      "Loss: 1.3082445859909058\n",
      "Loss: 1.6957669258117676\n",
      "Loss: 1.3881006240844727\n",
      "Loss: 1.1389787197113037\n",
      "Loss: 1.2690857648849487\n",
      "Loss: 1.2924765348434448\n",
      "Loss: 1.2732195854187012\n",
      "Loss: 1.3783135414123535\n",
      "Loss: 1.1578071117401123\n",
      "Loss: 1.2729986906051636\n",
      "Loss: 1.2044785022735596\n",
      "Loss: 1.6105166673660278\n",
      "Loss: 1.2413183450698853\n",
      "Loss: 1.2763643264770508\n",
      "Loss: 1.3232662677764893\n",
      "Loss: 1.3488343954086304\n",
      "Loss: 1.3592774868011475\n",
      "Loss: 1.1686570644378662\n",
      "Loss: 1.3567614555358887\n",
      "Loss: 0.972465455532074\n",
      "Loss: 1.3815529346466064\n",
      "Loss: 1.2685894966125488\n",
      "Loss: 1.516892433166504\n",
      "Loss: 0.9735252261161804\n",
      "Loss: 1.1969845294952393\n",
      "Loss: 1.527778148651123\n",
      "Loss: 1.4620269536972046\n",
      "Loss: 1.2142021656036377\n",
      "Loss: 1.3198715448379517\n",
      "Loss: 1.1401361227035522\n",
      "Loss: 1.440412998199463\n",
      "Loss: 1.3021124601364136\n",
      "Loss: 1.0407639741897583\n",
      "Loss: 1.4447875022888184\n",
      "Loss: 1.5229394435882568\n",
      "Loss: 1.2943857908248901\n",
      "Loss: 1.145472526550293\n",
      "Loss: 1.2881975173950195\n",
      "Loss: 1.2497212886810303\n",
      "Loss: 1.4801247119903564\n",
      "Loss: 1.471254825592041\n",
      "Loss: 1.3534903526306152\n",
      "Loss: 1.306382179260254\n",
      "Loss: 1.1106775999069214\n",
      "Loss: 1.3545576333999634\n",
      "Loss: 1.197367787361145\n",
      "Loss: 1.399017333984375\n",
      "Loss: 1.405777096748352\n",
      "Loss: 1.273564338684082\n",
      "Loss: 1.2077436447143555\n",
      "Loss: 1.21107816696167\n",
      "Loss: 1.6977577209472656\n",
      "Loss: 1.4204564094543457\n",
      "Loss: 1.3960223197937012\n",
      "Loss: 1.0763294696807861\n",
      "Loss: 1.6562833786010742\n",
      "Loss: 1.2713196277618408\n",
      "Loss: 1.5701539516448975\n",
      "Loss: 1.3118958473205566\n",
      "Loss: 1.5554640293121338\n",
      "Loss: 1.1938724517822266\n",
      "Loss: 1.5018643140792847\n",
      "Loss: 0.9862101078033447\n",
      "Loss: 1.2104026079177856\n",
      "Loss: 1.3606562614440918\n",
      "Loss: 1.3527519702911377\n",
      "Loss: 1.0234042406082153\n",
      "Loss: 1.6110926866531372\n",
      "Loss: 1.3655052185058594\n",
      "Loss: 1.4177418947219849\n",
      "Loss: 1.2504290342330933\n",
      "Loss: 1.281058669090271\n",
      "Loss: 1.4908162355422974\n",
      "Loss: 1.1582021713256836\n",
      "Loss: 1.3802497386932373\n",
      "Loss: 1.7428933382034302\n",
      "Loss: 1.3740097284317017\n",
      "Loss: 1.350883960723877\n",
      "Loss: 1.3555853366851807\n",
      "Loss: 1.4387869834899902\n",
      "Loss: 1.3497406244277954\n",
      "Loss: 1.4404776096343994\n",
      "Loss: 1.4294086694717407\n",
      "Loss: 1.3994793891906738\n",
      "Loss: 1.1176207065582275\n",
      "Loss: 1.2662333250045776\n",
      "Loss: 1.3188018798828125\n",
      "Loss: 1.5676251649856567\n",
      "Loss: 1.25221586227417\n",
      "Loss: 1.331807255744934\n",
      "Loss: 1.309208869934082\n",
      "Loss: 1.1133078336715698\n",
      "Loss: 1.2093943357467651\n",
      "Loss: 1.5013513565063477\n",
      "Loss: 1.4485805034637451\n",
      "Loss: 1.3317729234695435\n",
      "Loss: 1.325287938117981\n",
      "Loss: 1.2816686630249023\n",
      "Loss: 1.417911171913147\n",
      "Loss: 1.1697986125946045\n",
      "Loss: 1.0340372323989868\n",
      "Loss: 1.115986704826355\n",
      "Loss: 1.1329361200332642\n",
      "Loss: 1.5701605081558228\n",
      "Loss: 1.5115817785263062\n",
      "Loss: 0.9467302560806274\n",
      "Loss: 1.4670116901397705\n",
      "Loss: 1.727606177330017\n",
      "Loss: 1.234515905380249\n",
      "Loss: 1.3729747533798218\n",
      "Loss: 1.5465214252471924\n",
      "Loss: 1.481189250946045\n",
      "Loss: 1.371385097503662\n",
      "Loss: 1.5628552436828613\n",
      "Loss: 1.493166446685791\n",
      "Loss: 1.266355037689209\n",
      "Loss: 1.3681952953338623\n",
      "Loss: 1.3724770545959473\n",
      "Loss: 1.2958984375\n",
      "Loss: 1.341172456741333\n",
      "Loss: 1.175614356994629\n",
      "Loss: 1.266440987586975\n",
      "Loss: 1.4407055377960205\n",
      "Loss: 1.2259913682937622\n",
      "Loss: 1.2606842517852783\n",
      "Loss: 1.4258853197097778\n",
      "Loss: 1.3701539039611816\n",
      "Loss: 1.1784923076629639\n",
      "Loss: 1.2767226696014404\n",
      "Loss: 1.5774229764938354\n",
      "Loss: 1.3452231884002686\n",
      "Loss: 1.392154574394226\n",
      "Loss: 1.395392656326294\n",
      "Loss: 1.647821307182312\n",
      "Loss: 1.391106367111206\n",
      "Loss: 1.360924482345581\n",
      "Loss: 1.425182819366455\n",
      "Loss: 1.4191577434539795\n",
      "Loss: 1.2667592763900757\n",
      "Loss: 1.5452256202697754\n",
      "Loss: 1.232809066772461\n",
      "Loss: 1.1836203336715698\n",
      "Loss: 1.1305625438690186\n",
      "Loss: 1.2146278619766235\n",
      "Loss: 1.3588039875030518\n",
      "Loss: 1.294344425201416\n",
      "Loss: 1.1861323118209839\n",
      "Loss: 1.4416519403457642\n",
      "Loss: 1.92087984085083\n",
      "Loss: 1.22208571434021\n",
      "Loss: 1.1762897968292236\n",
      "Loss: 1.114728331565857\n",
      "Loss: 1.2011231184005737\n",
      "Loss: 1.396979570388794\n",
      "Loss: 1.40168035030365\n",
      "Loss: 1.0665974617004395\n",
      "Loss: 1.3867791891098022\n",
      "Loss: 1.3398354053497314\n",
      "Loss: 1.3459445238113403\n",
      "Loss: 1.2517892122268677\n",
      "Loss: 1.4088581800460815\n",
      "Loss: 1.3994723558425903\n",
      "Loss: 1.1834977865219116\n",
      "Loss: 1.3385205268859863\n",
      "Loss: 1.2256064414978027\n",
      "Loss: 1.4843811988830566\n",
      "Loss: 1.2477341890335083\n",
      "Loss: 1.2542048692703247\n",
      "Loss: 1.2841293811798096\n",
      "Loss: 1.3095680475234985\n",
      "Loss: 1.2474831342697144\n",
      "Loss: 1.551666259765625\n",
      "Loss: 1.1811431646347046\n",
      "Loss: 1.3064161539077759\n",
      "Loss: 1.3045532703399658\n",
      "Loss: 1.151222825050354\n",
      "Loss: 1.2664893865585327\n",
      "Loss: 1.1440316438674927\n",
      "Loss: 1.2265467643737793\n",
      "Loss: 1.490066409111023\n",
      "Loss: 1.2829350233078003\n",
      "Loss: 1.2001458406448364\n",
      "Loss: 1.1043200492858887\n",
      "Loss: 1.1664330959320068\n",
      "Loss: 1.3135865926742554\n",
      "Loss: 1.3732143640518188\n",
      "Loss: 1.294883131980896\n",
      "Loss: 1.6108795404434204\n",
      "Loss: 1.1686118841171265\n",
      "Loss: 1.1258306503295898\n",
      "Loss: 1.4538183212280273\n",
      "Loss: 1.2606663703918457\n",
      "Loss: 1.2080727815628052\n",
      "Loss: 1.3664528131484985\n",
      "Loss: 1.449884057044983\n",
      "Loss: 1.060133695602417\n",
      "Loss: 1.3724077939987183\n",
      "Loss: 1.4078861474990845\n",
      "Loss: 1.4503931999206543\n",
      "Loss: 1.3004794120788574\n",
      "Loss: 1.2263469696044922\n",
      "Loss: 1.1860857009887695\n",
      "Loss: 1.3519359827041626\n",
      "Loss: 1.6627120971679688\n",
      "Loss: 1.2475838661193848\n",
      "Loss: 1.3615710735321045\n",
      "Loss: 1.3489141464233398\n",
      "Loss: 1.192206859588623\n",
      "Loss: 1.1914840936660767\n",
      "Loss: 1.319625735282898\n",
      "Loss: 1.4179236888885498\n",
      "Loss: 1.5739282369613647\n",
      "Loss: 1.3753160238265991\n",
      "Loss: 1.5159525871276855\n",
      "Loss: 1.3236445188522339\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        X[X < 0] *= a\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.5792946815490723\n",
      "tensor([-0.1000])\n",
      "Loss: 2.3308827877044678\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.6838])\n",
      "Loss: 2.481755256652832\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.4184])\n",
      "Loss: 2.5653727054595947\n",
      "tensor(1.00000e-02 *\n",
      "       [-9.2078])\n",
      "Loss: 2.589242935180664\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.9768])\n",
      "Loss: 2.3537137508392334\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.7175])\n",
      "Loss: 2.3711869716644287\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.5124])\n",
      "Loss: 2.3142757415771484\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.3062])\n",
      "Loss: 2.1950597763061523\n",
      "tensor(1.00000e-02 *\n",
      "       [-8.0801])\n",
      "Loss: 2.3509299755096436\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.8741])\n",
      "Loss: 2.371683359146118\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.6462])\n",
      "Loss: 2.213571548461914\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.4286])\n",
      "Loss: 2.317676067352295\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.2300])\n",
      "Loss: 2.364468574523926\n",
      "tensor(1.00000e-02 *\n",
      "       [-7.0300])\n",
      "Loss: 2.41117000579834\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.8315])\n",
      "Loss: 2.282320499420166\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.6379])\n",
      "Loss: 2.2846732139587402\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.4533])\n",
      "Loss: 2.2214603424072266\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.2760])\n",
      "Loss: 2.1818299293518066\n",
      "tensor(1.00000e-02 *\n",
      "       [-6.1105])\n",
      "Loss: 2.294886827468872\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.9509])\n",
      "Loss: 2.117271900177002\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.7936])\n",
      "Loss: 2.2490220069885254\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.6403])\n",
      "Loss: 2.114712715148926\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.4953])\n",
      "Loss: 2.1805877685546875\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.3546])\n",
      "Loss: 2.121554374694824\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.2132])\n",
      "Loss: 2.260960817337036\n",
      "tensor(1.00000e-02 *\n",
      "       [-5.0774])\n",
      "Loss: 2.055075168609619\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.9403])\n",
      "Loss: 2.1880784034729004\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.8036])\n",
      "Loss: 2.0958566665649414\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.6685])\n",
      "Loss: 2.2016100883483887\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.5375])\n",
      "Loss: 2.0956292152404785\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.4075])\n",
      "Loss: 2.013721227645874\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.2796])\n",
      "Loss: 2.090980291366577\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.1535])\n",
      "Loss: 2.155125141143799\n",
      "tensor(1.00000e-02 *\n",
      "       [-4.0264])\n",
      "Loss: 2.1436803340911865\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.8978])\n",
      "Loss: 2.011852979660034\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.7655])\n",
      "Loss: 1.9860508441925049\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.6287])\n",
      "Loss: 2.04670786857605\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.4908])\n",
      "Loss: 2.021902561187744\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.3540])\n",
      "Loss: 1.999411940574646\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.2254])\n",
      "Loss: 2.0120372772216797\n",
      "tensor(1.00000e-02 *\n",
      "       [-3.0978])\n",
      "Loss: 2.0754892826080322\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.9717])\n",
      "Loss: 1.9019627571105957\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.8479])\n",
      "Loss: 1.8945170640945435\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.7218])\n",
      "Loss: 1.9065334796905518\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5990])\n",
      "Loss: 1.8375787734985352\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.4783])\n",
      "Loss: 1.7843190431594849\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.3602])\n",
      "Loss: 1.8781747817993164\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.2459])\n",
      "Loss: 1.8764976263046265\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.1332])\n",
      "Loss: 1.969343662261963\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0243])\n",
      "Loss: 1.8788022994995117\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9144])\n",
      "Loss: 1.8474220037460327\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8033])\n",
      "Loss: 1.857499122619629\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6887])\n",
      "Loss: 1.8819791078567505\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5777])\n",
      "Loss: 1.7869457006454468\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4692])\n",
      "Loss: 1.7753866910934448\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3645])\n",
      "Loss: 1.817145586013794\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2600])\n",
      "Loss: 1.7292447090148926\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1569])\n",
      "Loss: 1.8332335948944092\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0552])\n",
      "Loss: 1.7085403203964233\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.5446])\n",
      "Loss: 1.6917225122451782\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5624])\n",
      "Loss: 1.7586451768875122\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.5977])\n",
      "Loss: 1.7763028144836426\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7207])\n",
      "Loss: 1.8402540683746338\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8751])\n",
      "Loss: 1.7393041849136353\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0286])\n",
      "Loss: 1.6277166604995728\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3044])\n",
      "Loss: 1.7065144777297974\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6325])\n",
      "Loss: 1.737908959388733\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0638])\n",
      "Loss: 1.8011407852172852\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4989])\n",
      "Loss: 1.6686508655548096\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0570])\n",
      "Loss: 1.974963665008545\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7491])\n",
      "Loss: 1.8717105388641357\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5648])\n",
      "Loss: 1.6012954711914062\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5858])\n",
      "Loss: 1.9043316841125488\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6689])\n",
      "Loss: 1.7275832891464233\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9760])\n",
      "Loss: 1.6908165216445923\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3085])\n",
      "Loss: 1.5944526195526123\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7813])\n",
      "Loss: 1.6584534645080566\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2574])\n",
      "Loss: 1.6885924339294434\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5506])\n",
      "Loss: 1.4920895099639893\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8213])\n",
      "Loss: 1.5708895921707153\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0583])\n",
      "Loss: 1.5917844772338867\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4890])\n",
      "Loss: 1.4691659212112427\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8256])\n",
      "Loss: 1.8947646617889404\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1294])\n",
      "Loss: 1.5012600421905518\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5587])\n",
      "Loss: 1.3438893556594849\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2910])\n",
      "Loss: 1.6066948175430298\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6208])\n",
      "Loss: 1.640981912612915\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.8693])\n",
      "Loss: 1.6630842685699463\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6515])\n",
      "Loss: 1.6632920503616333\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2036])\n",
      "Loss: 1.3726953268051147\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1154])\n",
      "Loss: 1.6709872484207153\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8056])\n",
      "Loss: 1.703546404838562\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2997])\n",
      "Loss: 1.679588794708252\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3624])\n",
      "Loss: 1.3967328071594238\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8649])\n",
      "Loss: 1.4017225503921509\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1944])\n",
      "Loss: 1.6345820426940918\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5185])\n",
      "Loss: 1.4596128463745117\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5874])\n",
      "Loss: 1.371667504310608\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9347])\n",
      "Loss: 1.883819580078125\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4824])\n",
      "Loss: 1.6493995189666748\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.3563])\n",
      "Loss: 1.6537444591522217\n",
      "tensor(1.00000e-04 *\n",
      "       [-1.9938])\n",
      "Loss: 1.5333547592163086\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.2135])\n",
      "Loss: 1.5491725206375122\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2153])\n",
      "Loss: 1.5044087171554565\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3312])\n",
      "Loss: 1.550165057182312\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2321])\n",
      "Loss: 1.5405702590942383\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4974])\n",
      "Loss: 1.519869089126587\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9988])\n",
      "Loss: 1.4430221319198608\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0378])\n",
      "Loss: 1.5753097534179688\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6301])\n",
      "Loss: 1.5029140710830688\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.1164])\n",
      "Loss: 1.5647348165512085\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.8803])\n",
      "Loss: 1.6423534154891968\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.9071])\n",
      "Loss: 1.4518367052078247\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.6315])\n",
      "Loss: 1.5838959217071533\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1597])\n",
      "Loss: 1.546298623085022\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6893])\n",
      "Loss: 1.3552531003952026\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0447])\n",
      "Loss: 1.4751969575881958\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1125])\n",
      "Loss: 1.4463425874710083\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5725])\n",
      "Loss: 1.4756845235824585\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.2409])\n",
      "Loss: 1.2589010000228882\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.7403])\n",
      "Loss: 1.6368458271026611\n",
      "tensor(1.00000e-04 *\n",
      "       [ 6.6933])\n",
      "Loss: 1.6218159198760986\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0588])\n",
      "Loss: 1.3627870082855225\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2462])\n",
      "Loss: 1.6781210899353027\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1395])\n",
      "Loss: 1.470522403717041\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0687])\n",
      "Loss: 1.3217068910598755\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7562])\n",
      "Loss: 1.449757695198059\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8307])\n",
      "Loss: 1.4784446954727173\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5085])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5037728548049927\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5916])\n",
      "Loss: 1.6913496255874634\n",
      "tensor(1.00000e-05 *\n",
      "       [-2.3056])\n",
      "Loss: 1.507453441619873\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.6297])\n",
      "Loss: 1.3879448175430298\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.1309])\n",
      "Loss: 1.25860595703125\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7784])\n",
      "Loss: 1.2854734659194946\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3396])\n",
      "Loss: 1.5204304456710815\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9642])\n",
      "Loss: 1.6559078693389893\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7613])\n",
      "Loss: 1.4377409219741821\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2473])\n",
      "Loss: 1.5431606769561768\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6358])\n",
      "Loss: 1.4942843914031982\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5953])\n",
      "Loss: 1.2114412784576416\n",
      "tensor(1.00000e-05 *\n",
      "       [ 4.0083])\n",
      "Loss: 1.555799126625061\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.2971])\n",
      "Loss: 1.639043927192688\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.0940])\n",
      "Loss: 1.461340069770813\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.8517])\n",
      "Loss: 1.4727301597595215\n",
      "tensor(1.00000e-04 *\n",
      "       [ 8.7314])\n",
      "Loss: 1.4086781740188599\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.4907])\n",
      "Loss: 1.476842999458313\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9754])\n",
      "Loss: 1.4790124893188477\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4888])\n",
      "Loss: 1.4452053308486938\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7034])\n",
      "Loss: 1.437567949295044\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3890])\n",
      "Loss: 1.5311956405639648\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5883])\n",
      "Loss: 1.3796237707138062\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1138])\n",
      "Loss: 1.306260108947754\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1367])\n",
      "Loss: 1.4726275205612183\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1443])\n",
      "Loss: 1.267372488975525\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0425])\n",
      "Loss: 1.5686748027801514\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0637])\n",
      "Loss: 1.5150456428527832\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.8580])\n",
      "Loss: 1.6389923095703125\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.4635])\n",
      "Loss: 1.3597276210784912\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0036])\n",
      "Loss: 1.4133578538894653\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1452])\n",
      "Loss: 1.482243299484253\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2768])\n",
      "Loss: 1.5906480550765991\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3048])\n",
      "Loss: 1.661582589149475\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5938])\n",
      "Loss: 1.4054808616638184\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3413])\n",
      "Loss: 1.1781727075576782\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3495])\n",
      "Loss: 1.708268165588379\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0798])\n",
      "Loss: 1.2818161249160767\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6809])\n",
      "Loss: 1.2819929122924805\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.1477])\n",
      "Loss: 1.522471308708191\n",
      "tensor(1.00000e-04 *\n",
      "       [ 3.3355])\n",
      "Loss: 1.4802566766738892\n",
      "tensor(1.00000e-05 *\n",
      "       [ 8.6794])\n",
      "Loss: 1.5002474784851074\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.2556])\n",
      "Loss: 1.2535912990570068\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7921])\n",
      "Loss: 1.436002492904663\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8905])\n",
      "Loss: 1.5482887029647827\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7917])\n",
      "Loss: 1.6409660577774048\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1189])\n",
      "Loss: 1.5346808433532715\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8177])\n",
      "Loss: 1.3748234510421753\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2495])\n",
      "Loss: 1.2431238889694214\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0524])\n",
      "Loss: 1.4592604637145996\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.7826])\n",
      "Loss: 1.4675465822219849\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.7171])\n",
      "Loss: 1.4453635215759277\n",
      "tensor(1.00000e-03 *\n",
      "       [ 1.0271])\n",
      "Loss: 1.3782074451446533\n",
      "tensor(1.00000e-04 *\n",
      "       [ 2.6290])\n",
      "Loss: 1.4213917255401611\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2228])\n",
      "Loss: 1.1729024648666382\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8891])\n",
      "Loss: 1.3370753526687622\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0951])\n",
      "Loss: 1.3259233236312866\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9609])\n",
      "Loss: 1.4180091619491577\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4058])\n",
      "Loss: 1.221599817276001\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1218])\n",
      "Loss: 1.4942693710327148\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2265])\n",
      "Loss: 1.3770697116851807\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4713])\n",
      "Loss: 1.4377185106277466\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9127])\n",
      "Loss: 1.0966333150863647\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4924])\n",
      "Loss: 1.2848018407821655\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5205])\n",
      "Loss: 1.5586494207382202\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7832])\n",
      "Loss: 1.1873458623886108\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5654])\n",
      "Loss: 1.3932468891143799\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.2298])\n",
      "Loss: 1.1717658042907715\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7642])\n",
      "Loss: 1.3313305377960205\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0172])\n",
      "Loss: 1.2532545328140259\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8257])\n",
      "Loss: 1.2409700155258179\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8414])\n",
      "Loss: 1.1723659038543701\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7069])\n",
      "Loss: 1.2397247552871704\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7412])\n",
      "Loss: 1.2214854955673218\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6282])\n",
      "Loss: 1.2586983442306519\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9737])\n",
      "Loss: 1.0865815877914429\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1363])\n",
      "Loss: 1.226686954498291\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0373])\n",
      "Loss: 1.3008270263671875\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4181])\n",
      "Loss: 1.2657550573349\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1681])\n",
      "Loss: 1.2283395528793335\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3695])\n",
      "Loss: 1.312325119972229\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3130])\n",
      "Loss: 1.3538367748260498\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5714])\n",
      "Loss: 1.2102073431015015\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4949])\n",
      "Loss: 1.3054102659225464\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6881])\n",
      "Loss: 1.334822654724121\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0700])\n",
      "Loss: 1.399265170097351\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3645])\n",
      "Loss: 1.3810735940933228\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9109])\n",
      "Loss: 1.5011725425720215\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3958])\n",
      "Loss: 1.5354461669921875\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8611])\n",
      "Loss: 1.2935749292373657\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5127])\n",
      "Loss: 1.3818963766098022\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1086])\n",
      "Loss: 1.1867440938949585\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3853])\n",
      "Loss: 1.4480111598968506\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.0877])\n",
      "Loss: 1.1796530485153198\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.5168])\n",
      "Loss: 1.3758854866027832\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8736])\n",
      "Loss: 1.3270701169967651\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5245])\n",
      "Loss: 1.2822479009628296\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5674])\n",
      "Loss: 1.4074928760528564\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8398])\n",
      "Loss: 1.2358585596084595\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3188])\n",
      "Loss: 1.171298861503601\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4296])\n",
      "Loss: 1.7033164501190186\n",
      "tensor(1.00000e-04 *\n",
      "       [ 1.5879])\n",
      "Loss: 1.066272258758545\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.2500])\n",
      "Loss: 1.2070497274398804\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9162])\n",
      "Loss: 1.1071110963821411\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6285])\n",
      "Loss: 1.4271563291549683\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9420])\n",
      "Loss: 1.3700053691864014\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.6709])\n",
      "Loss: 1.450219750404358\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0332])\n",
      "Loss: 1.4142565727233887\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8939])\n",
      "Loss: 1.351528525352478\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1496])\n",
      "Loss: 1.3600114583969116\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0413])\n",
      "Loss: 1.278544545173645\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5922])\n",
      "Loss: 1.1799558401107788\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3130])\n",
      "Loss: 1.326947808265686\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.2451])\n",
      "Loss: 1.3436781167984009\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.8894])\n",
      "Loss: 1.136277198791504\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5184])\n",
      "Loss: 1.363399624824524\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.9086])\n",
      "Loss: 1.2917667627334595\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9377])\n",
      "Loss: 1.1056256294250488\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8029])\n",
      "Loss: 1.4436322450637817\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1969])\n",
      "Loss: 1.3051303625106812\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1564])\n",
      "Loss: 1.3600581884384155\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4416])\n",
      "Loss: 1.2012503147125244\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1826])\n",
      "Loss: 1.3457356691360474\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8112])\n",
      "Loss: 1.2863054275512695\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.3900])\n",
      "Loss: 1.3444373607635498\n",
      "tensor(1.00000e-05 *\n",
      "       [ 4.6301])\n",
      "Loss: 1.3161813020706177\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.0379])\n",
      "Loss: 1.2265163660049438\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7229])\n",
      "Loss: 1.4741778373718262\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4190])\n",
      "Loss: 1.099381923675537\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9203])\n",
      "Loss: 1.341750144958496\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2203])\n",
      "Loss: 1.1734849214553833\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3882])\n",
      "Loss: 1.6767373085021973\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3268])\n",
      "Loss: 1.4517309665679932\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.9663])\n",
      "Loss: 1.3635112047195435\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.0130])\n",
      "Loss: 1.402957797050476\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5937])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.418965458869934\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.9503])\n",
      "Loss: 1.2321873903274536\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.8444])\n",
      "Loss: 1.2828412055969238\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4544])\n",
      "Loss: 1.1184210777282715\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9389])\n",
      "Loss: 1.3173259496688843\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4148])\n",
      "Loss: 1.347092866897583\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1435])\n",
      "Loss: 1.117366075515747\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.0195])\n",
      "Loss: 1.2592750787734985\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.7871])\n",
      "Loss: 1.405979037284851\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6560])\n",
      "Loss: 1.3261746168136597\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0883])\n",
      "Loss: 1.2883803844451904\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7143])\n",
      "Loss: 1.1273224353790283\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0334])\n",
      "Loss: 1.5498366355895996\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.2634])\n",
      "Loss: 1.2449347972869873\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4743])\n",
      "Loss: 1.252565860748291\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.5786])\n",
      "Loss: 1.298201084136963\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0482])\n",
      "Loss: 1.2659692764282227\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1091])\n",
      "Loss: 1.229121208190918\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1400])\n",
      "Loss: 1.3816351890563965\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1250])\n",
      "Loss: 1.2281218767166138\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0742])\n",
      "Loss: 1.3148407936096191\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.9330])\n",
      "Loss: 1.4355839490890503\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8977])\n",
      "Loss: 1.1666759252548218\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.6405])\n",
      "Loss: 1.1369365453720093\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2548])\n",
      "Loss: 1.378027081489563\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8224])\n",
      "Loss: 1.2661387920379639\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3507])\n",
      "Loss: 1.0969980955123901\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.9785])\n",
      "Loss: 1.5708614587783813\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.0631])\n",
      "Loss: 1.477508544921875\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.3251])\n",
      "Loss: 1.4098128080368042\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4419])\n",
      "Loss: 1.3312387466430664\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5035])\n",
      "Loss: 1.378388524055481\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8725])\n",
      "Loss: 1.3106210231781006\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1804])\n",
      "Loss: 1.3847534656524658\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4996])\n",
      "Loss: 1.2527434825897217\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7495])\n",
      "Loss: 1.3677963018417358\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.9026])\n",
      "Loss: 1.2971769571304321\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.8798])\n",
      "Loss: 1.3692125082015991\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0748])\n",
      "Loss: 1.2160431146621704\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1433])\n",
      "Loss: 1.4410611391067505\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1969])\n",
      "Loss: 1.453355312347412\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2259])\n",
      "Loss: 1.3557721376419067\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2202])\n",
      "Loss: 1.464058518409729\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1870])\n",
      "Loss: 1.3843967914581299\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1244])\n",
      "Loss: 1.38858962059021\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0309])\n",
      "Loss: 1.3338929414749146\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1813])\n",
      "Loss: 1.4375187158584595\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.7763])\n",
      "Loss: 1.3956191539764404\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3706])\n",
      "Loss: 1.051456093788147\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0193])\n",
      "Loss: 1.4792746305465698\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7414])\n",
      "Loss: 1.3969664573669434\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5179])\n",
      "Loss: 1.6545019149780273\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4527])\n",
      "Loss: 1.0753450393676758\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.4110])\n",
      "Loss: 1.1826823949813843\n",
      "tensor(1.00000e-04 *\n",
      "       [-4.7192])\n",
      "Loss: 1.4118549823760986\n",
      "tensor(1.00000e-04 *\n",
      "       [-7.5497])\n",
      "Loss: 1.1734015941619873\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4857])\n",
      "Loss: 1.1520870923995972\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3138])\n",
      "Loss: 1.1955238580703735\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2450])\n",
      "Loss: 1.3427345752716064\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.3273])\n",
      "Loss: 1.1777907609939575\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3707])\n",
      "Loss: 1.3323867321014404\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.2782])\n",
      "Loss: 0.9994584918022156\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3275])\n",
      "Loss: 1.1141163110733032\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2850])\n",
      "Loss: 1.5813621282577515\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1964])\n",
      "Loss: 1.4519920349121094\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.8929])\n",
      "Loss: 1.4269136190414429\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0220])\n",
      "Loss: 1.479303240776062\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0388])\n",
      "Loss: 1.6578654050827026\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0274])\n",
      "Loss: 1.4868968725204468\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.8760])\n",
      "Loss: 1.3150488138198853\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1687])\n",
      "Loss: 1.3858544826507568\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1664])\n",
      "Loss: 1.1953001022338867\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9333])\n",
      "Loss: 1.0878947973251343\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5115])\n",
      "Loss: 1.1652803421020508\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0418])\n",
      "Loss: 1.1544135808944702\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5870])\n",
      "Loss: 1.2777246236801147\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4056])\n",
      "Loss: 1.4832148551940918\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.1210])\n",
      "Loss: 1.1807334423065186\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.8874])\n",
      "Loss: 1.1211214065551758\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.6328])\n",
      "Loss: 1.2414652109146118\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.7700])\n",
      "Loss: 1.1019080877304077\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9183])\n",
      "Loss: 1.2084934711456299\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.0453])\n",
      "Loss: 1.1332169771194458\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1239])\n",
      "Loss: 1.266586422920227\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1485])\n",
      "Loss: 1.4962170124053955\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1696])\n",
      "Loss: 1.3984572887420654\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.0078])\n",
      "Loss: 1.2612000703811646\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6118])\n",
      "Loss: 1.254387617111206\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0032])\n",
      "Loss: 1.4429748058319092\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0161])\n",
      "Loss: 1.2511653900146484\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0142])\n",
      "Loss: 1.4741086959838867\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.8629])\n",
      "Loss: 1.5478671789169312\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3637])\n",
      "Loss: 1.2748692035675049\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7969])\n",
      "Loss: 1.194693684577942\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.8640])\n",
      "Loss: 1.2358556985855103\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.6567])\n",
      "Loss: 1.2040270566940308\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2157])\n",
      "Loss: 1.1813462972640991\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9994])\n",
      "Loss: 1.3858169317245483\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.8515])\n",
      "Loss: 1.2681210041046143\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8383])\n",
      "Loss: 1.1859827041625977\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.3513])\n",
      "Loss: 1.1549835205078125\n",
      "tensor(1.00000e-04 *\n",
      "       [-5.0160])\n",
      "Loss: 1.499517798423767\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.7259])\n",
      "Loss: 1.2606256008148193\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8340])\n",
      "Loss: 1.3755215406417847\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2874])\n",
      "Loss: 1.243666648864746\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7319])\n",
      "Loss: 1.3024030923843384\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9477])\n",
      "Loss: 1.097682237625122\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.9739])\n",
      "Loss: 1.1892447471618652\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.9097])\n",
      "Loss: 1.281550645828247\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6308])\n",
      "Loss: 1.218276858329773\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.1115])\n",
      "Loss: 1.2552518844604492\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4141])\n",
      "Loss: 1.2689690589904785\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4538])\n",
      "Loss: 1.1367497444152832\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3421])\n",
      "Loss: 1.182563304901123\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.0652])\n",
      "Loss: 1.2063262462615967\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4434])\n",
      "Loss: 1.2701507806777954\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4884])\n",
      "Loss: 1.5675548315048218\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4475])\n",
      "Loss: 1.2727651596069336\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2631])\n",
      "Loss: 1.1669132709503174\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.9057])\n",
      "Loss: 1.2504792213439941\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5793])\n",
      "Loss: 1.4093654155731201\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7974])\n",
      "Loss: 1.2067238092422485\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4097])\n",
      "Loss: 1.2775226831436157\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.4523])\n",
      "Loss: 1.180351734161377\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2835])\n",
      "Loss: 1.3645466566085815\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0492])\n",
      "Loss: 1.338273048400879\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7720])\n",
      "Loss: 0.9964582324028015\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8121])\n",
      "Loss: 0.977383553981781\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8453])\n",
      "Loss: 1.2036668062210083\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7161])\n",
      "Loss: 1.4002048969268799\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1318])\n",
      "Loss: 1.7693877220153809\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3204])\n",
      "Loss: 1.4705696105957031\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4728])\n",
      "Loss: 1.116716980934143\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4973])\n",
      "Loss: 1.3468883037567139\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1416])\n",
      "Loss: 1.2442567348480225\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3139])\n",
      "Loss: 1.1390379667282104\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.4385])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.470557689666748\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.4634])\n",
      "Loss: 1.3628140687942505\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0593])\n",
      "Loss: 1.3920527696609497\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7371])\n",
      "Loss: 1.0459873676300049\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1517])\n",
      "Loss: 1.4572416543960571\n",
      "tensor(1.00000e-04 *\n",
      "       [-6.6046])\n",
      "Loss: 1.6917701959609985\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.8978])\n",
      "Loss: 1.4773287773132324\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0372])\n",
      "Loss: 1.4281824827194214\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9905])\n",
      "Loss: 1.295750379562378\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.6444])\n",
      "Loss: 1.4482492208480835\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.0304])\n",
      "Loss: 1.3754119873046875\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0168])\n",
      "Loss: 1.339665412902832\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0992])\n",
      "Loss: 1.4718228578567505\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1527])\n",
      "Loss: 1.6499202251434326\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1658])\n",
      "Loss: 1.276034951210022\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1335])\n",
      "Loss: 1.3479527235031128\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0639])\n",
      "Loss: 1.5132697820663452\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6320])\n",
      "Loss: 1.4035263061523438\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.3308])\n",
      "Loss: 1.4908416271209717\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7777])\n",
      "Loss: 1.3978760242462158\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.2337])\n",
      "Loss: 1.2533843517303467\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7914])\n",
      "Loss: 1.2872834205627441\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3123])\n",
      "Loss: 1.3029247522354126\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0492])\n",
      "Loss: 1.358393669128418\n",
      "tensor(1.00000e-04 *\n",
      "       [-3.0520])\n",
      "Loss: 1.6619774103164673\n",
      "tensor(1.00000e-05 *\n",
      "       [-1.8429])\n",
      "Loss: 1.2011746168136597\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5056])\n",
      "Loss: 0.9348381757736206\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.1636])\n",
      "Loss: 1.3663116693496704\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.7529])\n",
      "Loss: 1.4164097309112549\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.1734])\n",
      "Loss: 1.4686747789382935\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4943])\n",
      "Loss: 1.1102105379104614\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6626])\n",
      "Loss: 1.4910402297973633\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7056])\n",
      "Loss: 1.5009942054748535\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0622])\n",
      "Loss: 1.5805472135543823\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1370])\n",
      "Loss: 1.674271583557129\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1873])\n",
      "Loss: 1.5090551376342773\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2008])\n",
      "Loss: 1.375013828277588\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1815])\n",
      "Loss: 1.6742994785308838\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1381])\n",
      "Loss: 1.475034236907959\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0674])\n",
      "Loss: 1.717214822769165\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7082])\n",
      "Loss: 1.3884221315383911\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5474])\n",
      "Loss: 1.3667778968811035\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.3044])\n",
      "Loss: 1.3880243301391602\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9555])\n",
      "Loss: 1.4043073654174805\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5873])\n",
      "Loss: 1.0692694187164307\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.3602])\n",
      "Loss: 1.2474526166915894\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.1634])\n",
      "Loss: 1.3894057273864746\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.3240])\n",
      "Loss: 1.484947681427002\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0407])\n",
      "Loss: 1.2545028924942017\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0908])\n",
      "Loss: 1.341772198677063\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.1490])\n",
      "Loss: 1.853183388710022\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2989])\n",
      "Loss: 1.3543620109558105\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2586])\n",
      "Loss: 1.3027480840682983\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4145])\n",
      "Loss: 1.126198410987854\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5568])\n",
      "Loss: 1.1833887100219727\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.5999])\n",
      "Loss: 1.2267200946807861\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5171])\n",
      "Loss: 1.2276415824890137\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4039])\n",
      "Loss: 1.3850162029266357\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2189])\n",
      "Loss: 1.3611372709274292\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.9915])\n",
      "Loss: 1.266021728515625\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6282])\n",
      "Loss: 1.5408471822738647\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0131])\n",
      "Loss: 1.4291563034057617\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0396])\n",
      "Loss: 1.2499719858169556\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0426])\n",
      "Loss: 1.3073383569717407\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0070])\n",
      "Loss: 1.156043291091919\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.5773])\n",
      "Loss: 1.2729350328445435\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7392])\n",
      "Loss: 1.2214608192443848\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.6171])\n",
      "Loss: 1.4059993028640747\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.4085])\n",
      "Loss: 1.4059032201766968\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1582])\n",
      "Loss: 1.3331046104431152\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.0096])\n",
      "Loss: 1.1002681255340576\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.2936])\n",
      "Loss: 1.5075017213821411\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3923])\n",
      "Loss: 1.5433579683303833\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5948])\n",
      "Loss: 1.5301417112350464\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.8547])\n",
      "Loss: 1.1791605949401855\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.2978])\n",
      "Loss: 1.5126349925994873\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7678])\n",
      "Loss: 1.4604946374893188\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5631])\n",
      "Loss: 1.2784154415130615\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.8889])\n",
      "Loss: 1.4902997016906738\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.3014])\n",
      "Loss: 0.9660865664482117\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7988])\n",
      "Loss: 1.2149027585983276\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.1163])\n",
      "Loss: 1.5108702182769775\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.2763])\n",
      "Loss: 1.2634185552597046\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0259])\n",
      "Loss: 1.429853081703186\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1034])\n",
      "Loss: 1.5337480306625366\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1490])\n",
      "Loss: 1.4368014335632324\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1480])\n",
      "Loss: 1.4924647808074951\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0907])\n",
      "Loss: 1.2798781394958496\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0021])\n",
      "Loss: 1.2572340965270996\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8151])\n",
      "Loss: 1.2528562545776367\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4839])\n",
      "Loss: 1.1155714988708496\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.9987])\n",
      "Loss: 1.2543318271636963\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5053])\n",
      "Loss: 1.04349946975708\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.0580])\n",
      "Loss: 1.4580650329589844\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7553])\n",
      "Loss: 1.3053895235061646\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.9825])\n",
      "Loss: 0.9248299598693848\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.7523])\n",
      "Loss: 1.3829200267791748\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.6752])\n",
      "Loss: 1.1964868307113647\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7788])\n",
      "Loss: 1.1996688842773438\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8698])\n",
      "Loss: 1.223620057106018\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8855])\n",
      "Loss: 1.4408526420593262\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.7253])\n",
      "Loss: 1.2603224515914917\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.4338])\n",
      "Loss: 1.4151321649551392\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.0239])\n",
      "Loss: 1.3594342470169067\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5349])\n",
      "Loss: 1.6582365036010742\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8398])\n",
      "Loss: 1.4268407821655273\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.7565])\n",
      "Loss: 1.2078616619110107\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4606])\n",
      "Loss: 1.2861517667770386\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.8940])\n",
      "Loss: 1.2236450910568237\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1613])\n",
      "Loss: 1.2247787714004517\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0781])\n",
      "Loss: 1.228829264640808\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.9416])\n",
      "Loss: 1.1959378719329834\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7125])\n",
      "Loss: 1.1580268144607544\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2128])\n",
      "Loss: 1.6735658645629883\n",
      "tensor(1.00000e-04 *\n",
      "       [-9.7662])\n",
      "Loss: 1.318118929862976\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.4459])\n",
      "Loss: 1.4319674968719482\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.5244])\n",
      "Loss: 1.581868290901184\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4752])\n",
      "Loss: 1.2719064950942993\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.6526])\n",
      "Loss: 1.1275631189346313\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.8842])\n",
      "Loss: 1.224747657775879\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.0508])\n",
      "Loss: 1.3840385675430298\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.0874])\n",
      "Loss: 1.5081406831741333\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.0867])\n",
      "Loss: 1.1460100412368774\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.8602])\n",
      "Loss: 1.3628333806991577\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.4254])\n",
      "Loss: 1.4273189306259155\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.6042])\n",
      "Loss: 1.5324450731277466\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.3573])\n",
      "Loss: 1.2922751903533936\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6342])\n",
      "Loss: 1.3424228429794312\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.6603])\n",
      "Loss: 1.3231027126312256\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.3448])\n",
      "Loss: 1.2846335172653198\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.1291])\n",
      "Loss: 1.2374149560928345\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.7813])\n",
      "Loss: 1.097205400466919\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.4376])\n",
      "Loss: 1.3878840208053589\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.0397])\n",
      "Loss: 1.3547781705856323\n",
      "tensor(1.00000e-04 *\n",
      "       [-2.6885])\n",
      "Loss: 1.8197484016418457\n",
      "tensor(1.00000e-04 *\n",
      "       [ 5.6974])\n",
      "Loss: 1.2191799879074097\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.3502])\n",
      "Loss: 1.5574021339416504\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.5868])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.7447084188461304\n",
      "tensor(1.00000e-03 *\n",
      "       [-6.5438])\n",
      "Loss: 1.5175604820251465\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.2601])\n",
      "Loss: 1.4598640203475952\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7870])\n",
      "Loss: 1.4292387962341309\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1187])\n",
      "Loss: 1.4570183753967285\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2467])\n",
      "Loss: 1.7081387042999268\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3613])\n",
      "Loss: 1.7819392681121826\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4598])\n",
      "Loss: 1.9006860256195068\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5447])\n",
      "Loss: 1.9601951837539673\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6134])\n",
      "Loss: 1.9594799280166626\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6558])\n",
      "Loss: 1.8779799938201904\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6744])\n",
      "Loss: 2.5713717937469482\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6736])\n",
      "Loss: 2.4487171173095703\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6450])\n",
      "Loss: 2.3567659854888916\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5899])\n",
      "Loss: 2.122060537338257\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5041])\n",
      "Loss: 1.8207502365112305\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3937])\n",
      "Loss: 1.7882919311523438\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2714])\n",
      "Loss: 1.7261406183242798\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.1389])\n",
      "Loss: 1.5682679414749146\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.9966])\n",
      "Loss: 1.6056098937988281\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.6052])\n",
      "Loss: 1.2300822734832764\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.2282])\n",
      "Loss: 1.532260775566101\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.8954])\n",
      "Loss: 1.2800588607788086\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.6471])\n",
      "Loss: 1.2337841987609863\n",
      "tensor(1.00000e-03 *\n",
      "       [-3.4359])\n",
      "Loss: 1.4882302284240723\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.2536])\n",
      "Loss: 1.1192680597305298\n",
      "tensor(1.00000e-03 *\n",
      "       [-1.1295])\n",
      "Loss: 1.5934042930603027\n",
      "tensor(1.00000e-06 *\n",
      "       [-6.8708])\n",
      "Loss: 3.6034514904022217\n",
      "tensor(1.00000e-04 *\n",
      "       [ 9.2188])\n",
      "Loss: 1.9654144048690796\n",
      "tensor(1.00000e-04 *\n",
      "       [-8.5314])\n",
      "Loss: 1.476224422454834\n",
      "tensor(1.00000e-03 *\n",
      "       [-2.5711])\n",
      "Loss: 1.446318507194519\n",
      "tensor(1.00000e-03 *\n",
      "       [-4.1938])\n",
      "Loss: 1.513811469078064\n",
      "tensor(1.00000e-03 *\n",
      "       [-5.7097])\n",
      "Loss: 1.3594964742660522\n",
      "tensor(1.00000e-03 *\n",
      "       [-7.1212])\n",
      "Loss: 1.2076324224472046\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.4654])\n",
      "Loss: 1.2418262958526611\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7447])\n",
      "Loss: 1.2794685363769531\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0974])\n",
      "Loss: 1.4060957431793213\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2135])\n",
      "Loss: 1.6881202459335327\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3246])\n",
      "Loss: 1.9863221645355225\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4281])\n",
      "Loss: 1.8765352964401245\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5259])\n",
      "Loss: 1.661624789237976\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6190])\n",
      "Loss: 2.3028273582458496\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7066])\n",
      "Loss: 2.082724094390869\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7891])\n",
      "Loss: 2.639218807220459\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8652])\n",
      "Loss: 1.9719723463058472\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9312])\n",
      "Loss: 2.7327706813812256\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9866])\n",
      "Loss: 3.1742420196533203\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0306])\n",
      "Loss: 2.451648473739624\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0582])\n",
      "Loss: 2.471017837524414\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0724])\n",
      "Loss: 2.7381081581115723\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0717])\n",
      "Loss: 2.596940517425537\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0526])\n",
      "Loss: 2.6424789428710938\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.0130])\n",
      "Loss: 2.232841730117798\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.9559])\n",
      "Loss: 2.270380735397339\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.8794])\n",
      "Loss: 1.6397749185562134\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.7900])\n",
      "Loss: 1.8264273405075073\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.6890])\n",
      "Loss: 1.7562370300292969\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.5782])\n",
      "Loss: 1.6704462766647339\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.4613])\n",
      "Loss: 1.630548119544983\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.3396])\n",
      "Loss: 1.6822803020477295\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.2164])\n",
      "Loss: 1.465132474899292\n",
      "tensor(1.00000e-02 *\n",
      "       [-1.0942])\n",
      "Loss: 2.100687026977539\n",
      "tensor(1.00000e-03 *\n",
      "       [-9.7505])\n",
      "Loss: 1.4925845861434937\n",
      "tensor(1.00000e-03 *\n",
      "       [-8.5811])\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    print(a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

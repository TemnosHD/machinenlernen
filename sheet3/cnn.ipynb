{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 50 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset in trainset and testset. The trainset consists of 60000 images, the testset of 10000 imgs.\n",
    "\n",
    "trainset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "testset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classnames = [str(i) for i in range(10)]\n",
    "\n",
    "def imshow(img, title=\"\", cmap = \"Greys_r\"): #convert tensor to image\n",
    "    plt.title(title)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    numpyimg = img.numpy()[0]\n",
    "    plt.imshow(numpyimg, cmap = cmap)\n",
    "    \n",
    "\n",
    "def display_10_images_from_dataset(dataset, class_names):\n",
    "    \"\"\"\n",
    "    plots 10 randomly chosen images from a given dataset\n",
    "    \"\"\"\n",
    "    display = [] #holds tuples of image and respective label\n",
    "    for _ in range(10):\n",
    "        index = np.random.randint(0,len(dataset)+1)\n",
    "        display.append(dataset[index])\n",
    "    \n",
    "    nr = 1\n",
    "    fig, axes = plt.subplots(2,5,sharex='col',sharey='row', figsize = (14,9))\n",
    "    for image, label in display:\n",
    "        axes[(nr-1)//5][(nr-1)%5] = plt.subplot(2,5,nr)\n",
    "        plt.title(class_names[label], fontsize = 16)\n",
    "        imshow(image, title = class_names[label])\n",
    "        nr+=1\n",
    "    fig.subplots_adjust(hspace=-0.4)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[0:5]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in fig.axes[1:5]+fig.axes[6:]], visible=False)\n",
    "    plt.show()\n",
    "    plt.savefig(\"previewMNIST.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAFtCAYAAADccl8mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xn8VGX9///nSxaVRcUFIhJRcy0DfpJpWKJhmVpqlkqp\nlAukZqJpmol7iUuppVmQKKWpaZhKprkrGIq4YZI74ILwwRCRRbbr9wdTX2ZeF8xh5sycOTOP++3G\nTa4nZ868eL9fnvdczFznshCCAAAAACBP1sm6AAAAAABYW0xkAAAAAOQOExkAAAAAucNEBgAAAEDu\nMJEBAAAAkDtMZAAAAADkDhMZAAAAALnDRCYDZnaYmU01swVm9pqZfSHrmtA6zGxdM7vWzKab2Xwz\ne9bMvpp1XWg9ZvawmS02sw8Lv17Kuia0FjO7wczeNbMPzOxlMzsm65rQmnhtWJm2WRfQasxsb0kX\nSzpU0pOSumdbEVpQW0lvStpD0gxJ+0r6s5ntFEKYlmVhaEk/CCH8Pusi0LJGSBoSQlhoZttLetjM\nngkhTM66MLQOXhtWjolM/Z0n6fwQwsTC+O0si0HrCSEskHTuKtE4M3tD0s6SpmVREwBkIYTwwqrD\nwq+tJTGRQT3x2rBCfLSsjsysjaR+kjYzs1fN7C0zu8rM1s+6NrQuM+smaVtJ/8q6FrSki8xsjplN\nMLMBWReD1mNmvzGzhZL+LWmmpLszLgkthNeG1WEiU1/dJLWT9E1JX5DUR1JfSWdlWRRal5m1k3Sj\npDEhhH9nXQ9azumStpLUQ9JISXeZ2dbZloRWE0I4XlJnrfy5PFbSR9lWhBbDa8MqMJGpr0WF//46\nhDAzhDBH0i+1co0CUFdmto6kP0paIukHGZeDFhRCeCKEMD+E8FEIYYykCeJ6iAyEEJaHEMZL+oSk\n47KuBy2F14ZVYI1MHYUQ5prZW1r5Gdz/xVnVg9ZlZibpWq38l6B9QwhLMy4JkFZeDy3rItDS2mrl\nGhmgLnhtWB3ekam/6ySdaGZdzayLpJMljcu4JrSeayTtIOlrIYRF5Q4G0mZmG5nZV8xsPTNra2bf\nkfRFSfdkXRtaQ+Hn8GFm1snM2pjZVyQNkvRA1rWh5fDasEIWApO+eiqsSbhS0rclLZb0Z0k/DiEs\nzrQwtAwz20Ir7072kaRlq/zR0BDCjZkUhZZjZptp5aLq7SUt18qF1sNDCPdlWhhaRqEHb5PUWyv/\nYXe6pF+FEEZlWhhaDq8NK8dEBgAAAEDu8NEyAAAAALnDRAYAAABA7jCRAQAAAJA7TGQAAAAA5E5V\nExkz28fMXjKzV83sjLSKAgAAAIA1qfiuZWbWRtLLkvaW9JakSZIGhRBeXMNjuEUaVmdOCGGzejwR\nfYjVCSHUZTNGehBrwLUQjaAufUgPYg0S9WA178jsIunVEMLrIYQlkm6WdEAV50Nrm551AQDQALgW\nohHQh8haoh5sW8UT9JD05irjtyR9rvQgMxsiaUgVzwNUjT5E1uhBNAL6EFmjB5Gmaj5a9k1J+4QQ\njimMj5D0uRDCD9bwGN5CxOpMDiH0q8cT0YdYHT5ahgbAtRCNoC59SA9iDRL1YDUfLXtb0uarjD9R\nyAAAAACgpqqZyEyStI2ZbWlm7SUdJunOdMoCAAAAgNWreI1MCGGZmf1A0r2S2kgaHUL4V2qVAQAA\nAMBqVLPYXyGEuyXdnVItAAAAAJBIVRtiAgAAAEAWqnpHBtLs2bNdttlmfv+eSy65pGh8+umn16wm\nAAAAYFUbbrhh0fjCCy90x+y3334u69mzp8vatm2MKQTvyAAAAADIHSYyAAAAAHKHiQwAAACA3GmM\nD7jlRL9+foPRDh06uCwEv1HtqaeeWjR+6KGH3DH33HNPFdWhkf3yl79MdNx5551XNJ43b14tygEA\nAE1s4403dtnFF19cND7mmGPcMbHXsMuXL0+vsJTxjgwAAACA3GEiAwAAACB3mMgAAAAAyB0mMgAA\nAAByh8X+a6F///4uiy32jxk/fnzR+PXXX0+lJuTDsGHDXBZbUHf88ccXjUePHu2OmTRpksuuu+66\nKqoDAAB5dcQRR7js3HPPddmWW25ZNI69DlmxYoXLBg8eXHlxNcY7MgAAAAByh4kMAAAAgNxhIgMA\nAAAgd5jIAAAAAMgdiy30Sfxgs2mS5ktaLmlZCKFfmeMrf7IGcOutt7rs4IMPTvTYvfbaq2j88MMP\np1FSM5lcrn/SkkUf9uzZ02X33Xefyz75yU+WPVdsh91Fixa5bOzYsS6bPn160bhPnz7umEceeaRs\nDZJ01VVXuWzp0qWJHtuoQghWj+fJ+7UQNdXU18I999zTZQ899FBq519nHf/vs5/4xCdcVvqz+6ij\njnLHfPrTn3bZ448/7rLYjYCaQF36kGvhmnXv3t1lw4cPd9mxxx7rsjZt2pQ9/7Jly1x29NFHu+yP\nf/xj2XPVQKIeTOOuZXuGEOakcB4AAAAASISPlgEAAADInWonMkHS/WY22cyGxA4wsyFm9pSZPVXl\ncwEVow+RNXoQjYA+RNboQaSp2o+W7R5CeNvMukq6z8z+HUJ4dNUDQggjJY2U+CwkskMfImv0IBoB\nfYis0YNIU1WL/YtOZHaupA9DCJet4ZhcN+zcuXNdtuGGG7rs+eefd9lnP/vZonHeF0XXQFMvcI3Z\nZJNNXHbzzTcXjbfaait3TK9evWpV0mqZ+TXwP/zhD102evRoly1cuLAmNdUCi/3RAFruWlipgQMH\nuuw3v/mNy5LcRCWp2Gumvffe22UPPvhgas+ZERb7N4B//vOfLvvc5z5X8flKX3sOGeI/TDVmzJiK\nz5+yRD1Y8UfLzKyjmXX+7+8lfVnSC5WeDwAAAACSquajZd0k3V74l9q2kv4UQrgnlaoAAAAAYA0q\nnsiEEF6X1DvFWgAAAAAgEW6/DAAAACB30tgQsyntuOOOLmvXrl2ixy5ZssRlLO5Hqffee89lpYtG\nYzcEOOyww1z2/e9/32Xrr7++y7bccsu1KXGNrrzySpctX77cZaU7An/44Yep1YDmFOvd2C7VsRtJ\ntG/f3mUrVqwoGsd2f49p2zbZj8jY7tiLFy9O9FgkE/ue3XDDDS7r2rVrReeP/YyOff9jNz6J7b6O\nxtC/f3+XTZgwIYNKvB49ehSN77//fnfM1ltvXfH5Yz191113FY0baGF/xXhHBgAAAEDuMJEBAAAA\nkDtMZAAAAADkDhMZAAAAALnDYv/VOOuss1zWoUOHRI999tln0y4HLSp2Q4Crr746UdapUyeX7bPP\nPmWfc//993fZgQce6LLOnTu77KqrrnLZrrvuWjQePHhw2RrQHGKL9k888cSi8X777eeO6dfPb+Yc\nO9fzzz/vsp49e7qsdNHruuuu646J3aiiS5cuLouZN29e2dpuueUWd0xsF3rExRb7r7feeokeO2PG\nDJddf/31RePYTXouuOACl8VuMDFu3LhEdaD++vTp47JGWew/dOjQovF2221X8bli/Xv88ce7bPTo\n0RU/R6PiHRkAAAAAucNEBgAAAEDuMJEBAAAAkDuskSnYdttti8axNQEx8+fPd9nFF1+cSk1ANWIb\nT952221lHxc7Jra25s4773RZbNPCaj73i8Z08MEHu+zII4902R577OGyDTbYILU6YhsXT58+3WXd\nunUrGt96663umG984xsue/TRR1322muvrU2J/xNb44PkYpuO7rDDDi677LLLXBa7Vk2aNKlo/Mgj\nj7hjYptfjhw50mWxNVJoDLH1o1kYP368yz772c9WdK6PPvrIZaVrD6XmXA8TwzsyAAAAAHKHiQwA\nAACA3GEiAwAAACB3mMgAAAAAyJ2yi/3NbLSk/SXNDiF8upBtLOkWSb0kTZN0SAhhbu3KrL1DDz20\naJx0o60pU6a4rNLFoECjSroxYEzpQutevXq5Y6ZNm1bx+VFbI0aMcNnJJ5/ssnbt2rlszpw5Livd\nMDi2eDq2udv555/vsmHDhrksyQ0tYo4++uiKHofszJo1y2UnnXSSyzbccEOXPfDAA0XjHj16uGNi\nm1/GNstG69p8881ddtxxx7kstrA/ds0sFdvM9cILL3TZ73//+7LnalZJ3pG5XlLpLYvOkPRACGEb\nSQ8UxgAAAABQF2UnMiGERyX9pyQ+QNKYwu/HSEp2r2IAAAAASEGl+8h0CyHMLPz+XUndVnegmQ2R\nNKTC5wFSQR8ia/QgGgF9iKzRg0hT1RtihhCCmYU1/PlISSMlaU3HAbVEHyJr9CAaAX2IrNGDSFOl\nE5lZZtY9hDDTzLpLmp1mUXny0ksvZV0CULHYztWxhbE33nijy0LwP39WrFjhsq997WtFYxb2N7bS\nXdD32ad0iaS0fPlyl11zzTUuO+MMv3xy/vz5FdV1yy23uGzBggUVnQvNIXa9Wbx4scsee+wxl22x\nxRZlz//QQw+5bNGiRQmrQyvYf//9XRa77sWU/vyN9e5RRx3lsgcffDBhda2h0tsv3ylpcOH3gyXd\nkU45AAAAAFBe2YmMmd0k6Z+StjOzt8zsaEkjJO1tZq9IGlgYAwAAAEBdlP1oWQhh0Gr+6Esp1wIA\nAAAAiVT60TIAAAAAyEzVdy1rFkl2dY7t8vuzn/2sFuWsUadOnVy2xx57uGzy5MlF43fffbdmNSGf\nYgv733vvvYrP9/DDD7vshRdeqPh8qL9+/foVjdu29T8m/vjHP7rshBNOqFlNEgv74a233noumzRp\nksu22267is6/3377uWzs2LEuO/zww11Gv7aGXr16VfzY0sX9//jHP9wxsYX9sdeAAwYMcFnpa0BJ\nmjlzpsvyjndkAAAAAOQOExkAAAAAucNEBgAAAEDuMJEBAAAAkDss9i/YbLPNyh4TW0T4+uuv16Kc\n/9lpp51cdumll7rsy1/+sstKF3Vdfvnl7pjLLrusiuqQNz/84Q+LxldccUWix5XuQCxJQ4cOddmo\nUaMqKwwNo/SmIB/72MfcMV/4whdcFrtxxLx589IrDCjRsWNHl1W6sD+pAw44wGVPPfWUy/r27euy\n2M7tyI+TTz7ZZcOGDav4fNOmTSsax3qrmteAV199tctuvvnmovGECRPKldnweEcGAAAAQO4wkQEA\nAACQO0xkAAAAAOSOhRDq92Rm9XuytVS6nqRbt27umEceecRle+65Z2o1HHzwwS4bM2aMyzp06FDR\n+WMbem611VYumz17dkXnr9LkEEK/8odVr5H7sFKxzbD+/ve/u6x0c8N11vH/lhFb91W6SaIkffDB\nBy6r5/WkFkIIfjFQDTRyD7Zp06ZoHFtHFfss97rrruuyESNGuCy2Vg9FuBYmFNusdd9993XZ1772\nNZdNmTJljWNJOuWUU1w2cOBAl8V6P7Zp7ODBg13WwOrSh3nqwblz57ostjYwqUWLFhWNp06d6o7Z\nfvvtXRZbG5b0Z++HH35YNN5ggw0SPS4jiXqQd2QAAAAA5A4TGQAAAAC5w0QGAAAAQO4wkQEAAACQ\nO2UX+5vZaEn7S5odQvh0ITtX0rGS/q9w2JkhhLvLPlkDL+oaPnx40fi8885zx8Q2d+vdu7fLZsyY\nUfb5dtllF5c9+OCDLku6sH/+/Pkua9++fdE4tiDx7LPPdtmFF16Y6DlTxgLXhJ555hmXbbPNNi5b\nf/31y55ryZIlLuvatavLYv3VjFjsn0xssWlsw+DYQtX33nuvaHzggQe6Y5phk7YqcC1sYF/96ldd\nNnbs2ESP3X333YvGkydPTqWmGmGxf4m0F/tnoVUX+18vaZ9IfnkIoU/hV9lJDAAAAACkpexEJoTw\nqKT/1KEWAAAAAEikmjUyJ5rZ82Y22sy6rO4gMxtiZk+Z2VNVPBdQFfoQWaMH0QjoQ2SNHkSaKp3I\nXCNpK0l9JM2U9IvVHRhCGBlC6Fevz/wCMfQhskYPohHQh8gaPYg0+W1xEwghzPrv781slKRxqVXU\nwGKLuipd6NWzZ0+XJV3Y/84777hs7733dtmYMWOKxrHd2dHYLrjgApftuOOOLovtcB1Turj/qKOO\ncse0ysJ+VG7BggUu69+/v8tKr0GStP/++xeNH374YXfMl770JZc9+uija1EhUBt///vfXXbppZe6\n7KyzznLZ0UcfXTRu8MX+LWWjjTZy2SGHHFI0TvoaDfVV0TsyZtZ9leFBkl5IpxwAAAAAKK/sP+Oa\n2U2SBkja1MzeknSOpAFm1kdSkDRN0tAa1ggAAAAARcpOZEIIgyLxtTWoBQAAAAASqeauZQAAAACQ\niYoW++P/GTx4sMtOPfXUso9btGiRy5YvX+6yNm3auCy2iLB79+4uY3F//jzzzDNF42oW9scWoP7j\nH/8oGk+dOnUtqgNWL7br9de//nWX/fSnPy0an3/++e6Y22+/3WVdu3Z1WeyaCdTb4sWLsy4Ba6Fz\n584ue/zxx122/fbb17QOMysahxAqetzaPLYZ8Y4MAAAAgNxhIgMAAAAgd5jIAAAAAMgdJjIAAAAA\ncofF/gWlu/Wedtpp7phOnTq57Hvf+57LFi5c6LLLL7+8aPy3v/3NHRNbnH3GGWe47KSTTnJZz549\nXVbqo48+ctk999xT9nGojX322cdlpYsLky7sv+SSS1x25ZVXuuzdd99NWB1QGz/72c+KxmeeeaY7\npkuXLi477rjjXHbVVVelVxhQoc033zzRca28ILuRPPnkky7bbrvtMqiktpYsWeKyu+++O4NKaot3\nZAAAAADkDhMZAAAAALnDRAYAAABA7jCRAQAAAJA7Vs/FZ2aWm5Vu/fr1c9kjjzzisvXXXz/R+ebN\nm1c0vvfee90xsQWDu+22W6Lzx5R+b9977z13TGy37IxMDiH4L3oNNHIflvZJ7AYTMa+99prLxowZ\nU/b8sZ6eMmVKoueM6d69u8tmzpxZ8fnqLYTgt0yugUbpwV69erls2rRpda3h8MMPd9kf/vAHlz3x\nxBMuq+b62MC4Fjaw3r17u2zChAkui+2+/ulPf7po/MYbb6RXWPrq0odZ9GDsxkft2rWr6FzLly93\nWZs2bSo6VzVir+VPPPFEl/3mN7+pRzlpSdSDvCMDAAAAIHeYyAAAAADIHSYyAAAAAHKn7BoZM9tc\n0h8kdZMUJI0MIVxpZhtLukVSL0nTJB0SQphb5ly5/jxubOO2008/3WWxdQ2xz8umKfZ9/Otf/1o0\nPvjgg2taQ5X4XLikUaNGFY2/+93vumPWWSe9f39YsWKFyx599FGXxfo31nOlG3pK0r///e+Kahs5\ncqTLJk6cmKiOGTNmVPScrbZGZvbs2S7r379/0fiVV16paQ0DBgxw2UMPPeSyWB3bbrttLUrKGtfC\nhGJrETbZZBOXxfo8iY4dO7rsrrvuclmsh0uv5ZI0dOjQiurISNOukVm0aJHL2rdv77LSn4+xdae3\n3Xaby0455RSXDRw40GWlP1erWbMe2+z64x//eMXnaxCprZFZJulHIYQdJe0q6QQz21HSGZIeCCFs\nI+mBwhgAAAAAaq7sRCaEMDOE8HTh9/MlTZXUQ9IBkv47PR0j6cBaFQkAAAAAq2q7NgebWS9JfSU9\nIalbCOG/91V9Vys/ehZ7zBBJQyovEagefYis0YNoBPQhskYPIk2JJzJm1knSXyQNCyF8sOpn+0II\nYXWfcwwhjJQ0snCOXH8eF/lFHyJr9CAaAX2IrNGDSFOiDTHNrJ2kcZLuDSH8spC9JGlACGGmmXWX\n9HAIYbsy52mJhk1yU4DOnTtXfP7YJoYPPPCAyy688MKKnyMDLHCNiG1oteuuu7rsG9/4RkXnj904\nIHYDgJjYQtvY5mClG42lfeOLJUuWuKx0Y873338/0blabbF/7OuybNmyonHshhPjxo2r+Dm32GKL\novGkSZPcMZtuuqnLzjnnHJddcMEFFdfRwLgWRsSuVc8995zLttxyS5fNnevvQ1R6Q4kPPvjAHfOl\nL33JZdtt51/mlG40LEk9e/Z02fz5813WwJp2sf/ee+/tstjm4DfeeGNqz/njH//YZaU3AIjdECB2\n45PY672f//znVVTXsNJZ7G8rX3VcK2nqfycxBXdKGlz4/WBJd1RSJQAAAACsrSQfLesv6QhJU8zs\n2UJ2pqQRkv5sZkdLmi7pkNqUCAAAAADFyk5kQgjjJa3u4xb+fVcAAAAAqLH0dtYDAAAAgDpJtNg/\ntSfL0cJC1B0LXOtgww03LBr37t3bHfPoo48mOteBB/qto/7617+67IwzivfKjS3krrVTTz010XGt\nttg/dtORxx9/vGi89dZbu2Oq+blRepOI2I7aL730ksv69fOXhwULFlRcRwPjWhhRetMQSfroo48y\nqMSL3Vjn7LPPzqCSVDXtYn/kRjqL/QEAAACg0TCRAQAAAJA7TGQAAAAA5A4TGQAAAAC5w2J/NAoW\nuCJzrbbYP4mdd97ZZSNGjHBZbBf0pUuXumzixIlF42uuucYdc++997ostjt7k+JaGLHOOv7fXadM\nmeKyHXbYIbXnXLhwocsuuugil/3iF79w2eLFi1OrIyMs9kfWWOwPAAAAoDkxkQEAAACQO0xkAAAA\nAOQOExkAAAAAucNifzQKFrgicyz2T8bMf5k6dOjgshUrVrhs0aJFNampiXAtTKhv374uGzdunMs6\nd+7ssunTpxeNp02b5o45/PDDXTZv3ry1qDDXWOyPrLHYHwAAAEBzYiIDAAAAIHeYyAAAAADInbIT\nGTPb3MweMrMXzexfZnZSIT/XzN42s2cLv/atfbkAAAAAILVNcMwyST8KITxtZp0lTTaz+wp/dnkI\n4bLalQcAaDSxm8QsWLAgg0rQyp555hmX9ejRI4NKAGSl7EQmhDBT0szC7+eb2VRJXCkAAAAAZGat\n1siYWS9JfSU9UYhONLPnzWy0mXVZzWOGmNlTZvZUVZUCVaAPkTV6EI2APkTW6EGkKfE+MmbWSdIj\nkn4WQhhrZt0kzZEUJF0gqXsI4agy5+B+4Vgd9k5A5thHBg2AayEaAfvIIGvp7SNjZu0k/UXSjSGE\nsZIUQpgVQlgeQlghaZSkXaqpFgAAAACSSnLXMpN0raSpIYRfrpJ3X+WwgyS9kH55AAAAAOAluWtZ\nf0lHSJpiZs8WsjMlDTKzPlr50bJpkobWpEIAAAAAKJHkrmXjJcU+N353+uUAAAAAQHlrddcyAAAA\nAGgETGQAAAAA5A4TGQAAAAC5w0QGAAAAQO4wkQEAAACQO0xkAAAAAOROkn1k0jRH0nRJmxZ+n1d5\nr19qvL/DFnV8LvqwMTRa/Vn0oNR4X4e1Rf3p4lq49qg/ffXqQ66FjaPR6k/UgxZCqHUh/knNngoh\n9Kv7E6ck7/VLzfF3qFbevwbU3xzy/nWg/vzL+9eA+ptD3r8O1J8NPloGAAAAIHeYyAAAAADInawm\nMiMzet605L1+qTn+DtXK+9eA+ptD3r8O1J9/ef8aUH9zyPvXgfozkMkaGQAAAACoBh8tAwAAAJA7\nTGQAAAAA5A4TGQAAAAC5w0QGAAAAQO4wkQEAAACQO0xkAAAAAOQOExkAAAAAucNEBgAAAEDuMJEB\nAAAAkDtMZAAAAADkDhMZAAAAALnDRAYAAABA7jCRAQAAAJA7TGQAAAAA5A4TGQAAAAC5w0QGAAAA\nQO4wkQEAAACQO0xkAAAAAOQOExkAAAAAucNEBgAAAEDuMJEBAAAAkDtMZAAAAADkDhMZAAAAALnD\nRAYAAABA7jCRAQAAAJA7TGQAAAAA5A4TGQAAAAC5w0QGAAAAQO4wkQEAAACQO0xkMmJm25jZYjO7\nIeta0FrM7MOSX8vN7NdZ14XWQh+iEZjZxmZ2u5ktMLPpZvbtrGtC66EPK9c26wJa2NWSJmVdBFpP\nCKHTf39vZp0kvSvp1uwqQiuiD9Egrpa0RFI3SX0k/c3Mngsh/CvbstBi6MMK8Y5MBszsMEnvS3og\n61rQ8g6WNFvSY1kXgpZGH6LuzKyjVvbe8BDChyGE8ZLukHREtpWhldCH1WEiU2dmtoGk8yWdknUt\ngKTBkv4QQghZF4KWRh8iC9tKWhZCeHmV7DlJn8qoHrQm+rAKTGTq7wJJ14YQ3sq6ELQ2M9tC0h6S\nxmRdC1oXfYgMdZL0QUn2gaTOGdSC1kUfVoE1MnVkZn0kDZTUN+taAK1823p8COGNrAtBS6MPkZUP\nJW1Qkm0oaX4GtaB10YdVYCJTXwMk9ZI0w8yklbPwNma2Ywjh/8uwLrSmIyWNyLoItDz6EFl5WVJb\nM9smhPBKIestiQXWqCf6sArGR5Lrx8w6qHjWfapWTmyOCyH8XyZFoSWZ2ecl3SfpYyEE/tUHmaAP\nkTUzu1lSkHSMVn5a4m+SPs/dolBP9GHleEemjkIICyUt/O/YzD6UtJhJDDIwWNJYXjwiY/Qhsna8\npNFaede897TyHxZ58Yh6ow8rxDsyAAAAAHKHu5YBAAAAyB0mMgAAAAByh4kMAAAAgNypaiJjZvuY\n2Utm9qqZnZFWUQAAAACwJhUv9jezNlp57+u9Jb0laZKkQSGEF9fwGO4sgNWZE0LYrB5PRB9idUII\nVo/noQexBlwL0Qjq0of0INYgUQ9W847MLpJeDSG8HkJYIulmSQdUcT60tulZFwAADYBrIRoBfYis\nJerBaiYyPSS9ucr4rUIGAAAAADVV8w0xzWyIpCG1fh5gTehDZI0eRCOgD5E1ehBpqmaNzG6Szg0h\nfKUw/okkhRAuWsNj+CwkVmdyCKFfPZ6IPsTqsEYGDYBrIRpBXfqQHsQaJOrBaj5aNknSNma2pZm1\nl3SYpDurOB8AAAAAJFLxR8tCCMvM7AeS7pXURtLoEMK/UqsMAAAAAFajqjUyIYS7Jd2dUi0AAAAA\nkEhVG2ICAAAAQBaYyAAAAADIHSYyAAAAAHKHiQwAAACA3GEiAwAAACB3mMgAAAAAyB0mMgAAAABy\nh4kMAAAAgNxhIgMAAAAgd5jIAAAAAMgdJjIAAAAAcqdt1gUAAACkwcxcNnToUJedc845ReNu3bpV\nfP7p06e77N57701UB7AmXbt2ddkzzzzjsu7du1d0/lg/f/DBBy479dRTXTZq1KiKnjNtvCMDAAAA\nIHeYyAAPEmF3AAAZcklEQVQAAADIHSYyAAAAAHKnqjUyZjZN0nxJyyUtCyH0S6MoAAAAAFgTCyFU\n/uCVE5l+IYQ5CY+v/MnQ7CbXayJMH2J1Qgh+5WMN0INYA66FVejTp4/Lnn766YrOtXTpUpctWbLE\nZR07dnTZihUrXHbLLbcUjY8//nh3zLx589amxFqqSx82Yw8mtc46xR+Kuuiii9wxxxxzjMu6dOni\nsvfff99lsZsClNp2221d1qNHD5fFbgCw2267uWzq1Klln3MtJOpBPloGAAAAIHeqncgESfeb2WQz\nG5JGQQAAAABQTrX7yOweQnjbzLpKus/M/h1CeHTVAwoTHCY5yBR9iKzRg2gE9CGyRg8iTVW9IxNC\neLvw39mSbpe0S+SYkSGEftwIAFmiD5E1ehCNgD5E1uhBpKnid2TMrKOkdUII8wu//7Kk81OrrMl0\n6NChaDxixAh3zHHHHeeytm39t2jWrFku69u3r8tmzpy5NiUiJd/61rdc9uc//9llsRttlO6ym/Rm\nHC+//LLLhg8fnuixpSZOnOiyN998s6JzoXFsvPHGLpszp/x9WhYtWuSy2PUr5vrrr3cZvYS0fOc7\n33HZmDFjUjv/T3/6U5fdfvvtLpswYYLLYjuyDxo0qGj8+OOPu2OuvvrqtSkROVH6GlCSLr300qLx\nkUce6Y6JvXYYNmyYy2I3pli8eHHZutq1a+eyv//97y7ba6+9XHbooYe67Nxzzy37nGmr5qNl3STd\nXnjh1VbSn0II96RSFQAAAACsQcUTmRDC65J6p1gLAAAAACTC7ZcBAAAA5A4TGQAAAAC5Y0kXE6fy\nZC2yg2ts8dT55xffB+H0009P9TnfeOMNl2299dapPkeNNc1u1rHF/qU7OjeyyZMnu+zZZ59N9NhR\no0a57Mknn6y6pnoJIVj5o6qXxbWwc+fOLps2bZrLNtpoo9Sec+7cuS776KOPisax/rj22mtdtmzZ\nMpfdc09TLstsmmthmjbffHOXjR8/PtFxMaU3w3nuuefcMbGbCcR6Orb7+u9+9zuXld7M5aWXXnLH\nDBgwwGWxG/zUQV36ME89WI1TTz3VZaX9dcABB7hjZsyYUbOaVmfTTTd12Wuvveay//u//3PZJz/5\nyTRLSdSDvCMDAAAAIHeYyAAAAADIHSYyAAAAAHKHiQwAAACA3GGxfw0ccsghLrv55pvLPm7JkiUu\n+8lPfuKyCy64wGXrrbeey0p3G95jjz3K1pChplng2qVLF5fdcMMNLhs4cKDLYjeKyJPYTvEvvvhi\n0Xi//fZzx8R2IF6+fHl6hSXUzIv9Y37+85+7rHTX6NIFypLUvn371GqInT/2cynWD//85z9dFlt4\nXboo9f3331+bEuutaa6FaZoyZYrLPvWpTyV67CuvvOKy/v37F41j165q3HXXXS6LXftKxW5g8MUv\nfjGVmtYSi/1LxG7gEHvddtZZZ7ns7rvvdlnpa7LYDU0aRez/j9h1lMX+AAAAAJAAExkAAAAAucNE\nBgAAAEDusEamSoMGDXLZmDFjXNa2bduicemmcJJ00EEHuSz2ednYxkSbbbaZyyZOnFg0/vznP++O\naSAt97nw2Lqm2Pe73mKfcd1ggw1q+py//e1vXXbiiSe6rNbrZlptjUwSG2+8scsuvvhil/Xu3dtl\nO++8c9nzJ10jU40PPvigaBxbx9ZAWu5aGLPVVlsVjZ955hl3TGyT19g1IrbB37x586qorryePXu6\nLLYBbanYWoTY/1ulG3rWQMuvkTnhhBOKxldccYU7JrYp5A477OCyWvdbrbFGBgAAAABSxEQGAAAA\nQO4wkQEAAACQO2UnMmY22sxmm9kLq2Qbm9l9ZvZK4b8N/YFjAAAAAM2lbflDdL2kqyT9YZXsDEkP\nhBBGmNkZhfHp6ZfXWAYMGOCy2OKv0oX9kjR79uyi8Ve/+lV3TGwx4/777++y2ML+mGuvvTbRcchG\nbBPIfv3qssZ3jWI3sIhtPLfnnnu6bLfddqvoOb///e+7LLY4MrZBLGrrP//5j8uOPfZYl3Xq1Mll\nsX646KKLisaxxf477rijy6rZhLO0tuHDh7tjYhsNoz7WWcf/m+rppxe/pIgt7I956aWXXJbFQuvS\nn/mSNGPGjKJx7IYAsRsTHH/88S6L9TAqF/u6n3feeUXjNm3auGP+9re/uSzvC/tjG3PHrtONouw7\nMiGERyWV/iQ7QNJ/b801RtKBKdcFAAAAAKtV6RqZbiGE/977711J3VKqBwAAAADKSvLRsjUKIYQ1\n3QfczIZIGlLt8wDVoA+RNXoQjYA+RNboQaSp0ndkZplZd0kq/Nd/GLQghDAyhNCvXht8ATH0IbJG\nD6IR0IfIGj2INFX6jsydkgZLGlH47x2pVdQgunXzn5a79957XRZbFLVkyRKXHXPMMUXj2MJ+oBHc\ndNNNiY7r2rWry0p345b8jQKuvvpqd0xsIffJJ5/sstgNEkoXZCIbH374ocvuu+++RFmpQw45xGWf\n+cxnXFbpzR9CaNjNxFvSXnvt5bLYDSVKLV261GWf+9znUqmpWrFr1SOPPFI0PuKIIxKda99993VZ\n7Lq3bNmyhNWh1HHHHeeyjTfeuGgc+55eeOGFNaspKwcddJDLunTxNyeeO3duPcopK8ntl2+S9E9J\n25nZW2Z2tFZOYPY2s1ckDSyMAQAAAKAuyr4jE0Lw92Jd6Usp1wIAAAAAiVS6RgYAAAAAMsNEBgAA\nAEDuVH375WYxYMCAonHShf0xF198scvGjRtXUV1Ao4rtXB3LJk6cWDT+5Cc/6Y4p3cVbit8A4OMf\n//jalIic+vrXv+6yQYNW9ynn8m677baicTMu0M2z2M11koj9XF2wYEG15dRM6QL9pIv9+/bt67Jt\nttnGZVOnTq2ssBYT+3p++9vfLvu40uuIJE2fPj2VmhrJ9773vUTH3XDDDTWuJBnekQEAAACQO0xk\nAAAAAOQOExkAAAAAucNEBgAAAEDutORi/9gOpVdddVXROOnC/rvuustlWSwkXbhwoctitQH1tuWW\nWxaNv/Wtb2VUCRrRj3/8Y5cddthhFZ9v0qRJLiu9viM7HTp0cNnw4cMrOte0adOqrKa+SneGj/3c\njn19kK6TTz7ZZdtuu23Zx5122mm1KCdTsde6W2yxhcuWLl3qsnPPPbcWJa013pEBAAAAkDtMZAAA\nAADkDhMZAAAAALnT9GtkzMxl1113nct23HHHsue68847XRb7vH/ss4SVGjZsWKLjli9f7rLY5oRA\nLcU2tnv88cfLHhMzZcoUl1X6WXo0rthmqLHrdlKxjTPfeOONis+HdF122WUuS7I+Yf78+S675JJL\nUqmpXt55552i8dNPP+2O2X333etVTsu66aabXBa7brRp06Ye5WRq3333ddn222/vsoceeqge5VSE\nd2QAAAAA5A4TGQAAAAC5w0QGAAAAQO6UnciY2Wgzm21mL6ySnWtmb5vZs4Vf/kN2AAAAAFAjSRb7\nXy/pKkl/KMkvDyH4VXsNJraB0de//vWyj3v99dddduCBB7ps5513dtnee+/tso9//ONF45122qls\nDZLUp0+fRMett956LvvBD35QNI7d5GDBggWJzg+U6t27t8vuuOMOlyVd3F9q1113ddmiRYsqOhca\nR+l1KbZBcQgh0bmef/55l82ZM6eywlAXvXr1quhxRx99tMtmzZpVZTVoRbHF7LGF/U8++WTRuBlv\noPSHP5S+tI974YUXyh+UkbLvyIQQHpX0nzrUAgAAAACJVLNG5kQze77w0TP/T2oAAAAAUCOVTmSu\nkbSVpD6SZkr6xeoONLMhZvaUmT1V4XMBVaMPkTV6EI2APkTW6EGkqaINMUMI//tgqpmNkjRuDceO\nlDSycGyyDz4DKaMPkTV6EI2APkTW6EGkqaKJjJl1DyHMLAwPktQQq4BiiwjPPvvsRI+dO3du0fh3\nv/udO+b+++93WWwX3vbt2yd6zjS1a9fOZb/61a+Kxj/5yU/cMQ8//LDL/vrXv7rs1ltvrbw45F6P\nHj1cNnbsWJf17NmzovP/6U9/ctmSJUsqOhca28CBA4vGZpbocR999JHL+vbtm0pNaHzvvPNO1iVU\nbaONNioaV3rjA9THihUrisZJb0LSyH70ox8VjTt06OCOmTFjhstirx8bRdmJjJndJGmApE3N7C1J\n50gaYGZ9JAVJ0yQNrWGNAAAAAFCk7EQmhDAoEl9bg1oAAAAAIJFq7loGAAAAAJlgIgMAAAAgdypa\n7N8IYruFT5w40WWxhUwxpbtLX3zxxe6Y0oVfknTXXXe57M033yx73OTJkxPVdffdd7tsl112cVls\nIeyoUaOKxoceeqg75pvf/Gai7Nlnn3XZK6+84jLkS2xX9VtuucVlu+22m8s6duyY6DnefffdsueK\n7Zi8fPnyROdH4+rUqZPLPvaxjxWNYwtoFy9e7LIzzzwzvcKADBx55JFF40984hOJHjdr1iyXNeMu\n80jXaaed5rKLLrqoaBy72cp5553nsoULF6ZXWMp4RwYAAABA7jCRAQAAAJA7TGQAAAAA5A4TGQAA\nAAC5k9vF/v3793dZ165dKz7fokWLisaxRfyxHe9vvvnmip8ziWXLliU6LrYL+g9/+MM1jqX413HT\nTTd1GQv7m8OJJ55YND7ggAPcMXvttVeqz1n6/8306dNTPT8a18iRI1322c9+tuzjhg8f7rIrrrgi\nlZqAelhvvfVcdvrpp1d0rueee85l7733XkXnQnI77bRT0Th2c5y5c+fWq5w1+sY3vuGy0oX9krTO\nOsXvX3z/+993x1x33XXpFVYHvCMDAAAAIHeYyAAAAADIHSYyAAAAAHKHiQwAAACA3MntYv9PfepT\nFT928uTJLvvOd75TNH755ZcrPn8W7rjjjooeN2HChJQrQaOI7YR+9tlnF43bt2+f6nPecMMNLhs2\nbFiqz4HG1K9fP5fts88+FZ3rscceq7YcNJnDDjvMZY8//ngGlSSz5ZZbuqx79+5lHzdr1iyXHXnk\nkanUhJUWL16c6LiOHTsWjQ866CB3zOjRo1OpaXU22WQTlx111FEuO++881xmZi675pprisbXXntt\nFdU1Bt6RAQAAAJA7TGQAAAAA5E7ZiYyZbW5mD5nZi2b2LzM7qZBvbGb3mdkrhf/6G2wDAAAAQA0k\nWSOzTNKPQghPm1lnSZPN7D5J35X0QAhhhJmdIekMSZXt9lSB0s/5SdKcOXNc9sYbb7js/vvvd1nS\njScb1ezZs7MuAXXStq3/3/acc85xWWzztdhjS82bN89l8+fPd9m4ceNcFlsPE9usFc1nt912c9mG\nG25Y9nGxfmOzP5T67ne/67IOHTq4bOjQoS5bvnx5anVsvvnmLottNh1bx5DECy+84DJ+vqcr9vox\ntsakdHPw3/72t+6YzTbbzGW//vWvXbZw4UKXlW5Oefzxx7tjLrnkEpfFNluNee2111x2wgknJHps\nnpR9RyaEMDOE8HTh9/MlTZXUQ9IBksYUDhsj6cBaFQkAAAAAq1qrNTJm1ktSX0lPSOoWQphZ+KN3\nJXVLtTIAAAAAWI3Et182s06S/iJpWAjhg1Vv6xZCCGYWVvO4IZKGVFsoUA36EFmjB9EI6ENkjR5E\nmhK9I2Nm7bRyEnNjCGFsIZ5lZt0Lf95dUvRDnCGEkSGEfiEEv8kAUCf0IbJGD6IR0IfIGj2INFkI\n0TdS/t8BK996GSPpPyGEYavkl0p6b5XF/huHEH5c5lxrfjI448ePd9nnP/95l8UW2j7xxBM1qalG\nJtfropb3PjzrrLNcdv7556d2/lNOOcVlV1xxRWrnb2QhBL+DWA3kvQdjXnnlFZdttdVWZR8Xu3Y9\n+eSTqdSUU019LfzCF77gstiNQzp37lzR+d98802XxRZalx637bbbJjr/Bhts4LKNNtooYXXFrrzy\nSpfFruVz586t6PxVqksfNsq18Itf/KLLSm8MleRmOVL8BiaxG06UbljZpUuym//GXrfHXisOGjTI\nZe+8806i52gQiXowyXelv6QjJE0xs2cL2ZmSRkj6s5kdLWm6pEMqrRQAAAAA1kbZiUwIYbyk1f0r\n5ZfSLQcAAAAAyluru5YBAAAAQCNgIgMAAAAgd8ou9k/1yRpkUVeeJF3sH1sYuWDBgprUVCNNvcA1\nqYEDBxaNY7v67rDDDi5bd911K3q+F1980WX77befy6ZPn17R+fOGxf6VW7Rokcvat29f9nFt2rSp\nRTl51nLXwtgNAH71q18VjXv37l2vclIxY8YMl/385z8vGt90003umPnz59esprXUUov9Y7beeuui\n8cSJE90xm2yySU1riL2O+/3vf++yk08+uaZ1ZCRRD/KODAAAAIDcYSIDAAAAIHeYyAAAAADIHSYy\nAAAAAHIn2TalAFIXW+S89957F4379OlT8flju0HvvvvuReNXX33VHbN06dKKnxNYW5MnT3bZ9ddf\nn+ixTz/9tMsmTJhQbUmos8cee8xlu+yyS9H4y1/+sjvmm9/8pssOP/xwlyW5oURskf3YsWNd9swz\nz7jsxhtvdFns5hcLFy4sWwcax2uvvVY0jv08ji2yP+igg1y22WabuezBBx8sW8MvfvELl8X+f2ll\nvCMDAAAAIHeYyAAAAADIHSYyAAAAAHKHiQwAAACA3LEQ6repaiPv4Nqoxo8f77INN9zQZbFFaMuX\nL69JTTXScrtZd+zY0WVJdnWOLcaPLY6+5ZZbXJZkcWErCyFYPZ6nUXowTV/5yldcNmLECJd95jOf\nKRqb+S950p9Lsf9f3njjDZcNHTq0aPzkk08mOn9GWu5aiIZUlz6kB7EGiXqQd2QAAAAA5A4TGQAA\nAAC5U3YiY2abm9lDZvaimf3LzE4q5Oea2dtm9mzh1761LxcAAAAAkm2IuUzSj0IIT5tZZ0mTzey+\nwp9dHkK4rHblAQAAAIBXdiITQpgpaWbh9/PNbKqkHrUuDKv39ttvuyxnC/shafHixS674oorisbD\nhg1zxyxbtsxlpYuZgXq79957XRbbBf3YY48tGscW+5922mku69Spk8tiC/svu8z/21qDL+4HAFRo\nrdbImFkvSX0lPVGITjSz581stJl1Sbk2AAAAAIhKPJExs06S/iJpWAjhA0nXSNpKUh+tfMfmF6t5\n3BAze8rMnkqhXqAi9CGyRg+iEdCHyBo9iDQlWSMjM2unlZOYG0MIYyUphDBrlT8fJWlc7LEhhJGS\nRhaO437hyAR9iKzRg2gE9CGyRg8iTWU3xLSVH2AeI+k/IYRhq+TdC+tnZGYnS/pcCOGwMueiYbE6\nbAKHzLEhJhoA10I0AjbERNYS9WCSd2T6SzpC0hQze7aQnSlpkJn1kRQkTZPEamMAAAAAdZHkrmXj\nJcX+lfLu9MsBAAAAgPLW6q5lAAAAANAImMgAAAAAyB0mMgAAAAByh4kMAAAAgNxhIgMAAAAgd5jI\nAAAAAMgdJjIAAAAAcifJhphpmiNpuqRNC7/Pq7zXLzXe32GLOj4XfdgYGq3+LHpQaryvw9qi/nRx\nLVx71J++evUh18LG0Wj1J+pBCyHUuhD/pGZPhRD61f2JU5L3+qXm+DtUK+9fA+pvDnn/OlB//uX9\na0D9zSHvXwfqzwYfLQMAAACQO0xkAAAAAOROVhOZkRk9b1ryXr/UHH+HauX9a0D9zSHvXwfqz7+8\nfw2ovznk/etA/RnIZI0MAAAAAFSDj5YBAAAAyJ26T2TMbB8ze8nMXjWzM+r9/GvLzEab2Wwze2GV\nbGMzu8/MXin8t0uWNa6JmW1uZg+Z2Ytm9i8zO6mQ5+bvkLa89aBEHzajvPUhPdic6MP6og+9vPWg\nlO8+bLYerOtExszaSLpa0lcl7ShpkJntWM8aKnC9pH1KsjMkPRBC2EbSA4Vxo1om6UchhB0l7Srp\nhMLXPE9/h9TktAcl+rCp5LQPrxc92FTow0zQh6vIaQ9K+e7DpurBer8js4ukV0MIr4cQlki6WdIB\nda5hrYQQHpX0n5L4AEljCr8fI+nAuha1FkIIM0MITxd+P1/SVEk9lKO/Q8py14MSfdiEcteH9GBT\nog/rjD50cteDUr77sNl6sN4TmR6S3lxl/FYhy5tuIYSZhd+/K6lblsUkZWa9JPWV9IRy+ndIQbP0\noJTT7yF9KKl5+jCX3z968H/owwzRh5KapwelHH4Pm6EHWexfpbDytm8Nf+s3M+sk6S+ShoUQPlj1\nz/Lyd8Dq5eV7SB82r7x8/+jB5paX7yF92Nzy8D1slh6s90TmbUmbrzL+RCHLm1lm1l2SCv+dnXE9\na2Rm7bSyWW8MIYwtxLn6O6SoWXpQytn3kD4s0ix9mKvvHz3o0IcZoA+LNEsPSjn6HjZTD9Z7IjNJ\n0jZmtqWZtZd0mKQ761xDGu6UNLjw+8GS7siwljUyM5N0raSpIYRfrvJHufk7pKxZelDK0feQPnSa\npQ9z8/2jB6PowzqjD51m6UEpJ9/DpuvBEEJdf0naV9LLkl6T9NN6P38F9d4kaaakpVr52c2jJW2i\nlXd0eEXS/ZI2zrrONdS/u1a+Pfi8pGcLv/bN09+hBl+TXPVgoWb6sMl+5a0P6cHm/EUf1r1++tB/\nTXLVg4Wac9uHzdaDVvhLAQAAAEBusNgfAAAAQO4wkQEAAACQO0xkAAAAAOQOExkAAAAAucNEBgAA\nAEDuMJEBAAAAkDtMZAAAAADkDhMZAAAAALnz/wMp3hKQuCozkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b8837fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b8bf99358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_10_images_from_dataset(testset, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4616\n",
      "Loss: 2.2623\n",
      "Loss: 2.2687\n",
      "Loss: 2.3062\n",
      "Loss: 1.9459\n",
      "Loss: 1.8585\n",
      "Loss: 1.7999\n",
      "Loss: 1.8672\n",
      "Loss: 1.5682\n",
      "Loss: 1.6160\n",
      "Loss: 1.5095\n",
      "Loss: 1.4167\n",
      "Loss: 1.3357\n",
      "Loss: 1.2428\n",
      "Loss: 1.2929\n",
      "Loss: 1.2513\n",
      "Loss: 1.0737\n",
      "Loss: 1.0431\n",
      "Loss: 0.9244\n",
      "Loss: 0.8819\n",
      "Loss: 0.8752\n",
      "Loss: 0.9034\n",
      "Loss: 0.9561\n",
      "Loss: 0.9420\n",
      "Loss: 0.6420\n",
      "Loss: 0.8193\n",
      "Loss: 0.9044\n",
      "Loss: 0.8192\n",
      "Loss: 0.6635\n",
      "Loss: 0.9786\n",
      "Loss: 0.8285\n",
      "Loss: 0.8810\n",
      "Loss: 0.7102\n",
      "Loss: 0.8722\n",
      "Loss: 0.7622\n",
      "Loss: 0.9330\n",
      "Loss: 0.8628\n",
      "Loss: 0.5893\n",
      "Loss: 0.8791\n",
      "Loss: 0.8498\n",
      "Loss: 0.6238\n",
      "Loss: 0.8118\n",
      "Loss: 0.6374\n",
      "Loss: 0.6403\n",
      "Loss: 0.5777\n",
      "Loss: 0.6154\n",
      "Loss: 0.8124\n",
      "Loss: 0.7551\n",
      "Loss: 0.4743\n",
      "Loss: 0.9171\n",
      "Loss: 0.6680\n",
      "Loss: 0.4804\n",
      "Loss: 0.4202\n",
      "Loss: 0.7785\n",
      "Loss: 0.7798\n",
      "Loss: 0.6439\n",
      "Loss: 0.7925\n",
      "Loss: 0.5243\n",
      "Loss: 0.8232\n",
      "Loss: 1.1416\n",
      "Loss: 0.9142\n",
      "Loss: 0.5445\n",
      "Loss: 0.5664\n",
      "Loss: 0.5694\n",
      "Loss: 0.7296\n",
      "Loss: 0.7483\n",
      "Loss: 0.6519\n",
      "Loss: 0.6084\n",
      "Loss: 0.5755\n",
      "Loss: 0.6159\n",
      "Loss: 0.7813\n",
      "Loss: 0.8101\n",
      "Loss: 0.8068\n",
      "Loss: 1.1041\n",
      "Loss: 0.6524\n",
      "Loss: 0.7702\n",
      "Loss: 0.5993\n",
      "Loss: 0.5065\n",
      "Loss: 0.6113\n",
      "Loss: 0.6083\n",
      "Loss: 0.7676\n",
      "Loss: 0.5566\n",
      "Loss: 0.8223\n",
      "Loss: 0.8087\n",
      "Loss: 0.4615\n",
      "Loss: 0.7688\n",
      "Loss: 0.8530\n",
      "Loss: 0.6136\n",
      "Loss: 0.3724\n",
      "Loss: 0.5497\n",
      "Loss: 0.6407\n",
      "Loss: 0.4824\n",
      "Loss: 0.5681\n",
      "Loss: 0.4612\n",
      "Loss: 0.4049\n",
      "Loss: 0.5048\n",
      "Loss: 0.5059\n",
      "Loss: 0.3631\n",
      "Loss: 0.4935\n",
      "Loss: 0.4644\n",
      "Loss: 0.4627\n",
      "Loss: 0.5506\n",
      "Loss: 0.3552\n",
      "Loss: 0.3457\n",
      "Loss: 0.4223\n",
      "Loss: 0.3638\n",
      "Loss: 0.4555\n",
      "Loss: 0.5683\n",
      "Loss: 0.5084\n",
      "Loss: 0.6139\n",
      "Loss: 0.3561\n",
      "Loss: 0.3619\n",
      "Loss: 0.5608\n",
      "Loss: 0.4616\n",
      "Loss: 0.3960\n",
      "Loss: 0.5241\n",
      "Loss: 0.6519\n",
      "Loss: 0.4764\n",
      "Loss: 0.4469\n",
      "Loss: 0.3596\n",
      "Loss: 0.5653\n",
      "Loss: 0.5156\n",
      "Loss: 0.7987\n",
      "Loss: 0.6775\n",
      "Loss: 0.8012\n",
      "Loss: 0.4932\n",
      "Loss: 0.5695\n",
      "Loss: 0.5412\n",
      "Loss: 0.3161\n",
      "Loss: 0.4107\n",
      "Loss: 0.3143\n",
      "Loss: 0.6117\n",
      "Loss: 0.5519\n",
      "Loss: 0.4310\n",
      "Loss: 0.4528\n",
      "Loss: 0.6208\n",
      "Loss: 0.6337\n",
      "Loss: 0.5806\n",
      "Loss: 0.6459\n",
      "Loss: 0.5501\n",
      "Loss: 1.0889\n",
      "Loss: 0.6188\n",
      "Loss: 0.3678\n",
      "Loss: 0.6154\n",
      "Loss: 0.5195\n",
      "Loss: 0.6008\n",
      "Loss: 0.5994\n",
      "Loss: 0.4818\n",
      "Loss: 0.3397\n",
      "Loss: 0.5009\n",
      "Loss: 0.5956\n",
      "Loss: 0.6631\n",
      "Loss: 0.3033\n",
      "Loss: 0.3443\n",
      "Loss: 0.3661\n",
      "Loss: 0.5735\n",
      "Loss: 0.8253\n",
      "Loss: 0.5399\n",
      "Loss: 0.2460\n",
      "Loss: 0.6103\n",
      "Loss: 0.5278\n",
      "Loss: 0.3016\n",
      "Loss: 0.7616\n",
      "Loss: 0.3581\n",
      "Loss: 0.6241\n",
      "Loss: 0.6071\n",
      "Loss: 0.2660\n",
      "Loss: 0.5084\n",
      "Loss: 0.6929\n",
      "Loss: 0.6686\n",
      "Loss: 0.5319\n",
      "Loss: 0.5198\n",
      "Loss: 0.4971\n",
      "Loss: 0.3130\n",
      "Loss: 0.3810\n",
      "Loss: 0.4776\n",
      "Loss: 0.3751\n",
      "Loss: 0.5262\n",
      "Loss: 0.5847\n",
      "Loss: 0.4147\n",
      "Loss: 1.0081\n",
      "Loss: 0.6562\n",
      "Loss: 0.7714\n",
      "Loss: 0.4018\n",
      "Loss: 0.3387\n",
      "Loss: 0.5060\n",
      "Loss: 0.6864\n",
      "Loss: 0.3463\n",
      "Loss: 0.9258\n",
      "Loss: 0.5689\n",
      "Loss: 0.5029\n",
      "Loss: 0.2791\n",
      "Loss: 0.5064\n",
      "Loss: 0.7162\n",
      "Loss: 0.9503\n",
      "Loss: 0.7706\n",
      "Loss: 0.5334\n",
      "Loss: 0.7142\n",
      "Loss: 0.5468\n",
      "Loss: 0.4680\n",
      "Loss: 0.5346\n",
      "Loss: 0.3577\n",
      "Loss: 0.4539\n",
      "Loss: 0.5988\n",
      "Loss: 0.8486\n",
      "Loss: 0.5251\n",
      "Loss: 0.3513\n",
      "Loss: 0.8071\n",
      "Loss: 0.3385\n",
      "Loss: 0.3458\n",
      "Loss: 0.7672\n",
      "Loss: 0.5986\n",
      "Loss: 0.3748\n",
      "Loss: 0.5346\n",
      "Loss: 0.6651\n",
      "Loss: 0.7060\n",
      "Loss: 0.3089\n",
      "Loss: 0.3123\n",
      "Loss: 0.4823\n",
      "Loss: 0.5656\n",
      "Loss: 0.5826\n",
      "Loss: 0.6617\n",
      "Loss: 0.7189\n",
      "Loss: 0.5287\n",
      "Loss: 1.0457\n",
      "Loss: 0.5443\n",
      "Loss: 0.5592\n",
      "Loss: 0.4268\n",
      "Loss: 0.5496\n",
      "Loss: 0.8281\n",
      "Loss: 0.2701\n",
      "Loss: 0.4742\n",
      "Loss: 0.7734\n",
      "Loss: 0.5355\n",
      "Loss: 0.5746\n",
      "Loss: 0.5162\n",
      "Loss: 0.5794\n",
      "Loss: 0.4154\n",
      "Loss: 0.5395\n",
      "Loss: 0.4341\n",
      "Loss: 0.3951\n",
      "Loss: 0.7457\n",
      "Loss: 0.5920\n",
      "Loss: 0.5922\n",
      "Loss: 0.5333\n",
      "Loss: 0.3329\n",
      "Loss: 0.9013\n",
      "Loss: 0.3802\n",
      "Loss: 0.5442\n",
      "Loss: 0.4862\n",
      "Loss: 0.4633\n",
      "Loss: 0.5156\n",
      "Loss: 0.7939\n",
      "Loss: 0.7227\n",
      "Loss: 0.4730\n",
      "Loss: 0.3706\n",
      "Loss: 0.6891\n",
      "Loss: 0.4992\n",
      "Loss: 0.5920\n",
      "Loss: 0.4109\n",
      "Loss: 0.5846\n",
      "Loss: 0.4111\n",
      "Loss: 0.4905\n",
      "Loss: 0.4662\n",
      "Loss: 0.4490\n",
      "Loss: 0.3806\n",
      "Loss: 0.3043\n",
      "Loss: 0.2153\n",
      "Loss: 0.4967\n",
      "Loss: 0.6346\n",
      "Loss: 0.4570\n",
      "Loss: 0.6542\n",
      "Loss: 0.4453\n",
      "Loss: 0.4115\n",
      "Loss: 0.3210\n",
      "Loss: 0.3969\n",
      "Loss: 0.4671\n",
      "Loss: 0.4463\n",
      "Loss: 0.2722\n",
      "Loss: 0.6704\n",
      "Loss: 0.5603\n",
      "Loss: 0.4002\n",
      "Loss: 0.6814\n",
      "Loss: 0.3246\n",
      "Loss: 0.3658\n",
      "Loss: 0.4499\n",
      "Loss: 0.6866\n",
      "Loss: 0.5176\n",
      "Loss: 0.5008\n",
      "Loss: 0.3147\n",
      "Loss: 0.5580\n",
      "Loss: 0.5748\n",
      "Loss: 0.3781\n",
      "Loss: 0.2691\n",
      "Loss: 0.5608\n",
      "Loss: 0.4585\n",
      "Loss: 0.6194\n",
      "Loss: 0.3689\n",
      "Loss: 0.4253\n",
      "Loss: 0.3593\n",
      "Loss: 0.6162\n",
      "Loss: 0.3170\n",
      "Loss: 0.5060\n",
      "Loss: 0.4654\n",
      "Loss: 0.4376\n",
      "Loss: 0.8769\n",
      "Loss: 0.3793\n",
      "Loss: 0.3859\n",
      "Loss: 0.5717\n",
      "Loss: 0.3544\n",
      "Loss: 0.6459\n",
      "Loss: 0.4938\n",
      "Loss: 0.2960\n",
      "Loss: 0.4165\n",
      "Loss: 0.3877\n",
      "Loss: 0.3235\n",
      "Loss: 0.5462\n",
      "Loss: 0.5003\n",
      "Loss: 0.3738\n",
      "Loss: 0.4763\n",
      "Loss: 0.3834\n",
      "Loss: 0.5366\n",
      "Loss: 0.4890\n",
      "Loss: 0.5048\n",
      "Loss: 0.3665\n",
      "Loss: 0.5615\n",
      "Loss: 0.4951\n",
      "Loss: 0.3332\n",
      "Loss: 0.5571\n",
      "Loss: 0.3276\n",
      "Loss: 0.4268\n",
      "Loss: 0.5432\n",
      "Loss: 0.3775\n",
      "Loss: 0.4099\n",
      "Loss: 0.5498\n",
      "Loss: 0.5800\n",
      "Loss: 0.5582\n",
      "Loss: 0.3626\n",
      "Loss: 0.5827\n",
      "Loss: 0.3552\n",
      "Loss: 0.7910\n",
      "Loss: 0.4607\n",
      "Loss: 0.4372\n",
      "Loss: 0.2572\n",
      "Loss: 0.6701\n",
      "Loss: 0.5336\n",
      "Loss: 0.6299\n",
      "Loss: 0.2954\n",
      "Loss: 0.4850\n",
      "Loss: 0.4365\n",
      "Loss: 0.3882\n",
      "Loss: 0.4287\n",
      "Loss: 0.5315\n",
      "Loss: 0.6359\n",
      "Loss: 0.4263\n",
      "Loss: 0.4065\n",
      "Loss: 0.3427\n",
      "Loss: 0.5349\n",
      "Loss: 0.3398\n",
      "Loss: 0.4635\n",
      "Loss: 0.5358\n",
      "Loss: 0.4481\n",
      "Loss: 0.3800\n",
      "Loss: 0.5976\n",
      "Loss: 0.4899\n",
      "Loss: 0.4744\n",
      "Loss: 0.3063\n",
      "Loss: 0.5163\n",
      "Loss: 0.4555\n",
      "Loss: 0.5230\n",
      "Loss: 0.3794\n",
      "Loss: 0.7661\n",
      "Loss: 0.4034\n",
      "Loss: 0.4500\n",
      "Loss: 0.4440\n",
      "Loss: 0.4394\n",
      "Loss: 0.4078\n",
      "Loss: 0.5002\n",
      "Loss: 0.3511\n",
      "Loss: 0.4337\n",
      "Loss: 0.2991\n",
      "Loss: 0.7771\n",
      "Loss: 0.3169\n",
      "Loss: 0.2788\n",
      "Loss: 0.7220\n",
      "Loss: 0.2496\n",
      "Loss: 0.7759\n",
      "Loss: 0.4751\n",
      "Loss: 0.4411\n",
      "Loss: 0.4566\n",
      "Loss: 0.4143\n",
      "Loss: 0.4321\n",
      "Loss: 0.4727\n",
      "Loss: 0.5029\n",
      "Loss: 0.5534\n",
      "Loss: 0.3229\n",
      "Loss: 0.5087\n",
      "Loss: 0.4032\n",
      "Loss: 0.3273\n",
      "Loss: 0.3290\n",
      "Loss: 0.3601\n",
      "Loss: 0.4091\n",
      "Loss: 0.3301\n",
      "Loss: 0.5504\n",
      "Loss: 0.3214\n",
      "Loss: 0.4117\n",
      "Loss: 0.4683\n",
      "Loss: 0.5451\n",
      "Loss: 0.5662\n",
      "Loss: 0.4219\n",
      "Loss: 0.4620\n",
      "Loss: 0.4889\n",
      "Loss: 0.4099\n",
      "Loss: 0.3905\n",
      "Loss: 0.2928\n",
      "Loss: 0.2827\n",
      "Loss: 0.4515\n",
      "Loss: 0.3770\n",
      "Loss: 0.7293\n",
      "Loss: 0.6723\n",
      "Loss: 0.5357\n",
      "Loss: 0.4687\n",
      "Loss: 0.7209\n",
      "Loss: 0.4211\n",
      "Loss: 0.5916\n",
      "Loss: 0.3764\n",
      "Loss: 0.6307\n",
      "Loss: 0.4605\n",
      "Loss: 0.5263\n",
      "Loss: 0.3403\n",
      "Loss: 0.5405\n",
      "Loss: 0.2212\n",
      "Loss: 0.2955\n",
      "Loss: 0.5630\n",
      "Loss: 0.5846\n",
      "Loss: 0.4816\n",
      "Loss: 0.8059\n",
      "Loss: 0.4175\n",
      "Loss: 0.4718\n",
      "Loss: 0.4672\n",
      "Loss: 0.3800\n",
      "Loss: 0.5999\n",
      "Loss: 0.5009\n",
      "Loss: 0.4884\n",
      "Loss: 0.4238\n",
      "Loss: 0.3324\n",
      "Loss: 0.2835\n",
      "Loss: 0.2854\n",
      "Loss: 0.3697\n",
      "Loss: 0.2945\n",
      "Loss: 0.4416\n",
      "Loss: 0.3408\n",
      "Loss: 0.2571\n",
      "Loss: 0.3184\n",
      "Loss: 0.5421\n",
      "Loss: 0.3616\n",
      "Loss: 0.4156\n",
      "Loss: 0.3264\n",
      "Loss: 0.5469\n",
      "Loss: 0.3640\n",
      "Loss: 0.1283\n",
      "Loss: 0.2844\n",
      "Loss: 0.3389\n",
      "Loss: 0.6840\n",
      "Loss: 0.3857\n",
      "Loss: 0.3060\n",
      "Loss: 0.4610\n",
      "Loss: 0.4309\n",
      "Loss: 0.3358\n",
      "Loss: 0.3594\n",
      "Loss: 0.2930\n",
      "Loss: 0.2050\n",
      "Loss: 0.6327\n",
      "Loss: 0.3841\n",
      "Loss: 0.3991\n",
      "Loss: 0.6249\n",
      "Loss: 0.7920\n",
      "Loss: 0.8226\n",
      "Loss: 0.2881\n",
      "Loss: 0.4108\n",
      "Loss: 0.2964\n",
      "Loss: 0.4324\n",
      "Loss: 0.7763\n",
      "Loss: 0.5533\n",
      "Loss: 0.5627\n",
      "Loss: 0.2879\n",
      "Loss: 0.3243\n",
      "Loss: 0.3827\n",
      "Loss: 0.2166\n",
      "Loss: 0.2598\n",
      "Loss: 0.3612\n",
      "Loss: 0.4857\n",
      "Loss: 0.6551\n",
      "Loss: 0.3499\n",
      "Loss: 0.3770\n",
      "Loss: 0.4011\n",
      "Loss: 0.3225\n",
      "Loss: 0.6388\n",
      "Loss: 0.2935\n",
      "Loss: 0.2533\n",
      "Loss: 0.5639\n",
      "Loss: 0.6246\n",
      "Loss: 0.5892\n",
      "Loss: 0.4082\n",
      "Loss: 0.7108\n",
      "Loss: 0.4996\n",
      "Loss: 0.4141\n",
      "Loss: 0.3437\n",
      "Loss: 0.3003\n",
      "Loss: 0.2630\n",
      "Loss: 0.2989\n",
      "Loss: 0.4879\n",
      "Loss: 0.4748\n",
      "Loss: 0.2661\n",
      "Loss: 0.2179\n",
      "Loss: 0.5489\n",
      "Loss: 0.6702\n",
      "Loss: 0.4912\n",
      "Loss: 0.5852\n",
      "Loss: 0.5248\n",
      "Loss: 0.4798\n",
      "Loss: 0.3958\n",
      "Loss: 0.4397\n",
      "Loss: 0.2556\n",
      "Loss: 0.6624\n",
      "Loss: 0.3231\n",
      "Loss: 0.3485\n",
      "Loss: 0.4423\n",
      "Loss: 0.3493\n",
      "Loss: 0.3847\n",
      "Loss: 0.5707\n",
      "Loss: 0.4374\n",
      "Loss: 0.6499\n",
      "Loss: 0.4231\n",
      "Loss: 0.3916\n",
      "Loss: 0.2718\n",
      "Loss: 0.3710\n",
      "Loss: 0.3987\n",
      "Loss: 0.6048\n",
      "Loss: 0.2581\n",
      "Loss: 0.3456\n",
      "Loss: 0.3289\n",
      "Loss: 0.3561\n",
      "Loss: 0.3654\n",
      "Loss: 0.5201\n",
      "Loss: 0.2678\n",
      "Loss: 0.3805\n",
      "Loss: 0.1776\n",
      "Loss: 0.5063\n",
      "Loss: 0.3214\n",
      "Loss: 0.3035\n",
      "Loss: 0.4356\n",
      "Loss: 0.6392\n",
      "Loss: 0.5581\n",
      "Loss: 0.4488\n",
      "Loss: 0.3308\n",
      "Loss: 0.5436\n",
      "Loss: 0.4916\n",
      "Loss: 0.8371\n",
      "Loss: 0.4399\n",
      "Loss: 0.7166\n",
      "Loss: 0.3481\n",
      "Loss: 0.5592\n",
      "Loss: 0.1957\n",
      "Loss: 0.6562\n",
      "Loss: 0.2271\n",
      "Loss: 0.3201\n",
      "Loss: 0.4854\n",
      "Loss: 0.5390\n",
      "Loss: 0.3106\n",
      "Loss: 0.6640\n",
      "Loss: 0.4056\n",
      "Loss: 0.4467\n",
      "Loss: 0.4425\n",
      "Loss: 0.3170\n",
      "Loss: 0.4059\n",
      "Loss: 0.2320\n",
      "Loss: 0.3860\n",
      "Loss: 0.4788\n",
      "Loss: 0.4850\n",
      "Loss: 0.3389\n",
      "Loss: 0.4528\n",
      "Loss: 0.4506\n",
      "Loss: 0.4980\n",
      "Loss: 0.3779\n",
      "Loss: 0.7441\n",
      "Loss: 0.6545\n",
      "Loss: 0.3650\n",
      "Loss: 0.2083\n",
      "Loss: 0.5279\n",
      "Loss: 0.5429\n",
      "Loss: 0.2747\n",
      "Loss: 0.5620\n",
      "Loss: 0.4168\n",
      "Loss: 0.4494\n",
      "Loss: 0.4217\n",
      "Loss: 0.2789\n",
      "Loss: 0.3679\n",
      "Loss: 0.4033\n",
      "Loss: 0.3569\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print('Loss: %.4f' % cost)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.235685110092163\n",
      "Loss: 2.6122305393218994\n",
      "Loss: 2.633882761001587\n",
      "Loss: 2.463536262512207\n",
      "Loss: 2.253995418548584\n",
      "Loss: 2.227691173553467\n",
      "Loss: 2.2037668228149414\n",
      "Loss: 2.2376036643981934\n",
      "Loss: 2.2737526893615723\n",
      "Loss: 2.1525075435638428\n",
      "Loss: 2.1420462131500244\n",
      "Loss: 2.104735851287842\n",
      "Loss: 2.2754225730895996\n",
      "Loss: 2.1633718013763428\n",
      "Loss: 2.124685525894165\n",
      "Loss: 2.085428237915039\n",
      "Loss: 2.080772876739502\n",
      "Loss: 1.9635969400405884\n",
      "Loss: 1.9688693284988403\n",
      "Loss: 2.071608543395996\n",
      "Loss: 1.9973366260528564\n",
      "Loss: 1.8948277235031128\n",
      "Loss: 1.8804954290390015\n",
      "Loss: 1.899318814277649\n",
      "Loss: 1.8611576557159424\n",
      "Loss: 1.9064974784851074\n",
      "Loss: 1.904478907585144\n",
      "Loss: 1.9299267530441284\n",
      "Loss: 1.7353285551071167\n",
      "Loss: 1.746910572052002\n",
      "Loss: 1.9866044521331787\n",
      "Loss: 1.872605323791504\n",
      "Loss: 1.9907395839691162\n",
      "Loss: 1.8297711610794067\n",
      "Loss: 1.760430932044983\n",
      "Loss: 1.9318454265594482\n",
      "Loss: 1.8175140619277954\n",
      "Loss: 1.8376332521438599\n",
      "Loss: 1.6989854574203491\n",
      "Loss: 1.582572340965271\n",
      "Loss: 1.8445755243301392\n",
      "Loss: 1.7279452085494995\n",
      "Loss: 1.8538498878479004\n",
      "Loss: 1.6030325889587402\n",
      "Loss: 1.6972392797470093\n",
      "Loss: 1.833166480064392\n",
      "Loss: 1.6762057542800903\n",
      "Loss: 1.7540795803070068\n",
      "Loss: 1.7157599925994873\n",
      "Loss: 1.5768641233444214\n",
      "Loss: 1.4103089570999146\n",
      "Loss: 1.5442253351211548\n",
      "Loss: 1.577795386314392\n",
      "Loss: 1.6203781366348267\n",
      "Loss: 1.6437221765518188\n",
      "Loss: 1.670914888381958\n",
      "Loss: 1.6315300464630127\n",
      "Loss: 1.7126332521438599\n",
      "Loss: 1.8169559240341187\n",
      "Loss: 1.4232901334762573\n",
      "Loss: 1.5889582633972168\n",
      "Loss: 1.4107666015625\n",
      "Loss: 1.5896573066711426\n",
      "Loss: 1.4864424467086792\n",
      "Loss: 1.431215524673462\n",
      "Loss: 1.6170637607574463\n",
      "Loss: 1.5263373851776123\n",
      "Loss: 1.5201152563095093\n",
      "Loss: 1.66840398311615\n",
      "Loss: 1.4270063638687134\n",
      "Loss: 1.4221171140670776\n",
      "Loss: 1.619748830795288\n",
      "Loss: 1.5797163248062134\n",
      "Loss: 1.518632411956787\n",
      "Loss: 1.5847445726394653\n",
      "Loss: 1.669315218925476\n",
      "Loss: 1.4403001070022583\n",
      "Loss: 1.5390825271606445\n",
      "Loss: 1.648163914680481\n",
      "Loss: 1.5826245546340942\n",
      "Loss: 1.3668900728225708\n",
      "Loss: 1.497296929359436\n",
      "Loss: 1.6096783876419067\n",
      "Loss: 1.7891534566879272\n",
      "Loss: 1.590016484260559\n",
      "Loss: 1.3957737684249878\n",
      "Loss: 1.5452258586883545\n",
      "Loss: 1.5920830965042114\n",
      "Loss: 1.62259840965271\n",
      "Loss: 1.413475751876831\n",
      "Loss: 1.4523564577102661\n",
      "Loss: 1.3758866786956787\n",
      "Loss: 1.5771470069885254\n",
      "Loss: 1.4463021755218506\n",
      "Loss: 1.3915131092071533\n",
      "Loss: 1.313422679901123\n",
      "Loss: 1.5220309495925903\n",
      "Loss: 1.5391067266464233\n",
      "Loss: 1.5877798795700073\n",
      "Loss: 1.497165322303772\n",
      "Loss: 1.3383429050445557\n",
      "Loss: 1.379634976387024\n",
      "Loss: 1.3977731466293335\n",
      "Loss: 1.4110832214355469\n",
      "Loss: 1.4959138631820679\n",
      "Loss: 1.4388912916183472\n",
      "Loss: 1.1972342729568481\n",
      "Loss: 1.4660602807998657\n",
      "Loss: 1.4974255561828613\n",
      "Loss: 1.2635573148727417\n",
      "Loss: 1.6518816947937012\n",
      "Loss: 1.2052518129348755\n",
      "Loss: 1.3269137144088745\n",
      "Loss: 1.4603450298309326\n",
      "Loss: 1.7873073816299438\n",
      "Loss: 1.3309401273727417\n",
      "Loss: 1.2543457746505737\n",
      "Loss: 1.335268497467041\n",
      "Loss: 1.3518387079238892\n",
      "Loss: 1.397591233253479\n",
      "Loss: 1.6119184494018555\n",
      "Loss: 1.6606438159942627\n",
      "Loss: 1.3041276931762695\n",
      "Loss: 1.3731377124786377\n",
      "Loss: 1.409706711769104\n",
      "Loss: 1.4399464130401611\n",
      "Loss: 1.3342857360839844\n",
      "Loss: 1.457720160484314\n",
      "Loss: 1.2296181917190552\n",
      "Loss: 1.4223198890686035\n",
      "Loss: 1.383384108543396\n",
      "Loss: 1.2598140239715576\n",
      "Loss: 1.2187786102294922\n",
      "Loss: 1.2607779502868652\n",
      "Loss: 1.0820035934448242\n",
      "Loss: 1.6282519102096558\n",
      "Loss: 1.3300622701644897\n",
      "Loss: 1.098014235496521\n",
      "Loss: 1.4603869915008545\n",
      "Loss: 1.431526780128479\n",
      "Loss: 1.1109938621520996\n",
      "Loss: 1.2632489204406738\n",
      "Loss: 1.4098953008651733\n",
      "Loss: 1.1122719049453735\n",
      "Loss: 1.2248601913452148\n",
      "Loss: 1.6237895488739014\n",
      "Loss: 1.763085126876831\n",
      "Loss: 1.632572889328003\n",
      "Loss: 1.3014280796051025\n",
      "Loss: 1.3012524843215942\n",
      "Loss: 1.250795841217041\n",
      "Loss: 1.4027079343795776\n",
      "Loss: 1.3468444347381592\n",
      "Loss: 1.5457476377487183\n",
      "Loss: 1.277178168296814\n",
      "Loss: 1.196873664855957\n",
      "Loss: 1.2334718704223633\n",
      "Loss: 1.0899207592010498\n",
      "Loss: 1.5114836692810059\n",
      "Loss: 1.340662956237793\n",
      "Loss: 1.253896951675415\n",
      "Loss: 1.2866239547729492\n",
      "Loss: 1.342124581336975\n",
      "Loss: 1.324912428855896\n",
      "Loss: 1.2508654594421387\n",
      "Loss: 1.2259007692337036\n",
      "Loss: 1.2639505863189697\n",
      "Loss: 1.392605185508728\n",
      "Loss: 1.2682194709777832\n",
      "Loss: 1.2202367782592773\n",
      "Loss: 1.1189669370651245\n",
      "Loss: 1.4634824991226196\n",
      "Loss: 1.1182035207748413\n",
      "Loss: 1.617924451828003\n",
      "Loss: 1.312849998474121\n",
      "Loss: 1.7567520141601562\n",
      "Loss: 1.2642279863357544\n",
      "Loss: 1.0016878843307495\n",
      "Loss: 1.278499722480774\n",
      "Loss: 1.3395122289657593\n",
      "Loss: 1.0473235845565796\n",
      "Loss: 1.303757667541504\n",
      "Loss: 1.0364866256713867\n",
      "Loss: 1.3158003091812134\n",
      "Loss: 1.0334320068359375\n",
      "Loss: 1.3454898595809937\n",
      "Loss: 1.610535740852356\n",
      "Loss: 1.117316484451294\n",
      "Loss: 1.3290739059448242\n",
      "Loss: 1.1589869260787964\n",
      "Loss: 1.2751556634902954\n",
      "Loss: 1.118372917175293\n",
      "Loss: 1.0789942741394043\n",
      "Loss: 1.2052178382873535\n",
      "Loss: 1.25667142868042\n",
      "Loss: 1.087365746498108\n",
      "Loss: 1.26752507686615\n",
      "Loss: 1.3001240491867065\n",
      "Loss: 1.2131319046020508\n",
      "Loss: 1.383064866065979\n",
      "Loss: 1.0269824266433716\n",
      "Loss: 1.267236351966858\n",
      "Loss: 1.2051447629928589\n",
      "Loss: 1.1581027507781982\n",
      "Loss: 1.2143549919128418\n",
      "Loss: 1.4053155183792114\n",
      "Loss: 1.2736320495605469\n",
      "Loss: 1.4560455083847046\n",
      "Loss: 1.3637226819992065\n",
      "Loss: 1.280386209487915\n",
      "Loss: 0.9875839352607727\n",
      "Loss: 1.3149687051773071\n",
      "Loss: 1.1509945392608643\n",
      "Loss: 1.4508002996444702\n",
      "Loss: 1.1669307947158813\n",
      "Loss: 1.1835060119628906\n",
      "Loss: 1.4548437595367432\n",
      "Loss: 1.2214583158493042\n",
      "Loss: 1.1926418542861938\n",
      "Loss: 1.3775122165679932\n",
      "Loss: 1.3205671310424805\n",
      "Loss: 1.2841286659240723\n",
      "Loss: 1.219007968902588\n",
      "Loss: 1.1335276365280151\n",
      "Loss: 1.2119208574295044\n",
      "Loss: 1.2257529497146606\n",
      "Loss: 1.2654953002929688\n",
      "Loss: 1.1596870422363281\n",
      "Loss: 0.9888041615486145\n",
      "Loss: 1.4068117141723633\n",
      "Loss: 1.2808972597122192\n",
      "Loss: 1.1286488771438599\n",
      "Loss: 1.3028886318206787\n",
      "Loss: 1.3703080415725708\n",
      "Loss: 1.261032223701477\n",
      "Loss: 1.4445573091506958\n",
      "Loss: 1.1982226371765137\n",
      "Loss: 1.2251136302947998\n",
      "Loss: 1.4952645301818848\n",
      "Loss: 0.9675335884094238\n",
      "Loss: 1.4960349798202515\n",
      "Loss: 1.3032640218734741\n",
      "Loss: 1.3615292310714722\n",
      "Loss: 1.1314189434051514\n",
      "Loss: 1.0800020694732666\n",
      "Loss: 1.1606308221817017\n",
      "Loss: 1.2875251770019531\n",
      "Loss: 0.9796814918518066\n",
      "Loss: 1.0468345880508423\n",
      "Loss: 1.3220371007919312\n",
      "Loss: 1.2171556949615479\n",
      "Loss: 1.4108604192733765\n",
      "Loss: 1.0667273998260498\n",
      "Loss: 1.2724387645721436\n",
      "Loss: 0.9733000993728638\n",
      "Loss: 1.1657841205596924\n",
      "Loss: 1.2085964679718018\n",
      "Loss: 1.4145327806472778\n",
      "Loss: 1.0316940546035767\n",
      "Loss: 1.2020612955093384\n",
      "Loss: 1.4044190645217896\n",
      "Loss: 1.2166486978530884\n",
      "Loss: 1.3140478134155273\n",
      "Loss: 1.1699028015136719\n",
      "Loss: 1.0640208721160889\n",
      "Loss: 1.1109665632247925\n",
      "Loss: 1.282559871673584\n",
      "Loss: 1.13416588306427\n",
      "Loss: 1.378733515739441\n",
      "Loss: 1.3009631633758545\n",
      "Loss: 1.1336040496826172\n",
      "Loss: 1.117401361465454\n",
      "Loss: 0.9894036054611206\n",
      "Loss: 1.2349382638931274\n",
      "Loss: 1.293837547302246\n",
      "Loss: 1.0066263675689697\n",
      "Loss: 1.144315481185913\n",
      "Loss: 1.2892123460769653\n",
      "Loss: 1.3642756938934326\n",
      "Loss: 1.1503819227218628\n",
      "Loss: 1.238848090171814\n",
      "Loss: 1.2947592735290527\n",
      "Loss: 1.2793198823928833\n",
      "Loss: 1.3867994546890259\n",
      "Loss: 1.1846293210983276\n",
      "Loss: 1.3020211458206177\n",
      "Loss: 1.2613575458526611\n",
      "Loss: 1.4629641771316528\n",
      "Loss: 1.1246036291122437\n",
      "Loss: 1.4205130338668823\n",
      "Loss: 1.4473727941513062\n",
      "Loss: 1.1376562118530273\n",
      "Loss: 1.229251503944397\n",
      "Loss: 1.0496493577957153\n",
      "Loss: 1.1700372695922852\n",
      "Loss: 1.2896748781204224\n",
      "Loss: 1.1218352317810059\n",
      "Loss: 1.2222869396209717\n",
      "Loss: 1.0251193046569824\n",
      "Loss: 1.4456483125686646\n",
      "Loss: 1.1176317930221558\n",
      "Loss: 1.35329270362854\n",
      "Loss: 1.3475489616394043\n",
      "Loss: 1.1959071159362793\n",
      "Loss: 1.2581703662872314\n",
      "Loss: 1.0993905067443848\n",
      "Loss: 0.9449941515922546\n",
      "Loss: 1.2014377117156982\n",
      "Loss: 1.462572455406189\n",
      "Loss: 1.3420929908752441\n",
      "Loss: 1.0339393615722656\n",
      "Loss: 1.534349799156189\n",
      "Loss: 1.1335688829421997\n",
      "Loss: 1.3005404472351074\n",
      "Loss: 1.113824486732483\n",
      "Loss: 1.1736772060394287\n",
      "Loss: 1.0841693878173828\n",
      "Loss: 1.008832573890686\n",
      "Loss: 1.2442015409469604\n",
      "Loss: 1.378468632698059\n",
      "Loss: 1.2424261569976807\n",
      "Loss: 1.1010956764221191\n",
      "Loss: 1.2528785467147827\n",
      "Loss: 1.4006870985031128\n",
      "Loss: 1.3451794385910034\n",
      "Loss: 1.1506346464157104\n",
      "Loss: 1.104378581047058\n",
      "Loss: 1.1331069469451904\n",
      "Loss: 1.5056166648864746\n",
      "Loss: 1.4190624952316284\n",
      "Loss: 1.3183910846710205\n",
      "Loss: 1.2393198013305664\n",
      "Loss: 1.3059238195419312\n",
      "Loss: 1.1074869632720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2563494443893433\n",
      "Loss: 1.403342843055725\n",
      "Loss: 1.1448407173156738\n",
      "Loss: 1.2700246572494507\n",
      "Loss: 1.197383999824524\n",
      "Loss: 1.1524964570999146\n",
      "Loss: 1.1833837032318115\n",
      "Loss: 1.2829405069351196\n",
      "Loss: 1.2575676441192627\n",
      "Loss: 1.3691112995147705\n",
      "Loss: 1.1953160762786865\n",
      "Loss: 0.9843840003013611\n",
      "Loss: 1.049376130104065\n",
      "Loss: 1.2238192558288574\n",
      "Loss: 1.0818769931793213\n",
      "Loss: 1.221265435218811\n",
      "Loss: 1.296981692314148\n",
      "Loss: 1.2105437517166138\n",
      "Loss: 1.3517882823944092\n",
      "Loss: 1.4353647232055664\n",
      "Loss: 1.157196044921875\n",
      "Loss: 1.3252780437469482\n",
      "Loss: 1.3122432231903076\n",
      "Loss: 1.1853066682815552\n",
      "Loss: 1.2519562244415283\n",
      "Loss: 1.3399854898452759\n",
      "Loss: 1.3301687240600586\n",
      "Loss: 1.2446739673614502\n",
      "Loss: 0.9673069715499878\n",
      "Loss: 1.179593563079834\n",
      "Loss: 1.198248028755188\n",
      "Loss: 1.2492250204086304\n",
      "Loss: 1.1838258504867554\n",
      "Loss: 1.1154186725616455\n",
      "Loss: 1.2435503005981445\n",
      "Loss: 1.1733181476593018\n",
      "Loss: 1.2816259860992432\n",
      "Loss: 1.4226664304733276\n",
      "Loss: 1.1587389707565308\n",
      "Loss: 1.1353973150253296\n",
      "Loss: 1.1618744134902954\n",
      "Loss: 1.3599145412445068\n",
      "Loss: 1.1938884258270264\n",
      "Loss: 1.4208449125289917\n",
      "Loss: 1.2440860271453857\n",
      "Loss: 1.2013260126113892\n",
      "Loss: 1.167707085609436\n",
      "Loss: 1.190779685974121\n",
      "Loss: 1.1260404586791992\n",
      "Loss: 1.0278937816619873\n",
      "Loss: 1.303391456604004\n",
      "Loss: 1.3113423585891724\n",
      "Loss: 1.2229841947555542\n",
      "Loss: 1.0783264636993408\n",
      "Loss: 1.2692532539367676\n",
      "Loss: 1.2664270401000977\n",
      "Loss: 1.1705702543258667\n",
      "Loss: 1.1029691696166992\n",
      "Loss: 1.2340068817138672\n",
      "Loss: 1.0087275505065918\n",
      "Loss: 1.2008581161499023\n",
      "Loss: 1.1323069334030151\n",
      "Loss: 1.0741431713104248\n",
      "Loss: 1.0341225862503052\n",
      "Loss: 1.0548293590545654\n",
      "Loss: 1.1281077861785889\n",
      "Loss: 1.0293493270874023\n",
      "Loss: 1.1280714273452759\n",
      "Loss: 1.2192203998565674\n",
      "Loss: 1.3690423965454102\n",
      "Loss: 1.1918632984161377\n",
      "Loss: 1.2687883377075195\n",
      "Loss: 1.3266428709030151\n",
      "Loss: 1.1690545082092285\n",
      "Loss: 1.0636903047561646\n",
      "Loss: 1.2885087728500366\n",
      "Loss: 1.456192970275879\n",
      "Loss: 1.339510440826416\n",
      "Loss: 1.3222934007644653\n",
      "Loss: 1.2157245874404907\n",
      "Loss: 1.2957513332366943\n",
      "Loss: 1.1628409624099731\n",
      "Loss: 0.9602407217025757\n",
      "Loss: 1.3418344259262085\n",
      "Loss: 1.2332618236541748\n",
      "Loss: 1.0941405296325684\n",
      "Loss: 1.273746132850647\n",
      "Loss: 1.234391450881958\n",
      "Loss: 1.2861526012420654\n",
      "Loss: 1.3526252508163452\n",
      "Loss: 1.0530059337615967\n",
      "Loss: 1.195702075958252\n",
      "Loss: 1.2838160991668701\n",
      "Loss: 1.321865200996399\n",
      "Loss: 1.2850500345230103\n",
      "Loss: 1.1863939762115479\n",
      "Loss: 1.2922964096069336\n",
      "Loss: 0.9848041534423828\n",
      "Loss: 1.1725280284881592\n",
      "Loss: 1.1318854093551636\n",
      "Loss: 1.270263433456421\n",
      "Loss: 1.2737932205200195\n",
      "Loss: 1.2582436800003052\n",
      "Loss: 1.2190213203430176\n",
      "Loss: 1.478855848312378\n",
      "Loss: 1.263303518295288\n",
      "Loss: 1.0482492446899414\n",
      "Loss: 1.1727783679962158\n",
      "Loss: 1.2216111421585083\n",
      "Loss: 0.8183339834213257\n",
      "Loss: 1.4877914190292358\n",
      "Loss: 1.1857587099075317\n",
      "Loss: 0.9796085357666016\n",
      "Loss: 1.1918102502822876\n",
      "Loss: 1.1566601991653442\n",
      "Loss: 1.1214625835418701\n",
      "Loss: 1.2897753715515137\n",
      "Loss: 1.393506646156311\n",
      "Loss: 1.1876639127731323\n",
      "Loss: 1.3913214206695557\n",
      "Loss: 1.1665016412734985\n",
      "Loss: 1.1178789138793945\n",
      "Loss: 1.231469988822937\n",
      "Loss: 1.1592686176300049\n",
      "Loss: 1.1318601369857788\n",
      "Loss: 1.157965898513794\n",
      "Loss: 1.1896733045578003\n",
      "Loss: 1.0742554664611816\n",
      "Loss: 1.121546983718872\n",
      "Loss: 1.0540724992752075\n",
      "Loss: 1.296531081199646\n",
      "Loss: 1.1609470844268799\n",
      "Loss: 1.66414213180542\n",
      "Loss: 1.3626017570495605\n",
      "Loss: 1.2246931791305542\n",
      "Loss: 1.3278210163116455\n",
      "Loss: 0.998862087726593\n",
      "Loss: 1.0043548345565796\n",
      "Loss: 1.2588485479354858\n",
      "Loss: 1.3776724338531494\n",
      "Loss: 1.0444905757904053\n",
      "Loss: 1.3926353454589844\n",
      "Loss: 1.283640742301941\n",
      "Loss: 1.3935601711273193\n",
      "Loss: 1.2700749635696411\n",
      "Loss: 1.4010580778121948\n",
      "Loss: 1.2938357591629028\n",
      "Loss: 1.2393853664398193\n",
      "Loss: 1.1966496706008911\n",
      "Loss: 1.2760347127914429\n",
      "Loss: 1.4307186603546143\n",
      "Loss: 1.2587332725524902\n",
      "Loss: 1.5049831867218018\n",
      "Loss: 1.1174768209457397\n",
      "Loss: 1.3139582872390747\n",
      "Loss: 1.2776484489440918\n",
      "Loss: 1.1688232421875\n",
      "Loss: 1.2090564966201782\n",
      "Loss: 1.3832415342330933\n",
      "Loss: 1.4675743579864502\n",
      "Loss: 1.2586028575897217\n",
      "Loss: 1.2986814975738525\n",
      "Loss: 1.0805147886276245\n",
      "Loss: 1.240971326828003\n",
      "Loss: 1.2335131168365479\n",
      "Loss: 1.1076582670211792\n",
      "Loss: 1.2298142910003662\n",
      "Loss: 1.3393030166625977\n",
      "Loss: 1.3650034666061401\n",
      "Loss: 1.1393321752548218\n",
      "Loss: 1.3616055250167847\n",
      "Loss: 1.1057307720184326\n",
      "Loss: 1.307349443435669\n",
      "Loss: 1.2807817459106445\n",
      "Loss: 1.2801460027694702\n",
      "Loss: 1.2532916069030762\n",
      "Loss: 1.2900947332382202\n",
      "Loss: 1.0855380296707153\n",
      "Loss: 1.379518747329712\n",
      "Loss: 1.1467593908309937\n",
      "Loss: 0.9852449893951416\n",
      "Loss: 1.4831522703170776\n",
      "Loss: 1.1293200254440308\n",
      "Loss: 1.4231446981430054\n",
      "Loss: 1.0936334133148193\n",
      "Loss: 1.4027410745620728\n",
      "Loss: 1.1791912317276\n",
      "Loss: 1.1081740856170654\n",
      "Loss: 1.1920348405838013\n",
      "Loss: 1.1149975061416626\n",
      "Loss: 1.2038017511367798\n",
      "Loss: 1.0106333494186401\n",
      "Loss: 1.4339001178741455\n",
      "Loss: 1.1490801572799683\n",
      "Loss: 1.4813343286514282\n",
      "Loss: 1.8436601161956787\n",
      "Loss: 0.992838442325592\n",
      "Loss: 1.2004950046539307\n",
      "Loss: 1.2700743675231934\n",
      "Loss: 1.0762827396392822\n",
      "Loss: 1.3768911361694336\n",
      "Loss: 1.4488389492034912\n",
      "Loss: 1.3022938966751099\n",
      "Loss: 1.3164743185043335\n",
      "Loss: 1.0845494270324707\n",
      "Loss: 1.231898307800293\n",
      "Loss: 1.4370489120483398\n",
      "Loss: 1.406612515449524\n",
      "Loss: 1.1785396337509155\n",
      "Loss: 1.197891354560852\n",
      "Loss: 1.1143451929092407\n",
      "Loss: 1.0594983100891113\n",
      "Loss: 1.1966118812561035\n",
      "Loss: 1.439035177230835\n",
      "Loss: 0.9877829551696777\n",
      "Loss: 1.245133638381958\n",
      "Loss: 1.376039981842041\n",
      "Loss: 1.07768976688385\n",
      "Loss: 1.2206538915634155\n",
      "Loss: 1.2591651678085327\n",
      "Loss: 1.369421124458313\n",
      "Loss: 1.1184288263320923\n",
      "Loss: 1.0475637912750244\n",
      "Loss: 0.930880069732666\n",
      "Loss: 1.2883648872375488\n",
      "Loss: 1.217452883720398\n",
      "Loss: 1.0205053091049194\n",
      "Loss: 1.0486727952957153\n",
      "Loss: 1.3300988674163818\n",
      "Loss: 1.0711673498153687\n",
      "Loss: 1.2951669692993164\n",
      "Loss: 1.0698494911193848\n",
      "Loss: 1.7003238201141357\n",
      "Loss: 1.296053409576416\n",
      "Loss: 1.2219146490097046\n",
      "Loss: 1.202470302581787\n",
      "Loss: 1.3263145685195923\n",
      "Loss: 1.121506929397583\n",
      "Loss: 1.2151283025741577\n",
      "Loss: 1.3277678489685059\n",
      "Loss: 1.4927781820297241\n",
      "Loss: 1.1611453294754028\n",
      "Loss: 1.3179692029953003\n",
      "Loss: 1.4605231285095215\n",
      "Loss: 1.1721575260162354\n",
      "Loss: 1.158571481704712\n",
      "Loss: 1.1376765966415405\n",
      "Loss: 1.0068988800048828\n",
      "Loss: 1.4054572582244873\n",
      "Loss: 1.2447326183319092\n",
      "Loss: 1.4498543739318848\n",
      "Loss: 1.3311924934387207\n",
      "Loss: 1.0564143657684326\n",
      "Loss: 1.1576443910598755\n",
      "Loss: 1.2988070249557495\n",
      "Loss: 1.1543501615524292\n",
      "Loss: 1.1974117755889893\n",
      "Loss: 1.5257419347763062\n",
      "Loss: 1.471794605255127\n",
      "Loss: 1.1776920557022095\n",
      "Loss: 1.3780800104141235\n",
      "Loss: 1.1459885835647583\n",
      "Loss: 1.1982214450836182\n",
      "Loss: 1.676869511604309\n",
      "Loss: 1.2468976974487305\n",
      "Loss: 1.1324021816253662\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 3.4085\n",
      "a: -0.1000\n",
      "step:  1\n",
      "loss: 2.5221\n",
      "a: -0.0968\n",
      "step:  2\n",
      "loss: 2.5012\n",
      "a: -0.0941\n",
      "step:  3\n",
      "loss: 2.6047\n",
      "a: -0.0919\n",
      "step:  4\n",
      "loss: 2.3506\n",
      "a: -0.0891\n",
      "step:  5\n",
      "loss: 2.4400\n",
      "a: -0.0866\n",
      "step:  6\n",
      "loss: 2.3091\n",
      "a: -0.0840\n",
      "step:  7\n",
      "loss: 2.3205\n",
      "a: -0.0816\n",
      "step:  8\n",
      "loss: 2.3555\n",
      "a: -0.0792\n",
      "step:  9\n",
      "loss: 2.3413\n",
      "a: -0.0770\n",
      "step:  10\n",
      "loss: 2.3310\n",
      "a: -0.0750\n",
      "step:  11\n",
      "loss: 2.4262\n",
      "a: -0.0729\n",
      "step:  12\n",
      "loss: 2.2930\n",
      "a: -0.0709\n",
      "step:  13\n",
      "loss: 2.3100\n",
      "a: -0.0690\n",
      "step:  14\n",
      "loss: 2.2143\n",
      "a: -0.0672\n",
      "step:  15\n",
      "loss: 2.2310\n",
      "a: -0.0655\n",
      "step:  16\n",
      "loss: 2.2795\n",
      "a: -0.0639\n",
      "step:  17\n",
      "loss: 2.3911\n",
      "a: -0.0623\n",
      "step:  18\n",
      "loss: 2.2091\n",
      "a: -0.0607\n",
      "step:  19\n",
      "loss: 2.1138\n",
      "a: -0.0591\n",
      "step:  20\n",
      "loss: 2.1172\n",
      "a: -0.0576\n",
      "step:  21\n",
      "loss: 2.2578\n",
      "a: -0.0561\n",
      "step:  22\n",
      "loss: 2.2086\n",
      "a: -0.0546\n",
      "step:  23\n",
      "loss: 2.1716\n",
      "a: -0.0532\n",
      "step:  24\n",
      "loss: 2.0940\n",
      "a: -0.0518\n",
      "step:  25\n",
      "loss: 2.1407\n",
      "a: -0.0503\n",
      "step:  26\n",
      "loss: 2.1106\n",
      "a: -0.0489\n",
      "step:  27\n",
      "loss: 2.0607\n",
      "a: -0.0474\n",
      "step:  28\n",
      "loss: 2.1161\n",
      "a: -0.0460\n",
      "step:  29\n",
      "loss: 1.9676\n",
      "a: -0.0447\n",
      "step:  30\n",
      "loss: 2.1163\n",
      "a: -0.0434\n",
      "step:  31\n",
      "loss: 1.9599\n",
      "a: -0.0420\n",
      "step:  32\n",
      "loss: 2.1428\n",
      "a: -0.0407\n",
      "step:  33\n",
      "loss: 2.0046\n",
      "a: -0.0394\n",
      "step:  34\n",
      "loss: 1.9258\n",
      "a: -0.0381\n",
      "step:  35\n",
      "loss: 1.9724\n",
      "a: -0.0368\n",
      "step:  36\n",
      "loss: 1.8967\n",
      "a: -0.0355\n",
      "step:  37\n",
      "loss: 1.9229\n",
      "a: -0.0342\n",
      "step:  38\n",
      "loss: 1.8829\n",
      "a: -0.0329\n",
      "step:  39\n",
      "loss: 1.8738\n",
      "a: -0.0316\n",
      "step:  40\n",
      "loss: 1.9295\n",
      "a: -0.0304\n",
      "step:  41\n",
      "loss: 1.7957\n",
      "a: -0.0292\n",
      "step:  42\n",
      "loss: 1.8951\n",
      "a: -0.0280\n",
      "step:  43\n",
      "loss: 1.7749\n",
      "a: -0.0268\n",
      "step:  44\n",
      "loss: 1.9031\n",
      "a: -0.0256\n",
      "step:  45\n",
      "loss: 1.7559\n",
      "a: -0.0244\n",
      "step:  46\n",
      "loss: 1.8441\n",
      "a: -0.0233\n",
      "step:  47\n",
      "loss: 1.7691\n",
      "a: -0.0222\n",
      "step:  48\n",
      "loss: 1.6500\n",
      "a: -0.0210\n",
      "step:  49\n",
      "loss: 1.8786\n",
      "a: -0.0198\n",
      "step:  50\n",
      "loss: 1.6281\n",
      "a: -0.0186\n",
      "step:  51\n",
      "loss: 1.9056\n",
      "a: -0.0174\n",
      "step:  52\n",
      "loss: 1.9222\n",
      "a: -0.0162\n",
      "step:  53\n",
      "loss: 1.5522\n",
      "a: -0.0150\n",
      "step:  54\n",
      "loss: 1.7388\n",
      "a: -0.0138\n",
      "step:  55\n",
      "loss: 1.7017\n",
      "a: -0.0127\n",
      "step:  56\n",
      "loss: 1.8135\n",
      "a: -0.0116\n",
      "step:  57\n",
      "loss: 1.7568\n",
      "a: -0.0105\n",
      "step:  58\n",
      "loss: 1.5650\n",
      "a: -0.0095\n",
      "step:  59\n",
      "loss: 1.6894\n",
      "a: -0.0084\n",
      "step:  60\n",
      "loss: 1.7822\n",
      "a: -0.0074\n",
      "step:  61\n",
      "loss: 1.8146\n",
      "a: -0.0064\n",
      "step:  62\n",
      "loss: 1.7770\n",
      "a: -0.0055\n",
      "step:  63\n",
      "loss: 1.7493\n",
      "a: -0.0045\n",
      "step:  64\n",
      "loss: 1.6809\n",
      "a: -0.0035\n",
      "step:  65\n",
      "loss: 1.5123\n",
      "a: -0.0026\n",
      "step:  66\n",
      "loss: 1.4301\n",
      "a: -0.0017\n",
      "step:  67\n",
      "loss: 1.6585\n",
      "a: -0.0008\n",
      "step:  68\n",
      "loss: 1.6485\n",
      "a: -0.0001\n",
      "step:  69\n",
      "loss: 1.8119\n",
      "a: 0.0007\n",
      "step:  70\n",
      "loss: 1.3545\n",
      "a: 0.0014\n",
      "step:  71\n",
      "loss: 1.7212\n",
      "a: 0.0021\n",
      "step:  72\n",
      "loss: 1.7668\n",
      "a: 0.0025\n",
      "step:  73\n",
      "loss: 1.5602\n",
      "a: 0.0028\n",
      "step:  74\n",
      "loss: 1.8980\n",
      "a: 0.0030\n",
      "step:  75\n",
      "loss: 1.6417\n",
      "a: 0.0031\n",
      "step:  76\n",
      "loss: 1.6261\n",
      "a: 0.0029\n",
      "step:  77\n",
      "loss: 1.8146\n",
      "a: 0.0026\n",
      "step:  78\n",
      "loss: 1.9130\n",
      "a: 0.0020\n",
      "step:  79\n",
      "loss: 1.6634\n",
      "a: 0.0011\n",
      "step:  80\n",
      "loss: 1.7140\n",
      "a: 0.0001\n",
      "step:  81\n",
      "loss: 1.6001\n",
      "a: -0.0009\n",
      "step:  82\n",
      "loss: 1.5459\n",
      "a: -0.0019\n",
      "step:  83\n",
      "loss: 1.7669\n",
      "a: -0.0030\n",
      "step:  84\n",
      "loss: 1.6140\n",
      "a: -0.0040\n",
      "step:  85\n",
      "loss: 1.5271\n",
      "a: -0.0051\n",
      "step:  86\n",
      "loss: 1.4989\n",
      "a: -0.0062\n",
      "step:  87\n",
      "loss: 1.5702\n",
      "a: -0.0072\n",
      "step:  88\n",
      "loss: 1.6632\n",
      "a: -0.0082\n",
      "step:  89\n",
      "loss: 1.7257\n",
      "a: -0.0090\n",
      "step:  90\n",
      "loss: 1.8331\n",
      "a: -0.0096\n",
      "step:  91\n",
      "loss: 1.7898\n",
      "a: -0.0101\n",
      "step:  92\n",
      "loss: 1.5673\n",
      "a: -0.0104\n",
      "step:  93\n",
      "loss: 1.6740\n",
      "a: -0.0103\n",
      "step:  94\n",
      "loss: 1.6874\n",
      "a: -0.0100\n",
      "step:  95\n",
      "loss: 1.5068\n",
      "a: -0.0093\n",
      "step:  96\n",
      "loss: 1.7248\n",
      "a: -0.0085\n",
      "step:  97\n",
      "loss: 1.6170\n",
      "a: -0.0075\n",
      "step:  98\n",
      "loss: 1.5882\n",
      "a: -0.0063\n",
      "step:  99\n",
      "loss: 1.5802\n",
      "a: -0.0052\n",
      "step:  100\n",
      "loss: 1.6569\n",
      "a: -0.0041\n",
      "step:  101\n",
      "loss: 1.7392\n",
      "a: -0.0029\n",
      "step:  102\n",
      "loss: 1.5889\n",
      "a: -0.0020\n",
      "step:  103\n",
      "loss: 1.6225\n",
      "a: -0.0010\n",
      "step:  104\n",
      "loss: 1.5066\n",
      "a: -0.0003\n",
      "step:  105\n",
      "loss: 1.8229\n",
      "a: 0.0004\n",
      "step:  106\n",
      "loss: 1.6268\n",
      "a: 0.0010\n",
      "step:  107\n",
      "loss: 1.4473\n",
      "a: 0.0012\n",
      "step:  108\n",
      "loss: 1.5546\n",
      "a: 0.0013\n",
      "step:  109\n",
      "loss: 1.5561\n",
      "a: 0.0010\n",
      "step:  110\n",
      "loss: 1.5924\n",
      "a: 0.0002\n",
      "step:  111\n",
      "loss: 1.2335\n",
      "a: -0.0006\n",
      "step:  112\n",
      "loss: 1.6826\n",
      "a: -0.0015\n",
      "step:  113\n",
      "loss: 1.6981\n",
      "a: -0.0025\n",
      "step:  114\n",
      "loss: 1.3215\n",
      "a: -0.0034\n",
      "step:  115\n",
      "loss: 1.4294\n",
      "a: -0.0038\n",
      "step:  116\n",
      "loss: 1.6396\n",
      "a: -0.0041\n",
      "step:  117\n",
      "loss: 1.7817\n",
      "a: -0.0046\n",
      "step:  118\n",
      "loss: 1.7511\n",
      "a: -0.0051\n",
      "step:  119\n",
      "loss: 1.7832\n",
      "a: -0.0050\n",
      "step:  120\n",
      "loss: 1.6281\n",
      "a: -0.0046\n",
      "step:  121\n",
      "loss: 1.5801\n",
      "a: -0.0036\n",
      "step:  122\n",
      "loss: 1.4008\n",
      "a: -0.0029\n",
      "step:  123\n",
      "loss: 1.4286\n",
      "a: -0.0028\n",
      "step:  124\n",
      "loss: 1.6410\n",
      "a: -0.0028\n",
      "step:  125\n",
      "loss: 1.3077\n",
      "a: -0.0030\n",
      "step:  126\n",
      "loss: 1.6435\n",
      "a: -0.0031\n",
      "step:  127\n",
      "loss: 1.4600\n",
      "a: -0.0035\n",
      "step:  128\n",
      "loss: 1.7056\n",
      "a: -0.0044\n",
      "step:  129\n",
      "loss: 1.7571\n",
      "a: -0.0057\n",
      "step:  130\n",
      "loss: 1.4880\n",
      "a: -0.0061\n",
      "step:  131\n",
      "loss: 1.5708\n",
      "a: -0.0063\n",
      "step:  132\n",
      "loss: 1.5376\n",
      "a: -0.0054\n",
      "step:  133\n",
      "loss: 1.4449\n",
      "a: -0.0041\n",
      "step:  134\n",
      "loss: 1.6267\n",
      "a: -0.0026\n",
      "step:  135\n",
      "loss: 1.4549\n",
      "a: -0.0010\n",
      "step:  136\n",
      "loss: 1.5073\n",
      "a: 0.0003\n",
      "step:  137\n",
      "loss: 1.7053\n",
      "a: 0.0006\n",
      "step:  138\n",
      "loss: 1.4918\n",
      "a: 0.0003\n",
      "step:  139\n",
      "loss: 1.6660\n",
      "a: 0.0000\n",
      "step:  140\n",
      "loss: 1.4423\n",
      "a: 0.0002\n",
      "step:  141\n",
      "loss: 1.5049\n",
      "a: 0.0009\n",
      "step:  142\n",
      "loss: 1.5169\n",
      "a: 0.0013\n",
      "step:  143\n",
      "loss: 1.5914\n",
      "a: 0.0005\n",
      "step:  144\n",
      "loss: 1.5522\n",
      "a: -0.0002\n",
      "step:  145\n",
      "loss: 1.5344\n",
      "a: -0.0012\n",
      "step:  146\n",
      "loss: 1.4068\n",
      "a: -0.0027\n",
      "step:  147\n",
      "loss: 1.6969\n",
      "a: -0.0042\n",
      "step:  148\n",
      "loss: 1.6349\n",
      "a: -0.0054\n",
      "step:  149\n",
      "loss: 1.5064\n",
      "a: -0.0052\n",
      "step:  150\n",
      "loss: 1.4180\n",
      "a: -0.0037\n",
      "step:  151\n",
      "loss: 1.5857\n",
      "a: -0.0020\n",
      "step:  152\n",
      "loss: 1.4785\n",
      "a: -0.0012\n",
      "step:  153\n",
      "loss: 1.4036\n",
      "a: -0.0012\n",
      "step:  154\n",
      "loss: 1.4994\n",
      "a: -0.0025\n",
      "step:  155\n",
      "loss: 1.4950\n",
      "a: -0.0034\n",
      "step:  156\n",
      "loss: 1.4671\n",
      "a: -0.0043\n",
      "step:  157\n",
      "loss: 1.3805\n",
      "a: -0.0048\n",
      "step:  158\n",
      "loss: 1.6333\n",
      "a: -0.0048\n",
      "step:  159\n",
      "loss: 1.6569\n",
      "a: -0.0030\n",
      "step:  160\n",
      "loss: 1.4015\n",
      "a: -0.0014\n",
      "step:  161\n",
      "loss: 1.4531\n",
      "a: -0.0007\n",
      "step:  162\n",
      "loss: 1.5482\n",
      "a: -0.0009\n",
      "step:  163\n",
      "loss: 1.5637\n",
      "a: -0.0024\n",
      "step:  164\n",
      "loss: 1.5310\n",
      "a: -0.0034\n",
      "step:  165\n",
      "loss: 1.5554\n",
      "a: -0.0045\n",
      "step:  166\n",
      "loss: 1.5648\n",
      "a: -0.0047\n",
      "step:  167\n",
      "loss: 1.4782\n",
      "a: -0.0043\n",
      "step:  168\n",
      "loss: 1.4542\n",
      "a: -0.0036\n",
      "step:  169\n",
      "loss: 1.5651\n",
      "a: -0.0028\n",
      "step:  170\n",
      "loss: 1.6215\n",
      "a: -0.0008\n",
      "step:  171\n",
      "loss: 1.9141\n",
      "a: 0.0011\n",
      "step:  172\n",
      "loss: 1.5872\n",
      "a: -0.0013\n",
      "step:  173\n",
      "loss: 1.6786\n",
      "a: -0.0034\n",
      "step:  174\n",
      "loss: 1.4823\n",
      "a: -0.0051\n",
      "step:  175\n",
      "loss: 1.4915\n",
      "a: -0.0064\n",
      "step:  176\n",
      "loss: 1.7009\n",
      "a: -0.0071\n",
      "step:  177\n",
      "loss: 1.7038\n",
      "a: -0.0071\n",
      "step:  178\n",
      "loss: 1.5468\n",
      "a: -0.0060\n",
      "step:  179\n",
      "loss: 1.4559\n",
      "a: -0.0043\n",
      "step:  180\n",
      "loss: 1.4176\n",
      "a: -0.0026\n",
      "step:  181\n",
      "loss: 1.8261\n",
      "a: -0.0012\n",
      "step:  182\n",
      "loss: 1.7024\n",
      "a: -0.0000\n",
      "step:  183\n",
      "loss: 1.4918\n",
      "a: 0.0004\n",
      "step:  184\n",
      "loss: 1.4905\n",
      "a: 0.0003\n",
      "step:  185\n",
      "loss: 1.3890\n",
      "a: -0.0000\n",
      "step:  186\n",
      "loss: 1.4765\n",
      "a: -0.0009\n",
      "step:  187\n",
      "loss: 1.5421\n",
      "a: -0.0017\n",
      "step:  188\n",
      "loss: 1.2771\n",
      "a: -0.0026\n",
      "step:  189\n",
      "loss: 1.4485\n",
      "a: -0.0035\n",
      "step:  190\n",
      "loss: 1.4907\n",
      "a: -0.0041\n",
      "step:  191\n",
      "loss: 1.1742\n",
      "a: -0.0045\n",
      "step:  192\n",
      "loss: 1.6574\n",
      "a: -0.0045\n",
      "step:  193\n",
      "loss: 1.2833\n",
      "a: -0.0037\n",
      "step:  194\n",
      "loss: 1.5668\n",
      "a: -0.0030\n",
      "step:  195\n",
      "loss: 1.5464\n",
      "a: -0.0023\n",
      "step:  196\n",
      "loss: 1.5504\n",
      "a: -0.0014\n",
      "step:  197\n",
      "loss: 1.5030\n",
      "a: -0.0005\n",
      "step:  198\n",
      "loss: 1.6402\n",
      "a: -0.0003\n",
      "step:  199\n",
      "loss: 1.4054\n",
      "a: -0.0010\n",
      "step:  200\n",
      "loss: 1.3490\n",
      "a: -0.0025\n",
      "step:  201\n",
      "loss: 1.5555\n",
      "a: -0.0039\n",
      "step:  202\n",
      "loss: 1.2723\n",
      "a: -0.0054\n",
      "step:  203\n",
      "loss: 1.2722\n",
      "a: -0.0063\n",
      "step:  204\n",
      "loss: 1.5342\n",
      "a: -0.0073\n",
      "step:  205\n",
      "loss: 1.4231\n",
      "a: -0.0076\n",
      "step:  206\n",
      "loss: 1.4120\n",
      "a: -0.0072\n",
      "step:  207\n",
      "loss: 1.7237\n",
      "a: -0.0063\n",
      "step:  208\n",
      "loss: 1.7755\n",
      "a: -0.0051\n",
      "step:  209\n",
      "loss: 1.3786\n",
      "a: -0.0033\n",
      "step:  210\n",
      "loss: 1.5147\n",
      "a: -0.0017\n",
      "step:  211\n",
      "loss: 1.7378\n",
      "a: -0.0004\n",
      "step:  212\n",
      "loss: 1.5177\n",
      "a: 0.0004\n",
      "step:  213\n",
      "loss: 1.4615\n",
      "a: -0.0005\n",
      "step:  214\n",
      "loss: 1.4202\n",
      "a: -0.0017\n",
      "step:  215\n",
      "loss: 1.3972\n",
      "a: -0.0033\n",
      "step:  216\n",
      "loss: 1.3857\n",
      "a: -0.0048\n",
      "step:  217\n",
      "loss: 1.6005\n",
      "a: -0.0062\n",
      "step:  218\n",
      "loss: 1.5054\n",
      "a: -0.0073\n",
      "step:  219\n",
      "loss: 1.8596\n",
      "a: -0.0082\n",
      "step:  220\n",
      "loss: 1.1927\n",
      "a: -0.0086\n",
      "step:  221\n",
      "loss: 1.6698\n",
      "a: -0.0087\n",
      "step:  222\n",
      "loss: 1.6810\n",
      "a: -0.0080\n",
      "step:  223\n",
      "loss: 1.3340\n",
      "a: -0.0071\n",
      "step:  224\n",
      "loss: 1.5324\n",
      "a: -0.0058\n",
      "step:  225\n",
      "loss: 1.3293\n",
      "a: -0.0046\n",
      "step:  226\n",
      "loss: 1.4565\n",
      "a: -0.0033\n",
      "step:  227\n",
      "loss: 1.5513\n",
      "a: -0.0021\n",
      "step:  228\n",
      "loss: 1.6032\n",
      "a: -0.0014\n",
      "step:  229\n",
      "loss: 1.5490\n",
      "a: -0.0013\n",
      "step:  230\n",
      "loss: 1.4824\n",
      "a: -0.0018\n",
      "step:  231\n",
      "loss: 1.5488\n",
      "a: -0.0027\n",
      "step:  232\n",
      "loss: 1.3808\n",
      "a: -0.0035\n",
      "step:  233\n",
      "loss: 1.5430\n",
      "a: -0.0044\n",
      "step:  234\n",
      "loss: 1.4282\n",
      "a: -0.0052\n",
      "step:  235\n",
      "loss: 1.6667\n",
      "a: -0.0056\n",
      "step:  236\n",
      "loss: 1.5187\n",
      "a: -0.0058\n",
      "step:  237\n",
      "loss: 1.3366\n",
      "a: -0.0056\n",
      "step:  238\n",
      "loss: 1.5825\n",
      "a: -0.0054\n",
      "step:  239\n",
      "loss: 1.5864\n",
      "a: -0.0046\n",
      "step:  240\n",
      "loss: 1.6622\n",
      "a: -0.0037\n",
      "step:  241\n",
      "loss: 1.4211\n",
      "a: -0.0033\n",
      "step:  242\n",
      "loss: 1.5797\n",
      "a: -0.0030\n",
      "step:  243\n",
      "loss: 1.6200\n",
      "a: -0.0025\n",
      "step:  244\n",
      "loss: 1.1813\n",
      "a: -0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  245\n",
      "loss: 1.5209\n",
      "a: -0.0017\n",
      "step:  246\n",
      "loss: 1.5846\n",
      "a: -0.0019\n",
      "step:  247\n",
      "loss: 1.3325\n",
      "a: -0.0028\n",
      "step:  248\n",
      "loss: 1.3627\n",
      "a: -0.0039\n",
      "step:  249\n",
      "loss: 1.6347\n",
      "a: -0.0049\n",
      "step:  250\n",
      "loss: 1.4698\n",
      "a: -0.0052\n",
      "step:  251\n",
      "loss: 1.5196\n",
      "a: -0.0057\n",
      "step:  252\n",
      "loss: 1.4727\n",
      "a: -0.0055\n",
      "step:  253\n",
      "loss: 1.4236\n",
      "a: -0.0046\n",
      "step:  254\n",
      "loss: 1.3833\n",
      "a: -0.0031\n",
      "step:  255\n",
      "loss: 1.2631\n",
      "a: -0.0018\n",
      "step:  256\n",
      "loss: 1.5503\n",
      "a: -0.0013\n",
      "step:  257\n",
      "loss: 1.7177\n",
      "a: -0.0021\n",
      "step:  258\n",
      "loss: 1.5474\n",
      "a: -0.0038\n",
      "step:  259\n",
      "loss: 1.6490\n",
      "a: -0.0051\n",
      "step:  260\n",
      "loss: 1.3808\n",
      "a: -0.0059\n",
      "step:  261\n",
      "loss: 1.5072\n",
      "a: -0.0061\n",
      "step:  262\n",
      "loss: 1.4202\n",
      "a: -0.0058\n",
      "step:  263\n",
      "loss: 1.4471\n",
      "a: -0.0049\n",
      "step:  264\n",
      "loss: 1.4630\n",
      "a: -0.0037\n",
      "step:  265\n",
      "loss: 1.2740\n",
      "a: -0.0028\n",
      "step:  266\n",
      "loss: 1.6468\n",
      "a: -0.0021\n",
      "step:  267\n",
      "loss: 1.3712\n",
      "a: -0.0020\n",
      "step:  268\n",
      "loss: 1.4291\n",
      "a: -0.0029\n",
      "step:  269\n",
      "loss: 1.7333\n",
      "a: -0.0041\n",
      "step:  270\n",
      "loss: 1.5332\n",
      "a: -0.0057\n",
      "step:  271\n",
      "loss: 1.3080\n",
      "a: -0.0071\n",
      "step:  272\n",
      "loss: 1.6563\n",
      "a: -0.0082\n",
      "step:  273\n",
      "loss: 1.3753\n",
      "a: -0.0089\n",
      "step:  274\n",
      "loss: 1.5225\n",
      "a: -0.0095\n",
      "step:  275\n",
      "loss: 1.7651\n",
      "a: -0.0097\n",
      "step:  276\n",
      "loss: 1.4318\n",
      "a: -0.0091\n",
      "step:  277\n",
      "loss: 1.5283\n",
      "a: -0.0082\n",
      "step:  278\n",
      "loss: 1.3391\n",
      "a: -0.0069\n",
      "step:  279\n",
      "loss: 1.4175\n",
      "a: -0.0056\n",
      "step:  280\n",
      "loss: 1.2237\n",
      "a: -0.0042\n",
      "step:  281\n",
      "loss: 1.4786\n",
      "a: -0.0030\n",
      "step:  282\n",
      "loss: 1.6658\n",
      "a: -0.0023\n",
      "step:  283\n",
      "loss: 1.6725\n",
      "a: -0.0022\n",
      "step:  284\n",
      "loss: 1.4878\n",
      "a: -0.0030\n",
      "step:  285\n",
      "loss: 1.3781\n",
      "a: -0.0043\n",
      "step:  286\n",
      "loss: 1.3871\n",
      "a: -0.0055\n",
      "step:  287\n",
      "loss: 1.6144\n",
      "a: -0.0066\n",
      "step:  288\n",
      "loss: 1.4349\n",
      "a: -0.0078\n",
      "step:  289\n",
      "loss: 1.6048\n",
      "a: -0.0087\n",
      "step:  290\n",
      "loss: 1.4849\n",
      "a: -0.0093\n",
      "step:  291\n",
      "loss: 1.3999\n",
      "a: -0.0093\n",
      "step:  292\n",
      "loss: 1.5296\n",
      "a: -0.0088\n",
      "step:  293\n",
      "loss: 1.5687\n",
      "a: -0.0079\n",
      "step:  294\n",
      "loss: 1.3354\n",
      "a: -0.0065\n",
      "step:  295\n",
      "loss: 1.3680\n",
      "a: -0.0051\n",
      "step:  296\n",
      "loss: 1.2096\n",
      "a: -0.0038\n",
      "step:  297\n",
      "loss: 1.4351\n",
      "a: -0.0026\n",
      "step:  298\n",
      "loss: 1.5171\n",
      "a: -0.0015\n",
      "step:  299\n",
      "loss: 1.5058\n",
      "a: -0.0008\n",
      "step:  300\n",
      "loss: 1.6863\n",
      "a: -0.0019\n",
      "step:  301\n",
      "loss: 1.4714\n",
      "a: -0.0039\n",
      "step:  302\n",
      "loss: 1.3872\n",
      "a: -0.0059\n",
      "step:  303\n",
      "loss: 1.4142\n",
      "a: -0.0077\n",
      "step:  304\n",
      "loss: 1.5576\n",
      "a: -0.0091\n",
      "step:  305\n",
      "loss: 1.4400\n",
      "a: -0.0102\n",
      "step:  306\n",
      "loss: 1.7764\n",
      "a: -0.0110\n",
      "step:  307\n",
      "loss: 1.7344\n",
      "a: -0.0115\n",
      "step:  308\n",
      "loss: 1.6417\n",
      "a: -0.0113\n",
      "step:  309\n",
      "loss: 1.5149\n",
      "a: -0.0106\n",
      "step:  310\n",
      "loss: 1.4317\n",
      "a: -0.0094\n",
      "step:  311\n",
      "loss: 1.4205\n",
      "a: -0.0081\n",
      "step:  312\n",
      "loss: 1.4711\n",
      "a: -0.0067\n",
      "step:  313\n",
      "loss: 1.3685\n",
      "a: -0.0054\n",
      "step:  314\n",
      "loss: 1.3144\n",
      "a: -0.0041\n",
      "step:  315\n",
      "loss: 1.4795\n",
      "a: -0.0029\n",
      "step:  316\n",
      "loss: 1.4846\n",
      "a: -0.0020\n",
      "step:  317\n",
      "loss: 1.3948\n",
      "a: -0.0013\n",
      "step:  318\n",
      "loss: 1.5317\n",
      "a: -0.0010\n",
      "step:  319\n",
      "loss: 1.4584\n",
      "a: -0.0018\n",
      "step:  320\n",
      "loss: 1.3177\n",
      "a: -0.0030\n",
      "step:  321\n",
      "loss: 1.5132\n",
      "a: -0.0043\n",
      "step:  322\n",
      "loss: 1.4429\n",
      "a: -0.0054\n",
      "step:  323\n",
      "loss: 1.3838\n",
      "a: -0.0065\n",
      "step:  324\n",
      "loss: 1.3738\n",
      "a: -0.0073\n",
      "step:  325\n",
      "loss: 1.3955\n",
      "a: -0.0079\n",
      "step:  326\n",
      "loss: 1.5592\n",
      "a: -0.0084\n",
      "step:  327\n",
      "loss: 1.6626\n",
      "a: -0.0087\n",
      "step:  328\n",
      "loss: 1.6361\n",
      "a: -0.0089\n",
      "step:  329\n",
      "loss: 1.3335\n",
      "a: -0.0086\n",
      "step:  330\n",
      "loss: 1.4324\n",
      "a: -0.0081\n",
      "step:  331\n",
      "loss: 1.3349\n",
      "a: -0.0073\n",
      "step:  332\n",
      "loss: 1.3686\n",
      "a: -0.0063\n",
      "step:  333\n",
      "loss: 1.4735\n",
      "a: -0.0050\n",
      "step:  334\n",
      "loss: 1.2658\n",
      "a: -0.0036\n",
      "step:  335\n",
      "loss: 1.7588\n",
      "a: -0.0023\n",
      "step:  336\n",
      "loss: 1.3266\n",
      "a: -0.0017\n",
      "step:  337\n",
      "loss: 1.5655\n",
      "a: -0.0017\n",
      "step:  338\n",
      "loss: 1.3494\n",
      "a: -0.0024\n",
      "step:  339\n",
      "loss: 1.4804\n",
      "a: -0.0035\n",
      "step:  340\n",
      "loss: 1.5241\n",
      "a: -0.0045\n",
      "step:  341\n",
      "loss: 1.5804\n",
      "a: -0.0054\n",
      "step:  342\n",
      "loss: 1.3673\n",
      "a: -0.0061\n",
      "step:  343\n",
      "loss: 1.5484\n",
      "a: -0.0064\n",
      "step:  344\n",
      "loss: 1.5041\n",
      "a: -0.0066\n",
      "step:  345\n",
      "loss: 1.3817\n",
      "a: -0.0064\n",
      "step:  346\n",
      "loss: 1.3506\n",
      "a: -0.0059\n",
      "step:  347\n",
      "loss: 1.4246\n",
      "a: -0.0050\n",
      "step:  348\n",
      "loss: 1.3054\n",
      "a: -0.0038\n",
      "step:  349\n",
      "loss: 1.3737\n",
      "a: -0.0029\n",
      "step:  350\n",
      "loss: 1.5774\n",
      "a: -0.0025\n",
      "step:  351\n",
      "loss: 1.4159\n",
      "a: -0.0028\n",
      "step:  352\n",
      "loss: 1.4766\n",
      "a: -0.0029\n",
      "step:  353\n",
      "loss: 1.5666\n",
      "a: -0.0026\n",
      "step:  354\n",
      "loss: 1.5964\n",
      "a: -0.0029\n",
      "step:  355\n",
      "loss: 1.5003\n",
      "a: -0.0032\n",
      "step:  356\n",
      "loss: 1.2437\n",
      "a: -0.0035\n",
      "step:  357\n",
      "loss: 1.2803\n",
      "a: -0.0039\n",
      "step:  358\n",
      "loss: 1.5494\n",
      "a: -0.0038\n",
      "step:  359\n",
      "loss: 1.2628\n",
      "a: -0.0041\n",
      "step:  360\n",
      "loss: 1.4170\n",
      "a: -0.0046\n",
      "step:  361\n",
      "loss: 1.4638\n",
      "a: -0.0054\n",
      "step:  362\n",
      "loss: 1.6000\n",
      "a: -0.0060\n",
      "step:  363\n",
      "loss: 1.4271\n",
      "a: -0.0066\n",
      "step:  364\n",
      "loss: 1.3582\n",
      "a: -0.0062\n",
      "step:  365\n",
      "loss: 1.4264\n",
      "a: -0.0050\n",
      "step:  366\n",
      "loss: 1.5278\n",
      "a: -0.0033\n",
      "step:  367\n",
      "loss: 1.3411\n",
      "a: -0.0019\n",
      "step:  368\n",
      "loss: 1.7706\n",
      "a: -0.0013\n",
      "step:  369\n",
      "loss: 1.4430\n",
      "a: -0.0034\n",
      "step:  370\n",
      "loss: 1.4483\n",
      "a: -0.0055\n",
      "step:  371\n",
      "loss: 1.5394\n",
      "a: -0.0072\n",
      "step:  372\n",
      "loss: 1.6347\n",
      "a: -0.0081\n",
      "step:  373\n",
      "loss: 1.4695\n",
      "a: -0.0083\n",
      "step:  374\n",
      "loss: 1.5499\n",
      "a: -0.0080\n",
      "step:  375\n",
      "loss: 1.6720\n",
      "a: -0.0067\n",
      "step:  376\n",
      "loss: 1.1936\n",
      "a: -0.0049\n",
      "step:  377\n",
      "loss: 1.4614\n",
      "a: -0.0032\n",
      "step:  378\n",
      "loss: 1.3751\n",
      "a: -0.0015\n",
      "step:  379\n",
      "loss: 1.8183\n",
      "a: 0.0001\n",
      "step:  380\n",
      "loss: 1.1914\n",
      "a: -0.0024\n",
      "step:  381\n",
      "loss: 1.3997\n",
      "a: -0.0045\n",
      "step:  382\n",
      "loss: 1.3573\n",
      "a: -0.0063\n",
      "step:  383\n",
      "loss: 1.7872\n",
      "a: -0.0079\n",
      "step:  384\n",
      "loss: 1.4352\n",
      "a: -0.0093\n",
      "step:  385\n",
      "loss: 1.5285\n",
      "a: -0.0105\n",
      "step:  386\n",
      "loss: 1.7676\n",
      "a: -0.0115\n",
      "step:  387\n",
      "loss: 1.4668\n",
      "a: -0.0122\n",
      "step:  388\n",
      "loss: 1.8264\n",
      "a: -0.0127\n",
      "step:  389\n",
      "loss: 1.7786\n",
      "a: -0.0129\n",
      "step:  390\n",
      "loss: 1.8431\n",
      "a: -0.0128\n",
      "step:  391\n",
      "loss: 1.8258\n",
      "a: -0.0124\n",
      "step:  392\n",
      "loss: 1.7676\n",
      "a: -0.0116\n",
      "step:  393\n",
      "loss: 1.2490\n",
      "a: -0.0104\n",
      "step:  394\n",
      "loss: 1.5124\n",
      "a: -0.0092\n",
      "step:  395\n",
      "loss: 1.4598\n",
      "a: -0.0079\n",
      "step:  396\n",
      "loss: 1.4803\n",
      "a: -0.0067\n",
      "step:  397\n",
      "loss: 1.5367\n",
      "a: -0.0055\n",
      "step:  398\n",
      "loss: 1.7900\n",
      "a: -0.0044\n",
      "step:  399\n",
      "loss: 1.4096\n",
      "a: -0.0035\n",
      "step:  400\n",
      "loss: 1.3900\n",
      "a: -0.0027\n",
      "step:  401\n",
      "loss: 1.7690\n",
      "a: -0.0022\n",
      "step:  402\n",
      "loss: 1.6105\n",
      "a: -0.0020\n",
      "step:  403\n",
      "loss: 1.6909\n",
      "a: -0.0021\n",
      "step:  404\n",
      "loss: 1.6312\n",
      "a: -0.0027\n",
      "step:  405\n",
      "loss: 1.7770\n",
      "a: -0.0035\n",
      "step:  406\n",
      "loss: 1.5975\n",
      "a: -0.0045\n",
      "step:  407\n",
      "loss: 1.5194\n",
      "a: -0.0056\n",
      "step:  408\n",
      "loss: 1.6813\n",
      "a: -0.0068\n",
      "step:  409\n",
      "loss: 1.4421\n",
      "a: -0.0079\n",
      "step:  410\n",
      "loss: 1.5364\n",
      "a: -0.0090\n",
      "step:  411\n",
      "loss: 1.6065\n",
      "a: -0.0099\n",
      "step:  412\n",
      "loss: 1.4862\n",
      "a: -0.0107\n",
      "step:  413\n",
      "loss: 1.4724\n",
      "a: -0.0114\n",
      "step:  414\n",
      "loss: 1.6989\n",
      "a: -0.0118\n",
      "step:  415\n",
      "loss: 1.8300\n",
      "a: -0.0118\n",
      "step:  416\n",
      "loss: 1.5716\n",
      "a: -0.0115\n",
      "step:  417\n",
      "loss: 1.3857\n",
      "a: -0.0110\n",
      "step:  418\n",
      "loss: 1.6650\n",
      "a: -0.0102\n",
      "step:  419\n",
      "loss: 1.6840\n",
      "a: -0.0092\n",
      "step:  420\n",
      "loss: 1.5038\n",
      "a: -0.0081\n",
      "step:  421\n",
      "loss: 1.6525\n",
      "a: -0.0069\n",
      "step:  422\n",
      "loss: 1.4078\n",
      "a: -0.0056\n",
      "step:  423\n",
      "loss: 1.5590\n",
      "a: -0.0043\n",
      "step:  424\n",
      "loss: 1.3243\n",
      "a: -0.0031\n",
      "step:  425\n",
      "loss: 1.3945\n",
      "a: -0.0018\n",
      "step:  426\n",
      "loss: 1.6448\n",
      "a: -0.0008\n",
      "step:  427\n",
      "loss: 1.9456\n",
      "a: 0.0001\n",
      "step:  428\n",
      "loss: 1.5604\n",
      "a: 0.0003\n",
      "step:  429\n",
      "loss: 1.3839\n",
      "a: -0.0024\n",
      "step:  430\n",
      "loss: 1.5211\n",
      "a: -0.0045\n",
      "step:  431\n",
      "loss: 1.7385\n",
      "a: -0.0064\n",
      "step:  432\n",
      "loss: 1.3746\n",
      "a: -0.0080\n",
      "step:  433\n",
      "loss: 1.5746\n",
      "a: -0.0095\n",
      "step:  434\n",
      "loss: 1.8084\n",
      "a: -0.0109\n",
      "step:  435\n",
      "loss: 1.8677\n",
      "a: -0.0121\n",
      "step:  436\n",
      "loss: 1.7727\n",
      "a: -0.0132\n",
      "step:  437\n",
      "loss: 1.9030\n",
      "a: -0.0141\n",
      "step:  438\n",
      "loss: 2.2071\n",
      "a: -0.0150\n",
      "step:  439\n",
      "loss: 1.9243\n",
      "a: -0.0157\n",
      "step:  440\n",
      "loss: 2.1869\n",
      "a: -0.0162\n",
      "step:  441\n",
      "loss: 2.0041\n",
      "a: -0.0165\n",
      "step:  442\n",
      "loss: 2.3599\n",
      "a: -0.0167\n",
      "step:  443\n",
      "loss: 1.9475\n",
      "a: -0.0167\n",
      "step:  444\n",
      "loss: 1.9053\n",
      "a: -0.0165\n",
      "step:  445\n",
      "loss: 1.7523\n",
      "a: -0.0161\n",
      "step:  446\n",
      "loss: 2.0476\n",
      "a: -0.0155\n",
      "step:  447\n",
      "loss: 2.0135\n",
      "a: -0.0147\n",
      "step:  448\n",
      "loss: 1.8224\n",
      "a: -0.0137\n",
      "step:  449\n",
      "loss: 1.9502\n",
      "a: -0.0126\n",
      "step:  450\n",
      "loss: 1.6249\n",
      "a: -0.0115\n",
      "step:  451\n",
      "loss: 1.4393\n",
      "a: -0.0104\n",
      "step:  452\n",
      "loss: 1.7461\n",
      "a: -0.0092\n",
      "step:  453\n",
      "loss: 1.6359\n",
      "a: -0.0081\n",
      "step:  454\n",
      "loss: 1.6725\n",
      "a: -0.0069\n",
      "step:  455\n",
      "loss: 1.4999\n",
      "a: -0.0058\n",
      "step:  456\n",
      "loss: 1.6318\n",
      "a: -0.0047\n",
      "step:  457\n",
      "loss: 1.3595\n",
      "a: -0.0036\n",
      "step:  458\n",
      "loss: 1.4765\n",
      "a: -0.0026\n",
      "step:  459\n",
      "loss: 1.8408\n",
      "a: -0.0016\n",
      "step:  460\n",
      "loss: 1.6302\n",
      "a: -0.0006\n",
      "step:  461\n",
      "loss: 1.6242\n",
      "a: 0.0002\n",
      "step:  462\n",
      "loss: 1.9497\n",
      "a: 0.0004\n",
      "step:  463\n",
      "loss: 1.5056\n",
      "a: -0.0012\n",
      "step:  464\n",
      "loss: 1.4580\n",
      "a: -0.0025\n",
      "step:  465\n",
      "loss: 1.6800\n",
      "a: -0.0039\n",
      "step:  466\n",
      "loss: 1.5558\n",
      "a: -0.0052\n",
      "step:  467\n",
      "loss: 1.5933\n",
      "a: -0.0065\n",
      "step:  468\n",
      "loss: 1.5844\n",
      "a: -0.0077\n",
      "step:  469\n",
      "loss: 1.5516\n",
      "a: -0.0089\n",
      "step:  470\n",
      "loss: 1.4709\n",
      "a: -0.0100\n",
      "step:  471\n",
      "loss: 1.5399\n",
      "a: -0.0111\n",
      "step:  472\n",
      "loss: 1.5777\n",
      "a: -0.0121\n",
      "step:  473\n",
      "loss: 1.8503\n",
      "a: -0.0130\n",
      "step:  474\n",
      "loss: 1.8718\n",
      "a: -0.0139\n",
      "step:  475\n",
      "loss: 1.9781\n",
      "a: -0.0147\n",
      "step:  476\n",
      "loss: 1.9116\n",
      "a: -0.0153\n",
      "step:  477\n",
      "loss: 2.1566\n",
      "a: -0.0158\n",
      "step:  478\n",
      "loss: 2.1755\n",
      "a: -0.0161\n",
      "step:  479\n",
      "loss: 1.7344\n",
      "a: -0.0162\n",
      "step:  480\n",
      "loss: 2.3249\n",
      "a: -0.0162\n",
      "step:  481\n",
      "loss: 2.1301\n",
      "a: -0.0160\n",
      "step:  482\n",
      "loss: 1.9009\n",
      "a: -0.0156\n",
      "step:  483\n",
      "loss: 2.2466\n",
      "a: -0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  484\n",
      "loss: 1.8223\n",
      "a: -0.0142\n",
      "step:  485\n",
      "loss: 1.6479\n",
      "a: -0.0133\n",
      "step:  486\n",
      "loss: 1.8781\n",
      "a: -0.0122\n",
      "step:  487\n",
      "loss: 1.7053\n",
      "a: -0.0110\n",
      "step:  488\n",
      "loss: 1.6862\n",
      "a: -0.0098\n",
      "step:  489\n",
      "loss: 1.5665\n",
      "a: -0.0085\n",
      "step:  490\n",
      "loss: 1.5441\n",
      "a: -0.0072\n",
      "step:  491\n",
      "loss: 1.5239\n",
      "a: -0.0060\n",
      "step:  492\n",
      "loss: 1.6709\n",
      "a: -0.0048\n",
      "step:  493\n",
      "loss: 1.4837\n",
      "a: -0.0037\n",
      "step:  494\n",
      "loss: 1.5473\n",
      "a: -0.0026\n",
      "step:  495\n",
      "loss: 1.5519\n",
      "a: -0.0015\n",
      "step:  496\n",
      "loss: 1.8529\n",
      "a: -0.0005\n",
      "step:  497\n",
      "loss: 1.7404\n",
      "a: 0.0002\n",
      "step:  498\n",
      "loss: 1.6038\n",
      "a: -0.0001\n",
      "step:  499\n",
      "loss: 1.6841\n",
      "a: -0.0006\n",
      "step:  500\n",
      "loss: 1.5672\n",
      "a: -0.0014\n",
      "step:  501\n",
      "loss: 1.6632\n",
      "a: -0.0022\n",
      "step:  502\n",
      "loss: 1.6321\n",
      "a: -0.0032\n",
      "step:  503\n",
      "loss: 1.4383\n",
      "a: -0.0041\n",
      "step:  504\n",
      "loss: 1.6570\n",
      "a: -0.0051\n",
      "step:  505\n",
      "loss: 1.4947\n",
      "a: -0.0061\n",
      "step:  506\n",
      "loss: 1.6434\n",
      "a: -0.0070\n",
      "step:  507\n",
      "loss: 1.5852\n",
      "a: -0.0079\n",
      "step:  508\n",
      "loss: 1.6863\n",
      "a: -0.0088\n",
      "step:  509\n",
      "loss: 1.5260\n",
      "a: -0.0097\n",
      "step:  510\n",
      "loss: 1.6556\n",
      "a: -0.0104\n",
      "step:  511\n",
      "loss: 1.5281\n",
      "a: -0.0112\n",
      "step:  512\n",
      "loss: 1.5763\n",
      "a: -0.0118\n",
      "step:  513\n",
      "loss: 1.9250\n",
      "a: -0.0124\n",
      "step:  514\n",
      "loss: 1.7211\n",
      "a: -0.0128\n",
      "step:  515\n",
      "loss: 1.5164\n",
      "a: -0.0132\n",
      "step:  516\n",
      "loss: 1.9687\n",
      "a: -0.0133\n",
      "step:  517\n",
      "loss: 1.7745\n",
      "a: -0.0133\n",
      "step:  518\n",
      "loss: 1.6445\n",
      "a: -0.0129\n",
      "step:  519\n",
      "loss: 1.7095\n",
      "a: -0.0125\n",
      "step:  520\n",
      "loss: 1.7216\n",
      "a: -0.0118\n",
      "step:  521\n",
      "loss: 1.7386\n",
      "a: -0.0111\n",
      "step:  522\n",
      "loss: 1.8622\n",
      "a: -0.0101\n",
      "step:  523\n",
      "loss: 1.6694\n",
      "a: -0.0091\n",
      "step:  524\n",
      "loss: 1.6177\n",
      "a: -0.0080\n",
      "step:  525\n",
      "loss: 1.5338\n",
      "a: -0.0069\n",
      "step:  526\n",
      "loss: 1.5866\n",
      "a: -0.0057\n",
      "step:  527\n",
      "loss: 1.6463\n",
      "a: -0.0044\n",
      "step:  528\n",
      "loss: 1.5078\n",
      "a: -0.0032\n",
      "step:  529\n",
      "loss: 1.8282\n",
      "a: -0.0021\n",
      "step:  530\n",
      "loss: 1.5575\n",
      "a: -0.0011\n",
      "step:  531\n",
      "loss: 1.6924\n",
      "a: -0.0002\n",
      "step:  532\n",
      "loss: 2.4011\n",
      "a: 0.0006\n",
      "step:  533\n",
      "loss: 1.6422\n",
      "a: -0.0020\n",
      "step:  534\n",
      "loss: 1.4662\n",
      "a: -0.0041\n",
      "step:  535\n",
      "loss: 1.2930\n",
      "a: -0.0060\n",
      "step:  536\n",
      "loss: 1.5241\n",
      "a: -0.0076\n",
      "step:  537\n",
      "loss: 1.6975\n",
      "a: -0.0091\n",
      "step:  538\n",
      "loss: 1.4229\n",
      "a: -0.0104\n",
      "step:  539\n",
      "loss: 1.7580\n",
      "a: -0.0117\n",
      "step:  540\n",
      "loss: 1.7183\n",
      "a: -0.0129\n",
      "step:  541\n",
      "loss: 2.0004\n",
      "a: -0.0141\n",
      "step:  542\n",
      "loss: 2.1851\n",
      "a: -0.0152\n",
      "step:  543\n",
      "loss: 2.1807\n",
      "a: -0.0162\n",
      "step:  544\n",
      "loss: 2.2652\n",
      "a: -0.0171\n",
      "step:  545\n",
      "loss: 2.1195\n",
      "a: -0.0179\n",
      "step:  546\n",
      "loss: 2.0966\n",
      "a: -0.0187\n",
      "step:  547\n",
      "loss: 2.4934\n",
      "a: -0.0193\n",
      "step:  548\n",
      "loss: 2.6251\n",
      "a: -0.0199\n",
      "step:  549\n",
      "loss: 2.4641\n",
      "a: -0.0203\n",
      "step:  550\n",
      "loss: 2.4123\n",
      "a: -0.0205\n",
      "step:  551\n",
      "loss: 2.4635\n",
      "a: -0.0207\n",
      "step:  552\n",
      "loss: 2.4450\n",
      "a: -0.0207\n",
      "step:  553\n",
      "loss: 2.2607\n",
      "a: -0.0205\n",
      "step:  554\n",
      "loss: 2.2623\n",
      "a: -0.0201\n",
      "step:  555\n",
      "loss: 2.1236\n",
      "a: -0.0196\n",
      "step:  556\n",
      "loss: 1.9308\n",
      "a: -0.0189\n",
      "step:  557\n",
      "loss: 2.1605\n",
      "a: -0.0180\n",
      "step:  558\n",
      "loss: 2.3417\n",
      "a: -0.0170\n",
      "step:  559\n",
      "loss: 1.7416\n",
      "a: -0.0158\n",
      "step:  560\n",
      "loss: 1.9004\n",
      "a: -0.0146\n",
      "step:  561\n",
      "loss: 1.7424\n",
      "a: -0.0133\n",
      "step:  562\n",
      "loss: 1.6182\n",
      "a: -0.0120\n",
      "step:  563\n",
      "loss: 1.4844\n",
      "a: -0.0108\n",
      "step:  564\n",
      "loss: 1.7367\n",
      "a: -0.0095\n",
      "step:  565\n",
      "loss: 1.6736\n",
      "a: -0.0083\n",
      "step:  566\n",
      "loss: 1.8476\n",
      "a: -0.0071\n",
      "step:  567\n",
      "loss: 1.8394\n",
      "a: -0.0060\n",
      "step:  568\n",
      "loss: 1.6672\n",
      "a: -0.0048\n",
      "step:  569\n",
      "loss: 1.8599\n",
      "a: -0.0038\n",
      "step:  570\n",
      "loss: 2.3641\n",
      "a: -0.0028\n",
      "step:  571\n",
      "loss: 1.8664\n",
      "a: -0.0019\n",
      "step:  572\n",
      "loss: 1.6074\n",
      "a: -0.0010\n",
      "step:  573\n",
      "loss: 1.3696\n",
      "a: -0.0002\n",
      "step:  574\n",
      "loss: 2.2987\n",
      "a: 0.0005\n",
      "step:  575\n",
      "loss: 2.0533\n",
      "a: 0.0003\n",
      "step:  576\n",
      "loss: 1.4549\n",
      "a: -0.0008\n",
      "step:  577\n",
      "loss: 1.7564\n",
      "a: -0.0020\n",
      "step:  578\n",
      "loss: 1.6598\n",
      "a: -0.0032\n",
      "step:  579\n",
      "loss: 1.5558\n",
      "a: -0.0044\n",
      "step:  580\n",
      "loss: 1.6511\n",
      "a: -0.0056\n",
      "step:  581\n",
      "loss: 1.5704\n",
      "a: -0.0068\n",
      "step:  582\n",
      "loss: 1.4829\n",
      "a: -0.0080\n",
      "step:  583\n",
      "loss: 1.5268\n",
      "a: -0.0090\n",
      "step:  584\n",
      "loss: 1.5883\n",
      "a: -0.0101\n",
      "step:  585\n",
      "loss: 1.7334\n",
      "a: -0.0111\n",
      "step:  586\n",
      "loss: 2.0150\n",
      "a: -0.0121\n",
      "step:  587\n",
      "loss: 1.7514\n",
      "a: -0.0130\n",
      "step:  588\n",
      "loss: 2.1706\n",
      "a: -0.0139\n",
      "step:  589\n",
      "loss: 1.9335\n",
      "a: -0.0147\n",
      "step:  590\n",
      "loss: 2.0530\n",
      "a: -0.0154\n",
      "step:  591\n",
      "loss: 2.1806\n",
      "a: -0.0160\n",
      "step:  592\n",
      "loss: 2.2779\n",
      "a: -0.0165\n",
      "step:  593\n",
      "loss: 2.2316\n",
      "a: -0.0169\n",
      "step:  594\n",
      "loss: 2.4358\n",
      "a: -0.0172\n",
      "step:  595\n",
      "loss: 2.4354\n",
      "a: -0.0173\n",
      "step:  596\n",
      "loss: 2.0021\n",
      "a: -0.0173\n",
      "step:  597\n",
      "loss: 2.2460\n",
      "a: -0.0171\n",
      "step:  598\n",
      "loss: 1.9036\n",
      "a: -0.0167\n",
      "step:  599\n",
      "loss: 2.0858\n",
      "a: -0.0162\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code snippets to build the convolutional network:\n",
    "\n",
    "```python\n",
    "    from torch . nn . functional import conv2d , max_pool2d\n",
    "    convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "    subsampleing_layer = max_pool_2d ( convolutional_layer , (2 , 2) ) # reduces window 2x2 to 1 pixel\n",
    "    out_layer = dropout ( subsample_layer , p_drop_input )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  80\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "# modify for more speed\n",
    "f1, f2, f3 = 20, 40, 80\n",
    "pic_in1, pic_in2, pic_in3 = 1, 20, 40\n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        X = conv_layer(X, w_conv3, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, w_conv3, p_drop_input)\n",
    "    return Y.size()[1]\n",
    "\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, w_conv3, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([-0.1], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 1., 1.)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(train_loader, test_loader, num_epochs, log_interval):\n",
    "    # run training\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, epoch, log_interval)\n",
    "        test(test_loader)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 17.1060\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 7.5555\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 3.5846\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.9636\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.4309\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.2460\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\src\\TH\\THGeneral.c:218",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-649f2faeaf45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-3f284de02341>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(train_loader, test_loader, num_epochs, log_interval)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-88-3c012f2b7d03>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(test_loader)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# returns average over minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;31m# maybe loss.data[0] ?  # sum up batch loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-8705e152917c>\u001b[0m in \u001b[0;36mcnn\u001b[1;34m(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m# add a here if running with pReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-8705e152917c>\u001b[0m in \u001b[0;36mconv_layer\u001b[1;34m(X, weightvector, p_drop)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrectify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv2d\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# reduces window 2x2 to 1 pixel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\src\\TH\\THGeneral.c:218"
     ]
    }
   ],
   "source": [
    "N_epochs = 30\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-03 *\n",
      "       [ 4.2836])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues:\n",
    " - find nice hyperparameters (learning rate, dropout, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4.2 Application of Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we sketch the network architecure in order to determine the number of neurons in the last layer. For this purpose we have to take into account the following:<br>\n",
    "<ul>\n",
    "<li>Convolving an (n x n) image with a (5 x 5) filter without padding produces an output of dimension (n-4 x n-4) since we lose 2 pixels on each side of the image.</li>\n",
    "<li>Applying a (2 x 2) max pooling layer on  a (n x n) image gives an (n/2 x n/2) output.</li>\n",
    "<li>Convolving an (n x n) image with a (2 x 2) filter witout padding produces an output of dimension (n-1 x n-1).</li>\n",
    "<li>In the last step, we apply a (2 x 2) pooling operation on a (3 x 3) image where the dimensions of filter and image do not really match. Nevertheless we obtain a (1 x 1) output, but maybe, it would be more convenient to apply a (3 x 3) filter in the previous layer such that we could apply the (2 x 2) pooling to a (2 x 2) image?</li>\n",
    "</ul>\n",
    "Taking all this into account, we obtain the architecture displayed below and can read off that the last convolutional layer contains (128 x 1 x 1) = 128 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 2546.5, 782.5, -0.5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACHUAAAKvCAYAAADu/WGZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAABPQ/8KjfCGKKCQcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b88ca4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "architecture = plt.imread('architecture.png')\n",
    "fig, ax = plt.subplots(figsize=(40,12))\n",
    "ax.imshow(architecture)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize one image of the test set, its convolution with 3 filters of the first convolutional layer and the corresponding filter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x9b896fdeb8>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b8870e160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1\n",
    "selected_image = testset[index]\n",
    "plt.imshow(selected_image[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_of_interest = torch.tensor([1,2,3])\n",
    "feature_maps = conv2d(selected_image[0].unsqueeze(0), w_conv1[filters_of_interest,:,:,:])\n",
    "feature_maps = feature_maps.detach()\n",
    "filters = w_conv1.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAJCCAYAAAA2rjfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b89239278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(2,3, figsize=(20,10))\n",
    "for i in range(3):\n",
    "    ax1[0,i].imshow(feature_maps[0,i,:,:])\n",
    "    ax1[1,i].imshow(filters[filters_of_interest[i]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upper row we show the feature maps which have been obtained by convolving the input image with the three filters shown in the lower row."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset in trainset and testset. The trainset consists of 60000 images, the testset of 10000 imgs.\n",
    "\n",
    "trainset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "testset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = [str(i) for i in range(10)]\n",
    "\n",
    "def imshow(img, title=\"\", cmap = \"Greys_r\"): #convert tensor to image\n",
    "    plt.title(title)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    numpyimg = img.numpy()[0]\n",
    "    plt.imshow(numpyimg, cmap = cmap)\n",
    "    \n",
    "\n",
    "def display_10_images_from_dataset(dataset, class_names):\n",
    "    \"\"\"\n",
    "    plots 10 randomly chosen images from a given dataset\n",
    "    \"\"\"\n",
    "    display = [] #holds tuples of image and respective label\n",
    "    for _ in range(10):\n",
    "        index = np.random.randint(0,len(dataset)+1)\n",
    "        display.append(dataset[index])\n",
    "    \n",
    "    nr = 1\n",
    "    fig, axes = plt.subplots(2,5,sharex='col',sharey='row', figsize = (14,9))\n",
    "    for image, label in display:\n",
    "        axes[(nr-1)//5][(nr-1)%5] = plt.subplot(2,5,nr)\n",
    "        plt.title(class_names[label], fontsize = 16)\n",
    "        imshow(image, title = class_names[label])\n",
    "        nr+=1\n",
    "    fig.subplots_adjust(hspace=-0.4)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[0:5]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in fig.axes[1:5]+fig.axes[6:]], visible=False)\n",
    "    plt.show()\n",
    "    plt.savefig(\"previewMNIST.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAFtCAYAAADccl8mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQQYAAABAdBhkAAAAAESHQQYAAABAdP4/3UFmMnFMSDYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3b81d7668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff351cee588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_10_images_from_dataset(testset, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4778\n",
      "Loss: 2.8132\n",
      "Loss: 2.2816\n",
      "Loss: 2.2995\n",
      "Loss: 2.1588\n",
      "Loss: 2.2253\n",
      "Loss: 2.0827\n",
      "Loss: 1.9358\n",
      "Loss: 1.9803\n",
      "Loss: 1.9186\n",
      "Loss: 1.7410\n",
      "Loss: 1.7339\n",
      "Loss: 1.8249\n",
      "Loss: 1.6397\n",
      "Loss: 1.6444\n",
      "Loss: 1.6895\n",
      "Loss: 1.5673\n",
      "Loss: 1.3740\n",
      "Loss: 1.4253\n",
      "Loss: 1.3858\n",
      "Loss: 1.3975\n",
      "Loss: 1.3086\n",
      "Loss: 1.1707\n",
      "Loss: 0.9304\n",
      "Loss: 1.2828\n",
      "Loss: 1.1762\n",
      "Loss: 1.0824\n",
      "Loss: 1.1135\n",
      "Loss: 0.9949\n",
      "Loss: 1.1272\n",
      "Loss: 0.9650\n",
      "Loss: 0.9824\n",
      "Loss: 1.1068\n",
      "Loss: 1.0798\n",
      "Loss: 0.8536\n",
      "Loss: 0.8355\n",
      "Loss: 1.0288\n",
      "Loss: 0.9916\n",
      "Loss: 1.0291\n",
      "Loss: 0.8135\n",
      "Loss: 0.8980\n",
      "Loss: 0.9110\n",
      "Loss: 0.9535\n",
      "Loss: 0.9614\n",
      "Loss: 0.8859\n",
      "Loss: 1.0265\n",
      "Loss: 0.9178\n",
      "Loss: 1.0005\n",
      "Loss: 0.9681\n",
      "Loss: 0.8342\n",
      "Loss: 0.8430\n",
      "Loss: 0.9363\n",
      "Loss: 0.5884\n",
      "Loss: 0.7888\n",
      "Loss: 0.7987\n",
      "Loss: 0.9753\n",
      "Loss: 0.8285\n",
      "Loss: 0.8730\n",
      "Loss: 0.8362\n",
      "Loss: 0.9468\n",
      "Loss: 0.8570\n",
      "Loss: 0.6032\n",
      "Loss: 0.5639\n",
      "Loss: 1.0434\n",
      "Loss: 0.5468\n",
      "Loss: 0.5439\n",
      "Loss: 0.6812\n",
      "Loss: 0.6342\n",
      "Loss: 0.9023\n",
      "Loss: 0.7769\n",
      "Loss: 0.5918\n",
      "Loss: 0.5784\n",
      "Loss: 0.5076\n",
      "Loss: 0.6765\n",
      "Loss: 0.5272\n",
      "Loss: 0.6144\n",
      "Loss: 0.8333\n",
      "Loss: 0.4465\n",
      "Loss: 0.6302\n",
      "Loss: 0.6123\n",
      "Loss: 0.5848\n",
      "Loss: 0.6893\n",
      "Loss: 0.8228\n",
      "Loss: 0.7303\n",
      "Loss: 0.9506\n",
      "Loss: 0.7940\n",
      "Loss: 0.5459\n",
      "Loss: 0.7910\n",
      "Loss: 0.5798\n",
      "Loss: 0.8218\n",
      "Loss: 0.4789\n",
      "Loss: 0.9117\n",
      "Loss: 0.6080\n",
      "Loss: 0.6927\n",
      "Loss: 0.7086\n",
      "Loss: 1.0679\n",
      "Loss: 0.6524\n",
      "Loss: 0.8788\n",
      "Loss: 0.7899\n",
      "Loss: 0.6785\n",
      "Loss: 0.5576\n",
      "Loss: 0.7479\n",
      "Loss: 0.8844\n",
      "Loss: 0.4630\n",
      "Loss: 0.6583\n",
      "Loss: 0.4532\n",
      "Loss: 0.7315\n",
      "Loss: 0.6864\n",
      "Loss: 0.7459\n",
      "Loss: 0.6914\n",
      "Loss: 0.7604\n",
      "Loss: 0.6208\n",
      "Loss: 0.7777\n",
      "Loss: 0.4708\n",
      "Loss: 0.6396\n",
      "Loss: 0.4728\n",
      "Loss: 0.5928\n",
      "Loss: 0.6774\n",
      "Loss: 0.6669\n",
      "Loss: 0.5455\n",
      "Loss: 0.7691\n",
      "Loss: 0.7052\n",
      "Loss: 0.9079\n",
      "Loss: 0.4964\n",
      "Loss: 0.5658\n",
      "Loss: 0.5379\n",
      "Loss: 0.5094\n",
      "Loss: 0.3432\n",
      "Loss: 0.8273\n",
      "Loss: 0.7383\n",
      "Loss: 0.5596\n",
      "Loss: 0.8279\n",
      "Loss: 0.5961\n",
      "Loss: 0.7658\n",
      "Loss: 0.5847\n",
      "Loss: 0.4539\n",
      "Loss: 0.5444\n",
      "Loss: 0.8093\n",
      "Loss: 0.5223\n",
      "Loss: 0.5791\n",
      "Loss: 0.3757\n",
      "Loss: 0.5168\n",
      "Loss: 0.4969\n",
      "Loss: 0.6679\n",
      "Loss: 0.5451\n",
      "Loss: 0.8872\n",
      "Loss: 0.9629\n",
      "Loss: 0.4319\n",
      "Loss: 0.5564\n",
      "Loss: 0.6584\n",
      "Loss: 0.6834\n",
      "Loss: 0.6623\n",
      "Loss: 0.7312\n",
      "Loss: 0.4016\n",
      "Loss: 0.6345\n",
      "Loss: 0.6013\n",
      "Loss: 0.3999\n",
      "Loss: 0.4969\n",
      "Loss: 0.6048\n",
      "Loss: 0.5402\n",
      "Loss: 0.5018\n",
      "Loss: 0.4883\n",
      "Loss: 0.5515\n",
      "Loss: 0.4669\n",
      "Loss: 0.3522\n",
      "Loss: 0.3353\n",
      "Loss: 0.6472\n",
      "Loss: 0.5642\n",
      "Loss: 0.4461\n",
      "Loss: 0.5875\n",
      "Loss: 0.4766\n",
      "Loss: 0.5408\n",
      "Loss: 0.7244\n",
      "Loss: 0.7342\n",
      "Loss: 0.3963\n",
      "Loss: 0.5476\n",
      "Loss: 0.4040\n",
      "Loss: 0.5996\n",
      "Loss: 0.7118\n",
      "Loss: 0.2216\n",
      "Loss: 0.5762\n",
      "Loss: 0.6212\n",
      "Loss: 0.3852\n",
      "Loss: 0.5097\n",
      "Loss: 0.5232\n",
      "Loss: 0.5012\n",
      "Loss: 0.7010\n",
      "Loss: 0.4425\n",
      "Loss: 0.5033\n",
      "Loss: 0.3716\n",
      "Loss: 0.7045\n",
      "Loss: 0.3610\n",
      "Loss: 0.5632\n",
      "Loss: 0.4756\n",
      "Loss: 0.5353\n",
      "Loss: 0.8737\n",
      "Loss: 0.4676\n",
      "Loss: 0.6202\n",
      "Loss: 0.2704\n",
      "Loss: 0.5723\n",
      "Loss: 0.4813\n",
      "Loss: 0.7575\n",
      "Loss: 0.7600\n",
      "Loss: 0.5921\n",
      "Loss: 0.4793\n",
      "Loss: 0.5537\n",
      "Loss: 0.4597\n",
      "Loss: 0.8236\n",
      "Loss: 0.5743\n",
      "Loss: 0.4551\n",
      "Loss: 0.5287\n",
      "Loss: 0.7758\n",
      "Loss: 0.3416\n",
      "Loss: 0.4806\n",
      "Loss: 0.4311\n",
      "Loss: 0.4885\n",
      "Loss: 0.5959\n",
      "Loss: 0.5454\n",
      "Loss: 0.5071\n",
      "Loss: 0.8081\n",
      "Loss: 0.4414\n",
      "Loss: 0.3491\n",
      "Loss: 0.4955\n",
      "Loss: 0.3310\n",
      "Loss: 0.4490\n",
      "Loss: 0.5451\n",
      "Loss: 0.3743\n",
      "Loss: 0.5199\n",
      "Loss: 0.6842\n",
      "Loss: 0.5961\n",
      "Loss: 0.8176\n",
      "Loss: 0.5627\n",
      "Loss: 0.5176\n",
      "Loss: 0.5359\n",
      "Loss: 0.4347\n",
      "Loss: 0.7904\n",
      "Loss: 0.4767\n",
      "Loss: 0.4179\n",
      "Loss: 0.5347\n",
      "Loss: 0.3493\n",
      "Loss: 0.2633\n",
      "Loss: 0.5682\n",
      "Loss: 0.3329\n",
      "Loss: 0.4926\n",
      "Loss: 0.4691\n",
      "Loss: 0.5764\n",
      "Loss: 0.4246\n",
      "Loss: 0.3648\n",
      "Loss: 0.4570\n",
      "Loss: 0.4624\n",
      "Loss: 0.5114\n",
      "Loss: 0.4799\n",
      "Loss: 0.7315\n",
      "Loss: 0.4759\n",
      "Loss: 0.4032\n",
      "Loss: 0.5707\n",
      "Loss: 0.5542\n",
      "Loss: 0.3529\n",
      "Loss: 0.4524\n",
      "Loss: 0.5520\n",
      "Loss: 0.4525\n",
      "Loss: 0.4012\n",
      "Loss: 0.5638\n",
      "Loss: 0.3655\n",
      "Loss: 0.4990\n",
      "Loss: 0.4149\n",
      "Loss: 0.5674\n",
      "Loss: 0.4888\n",
      "Loss: 0.4843\n",
      "Loss: 0.3895\n",
      "Loss: 0.4223\n",
      "Loss: 0.3871\n",
      "Loss: 0.4016\n",
      "Loss: 0.7409\n",
      "Loss: 0.7299\n",
      "Loss: 0.6082\n",
      "Loss: 0.6811\n",
      "Loss: 0.3809\n",
      "Loss: 0.4391\n",
      "Loss: 0.5205\n",
      "Loss: 0.7217\n",
      "Loss: 0.4078\n",
      "Loss: 0.5330\n",
      "Loss: 0.6122\n",
      "Loss: 0.3779\n",
      "Loss: 0.5499\n",
      "Loss: 0.4148\n",
      "Loss: 0.5048\n",
      "Loss: 0.5891\n",
      "Loss: 0.3312\n",
      "Loss: 0.5972\n",
      "Loss: 0.4694\n",
      "Loss: 0.5709\n",
      "Loss: 0.4690\n",
      "Loss: 0.4577\n",
      "Loss: 0.2538\n",
      "Loss: 0.2435\n",
      "Loss: 0.5268\n",
      "Loss: 0.4531\n",
      "Loss: 0.4163\n",
      "Loss: 0.6025\n",
      "Loss: 0.6848\n",
      "Loss: 0.5354\n",
      "Loss: 0.4826\n",
      "Loss: 0.4450\n",
      "Loss: 0.6306\n",
      "Loss: 0.2564\n",
      "Loss: 0.3080\n",
      "Loss: 0.4998\n",
      "Loss: 0.4153\n",
      "Loss: 0.3801\n",
      "Loss: 0.3171\n",
      "Loss: 0.4536\n",
      "Loss: 0.5654\n",
      "Loss: 0.5629\n",
      "Loss: 0.4343\n",
      "Loss: 0.4650\n",
      "Loss: 0.4171\n",
      "Loss: 0.3698\n",
      "Loss: 0.4760\n",
      "Loss: 0.5908\n",
      "Loss: 0.3389\n",
      "Loss: 0.4021\n",
      "Loss: 0.8501\n",
      "Loss: 0.7323\n",
      "Loss: 0.5470\n",
      "Loss: 0.6896\n",
      "Loss: 0.7975\n",
      "Loss: 0.2379\n",
      "Loss: 0.5325\n",
      "Loss: 0.6367\n",
      "Loss: 0.4930\n",
      "Loss: 0.4360\n",
      "Loss: 0.3875\n",
      "Loss: 0.7462\n",
      "Loss: 0.4158\n",
      "Loss: 0.5452\n",
      "Loss: 0.4656\n",
      "Loss: 0.5516\n",
      "Loss: 0.7693\n",
      "Loss: 0.4253\n",
      "Loss: 0.6567\n",
      "Loss: 0.5229\n",
      "Loss: 0.4763\n",
      "Loss: 0.4260\n",
      "Loss: 0.6470\n",
      "Loss: 0.5526\n",
      "Loss: 0.5139\n",
      "Loss: 0.4341\n",
      "Loss: 0.6560\n",
      "Loss: 0.3825\n",
      "Loss: 0.3204\n",
      "Loss: 0.3847\n",
      "Loss: 0.5553\n",
      "Loss: 0.3107\n",
      "Loss: 0.6836\n",
      "Loss: 0.5586\n",
      "Loss: 0.5962\n",
      "Loss: 0.5459\n",
      "Loss: 0.4831\n",
      "Loss: 0.4673\n",
      "Loss: 0.5891\n",
      "Loss: 0.5566\n",
      "Loss: 0.3816\n",
      "Loss: 0.3550\n",
      "Loss: 0.1646\n",
      "Loss: 0.3733\n",
      "Loss: 0.5002\n",
      "Loss: 0.4156\n",
      "Loss: 0.5096\n",
      "Loss: 0.5526\n",
      "Loss: 0.4856\n",
      "Loss: 0.3263\n",
      "Loss: 0.3103\n",
      "Loss: 0.5332\n",
      "Loss: 0.4645\n",
      "Loss: 0.4243\n",
      "Loss: 0.6297\n",
      "Loss: 0.5596\n",
      "Loss: 0.6073\n",
      "Loss: 0.2627\n",
      "Loss: 0.2511\n",
      "Loss: 0.4399\n",
      "Loss: 0.4664\n",
      "Loss: 0.3347\n",
      "Loss: 0.6353\n",
      "Loss: 0.1449\n",
      "Loss: 0.5204\n",
      "Loss: 0.6273\n",
      "Loss: 0.3794\n",
      "Loss: 0.7270\n",
      "Loss: 0.5829\n",
      "Loss: 0.5936\n",
      "Loss: 0.3213\n",
      "Loss: 0.3023\n",
      "Loss: 0.5144\n",
      "Loss: 0.2910\n",
      "Loss: 0.3918\n",
      "Loss: 0.4481\n",
      "Loss: 0.5464\n",
      "Loss: 0.4328\n",
      "Loss: 0.3248\n",
      "Loss: 0.7680\n",
      "Loss: 0.6519\n",
      "Loss: 0.5330\n",
      "Loss: 0.6493\n",
      "Loss: 0.6969\n",
      "Loss: 0.7230\n",
      "Loss: 0.5682\n",
      "Loss: 0.3828\n",
      "Loss: 0.3418\n",
      "Loss: 0.4612\n",
      "Loss: 0.7261\n",
      "Loss: 0.5658\n",
      "Loss: 0.4051\n",
      "Loss: 0.4461\n",
      "Loss: 0.5298\n",
      "Loss: 0.5214\n",
      "Loss: 0.5800\n",
      "Loss: 0.5851\n",
      "Loss: 0.4367\n",
      "Loss: 0.7601\n",
      "Loss: 0.4398\n",
      "Loss: 0.4688\n",
      "Loss: 0.5417\n",
      "Loss: 0.6612\n",
      "Loss: 0.4692\n",
      "Loss: 0.5371\n",
      "Loss: 0.4485\n",
      "Loss: 0.3461\n",
      "Loss: 0.2455\n",
      "Loss: 0.2657\n",
      "Loss: 0.8599\n",
      "Loss: 0.3667\n",
      "Loss: 0.4799\n",
      "Loss: 0.4115\n",
      "Loss: 0.2979\n",
      "Loss: 0.6066\n",
      "Loss: 0.5547\n",
      "Loss: 0.5686\n",
      "Loss: 0.4192\n",
      "Loss: 0.4781\n",
      "Loss: 0.4574\n",
      "Loss: 0.4100\n",
      "Loss: 0.2381\n",
      "Loss: 0.4338\n",
      "Loss: 0.4713\n",
      "Loss: 0.4675\n",
      "Loss: 0.3734\n",
      "Loss: 0.3740\n",
      "Loss: 0.4242\n",
      "Loss: 0.3656\n",
      "Loss: 0.3887\n",
      "Loss: 0.3924\n",
      "Loss: 0.4674\n",
      "Loss: 0.3813\n",
      "Loss: 0.4709\n",
      "Loss: 0.3294\n",
      "Loss: 0.4594\n",
      "Loss: 0.4803\n",
      "Loss: 0.5004\n",
      "Loss: 0.4029\n",
      "Loss: 0.3807\n",
      "Loss: 0.4306\n",
      "Loss: 0.5567\n",
      "Loss: 0.5665\n",
      "Loss: 0.3925\n",
      "Loss: 0.5334\n",
      "Loss: 0.3803\n",
      "Loss: 0.3758\n",
      "Loss: 0.4542\n",
      "Loss: 0.4821\n",
      "Loss: 0.4971\n",
      "Loss: 0.5495\n",
      "Loss: 0.5037\n",
      "Loss: 0.4114\n",
      "Loss: 0.6149\n",
      "Loss: 0.6140\n",
      "Loss: 0.5707\n",
      "Loss: 0.6154\n",
      "Loss: 0.3583\n",
      "Loss: 0.5013\n",
      "Loss: 0.5089\n",
      "Loss: 0.3674\n",
      "Loss: 0.5228\n",
      "Loss: 0.3611\n",
      "Loss: 0.5253\n",
      "Loss: 0.7166\n",
      "Loss: 0.4517\n",
      "Loss: 0.2825\n",
      "Loss: 0.3782\n",
      "Loss: 0.4785\n",
      "Loss: 0.4862\n",
      "Loss: 0.4466\n",
      "Loss: 0.4780\n",
      "Loss: 0.4291\n",
      "Loss: 0.5181\n",
      "Loss: 0.5071\n",
      "Loss: 0.1835\n",
      "Loss: 0.5212\n",
      "Loss: 0.3916\n",
      "Loss: 0.5432\n",
      "Loss: 0.4307\n",
      "Loss: 0.4396\n",
      "Loss: 0.5632\n",
      "Loss: 0.2767\n",
      "Loss: 0.4574\n",
      "Loss: 0.3330\n",
      "Loss: 0.5449\n",
      "Loss: 0.7600\n",
      "Loss: 0.4593\n",
      "Loss: 0.6589\n",
      "Loss: 0.8356\n",
      "Loss: 0.7380\n",
      "Loss: 0.6477\n",
      "Loss: 0.4797\n",
      "Loss: 0.4043\n",
      "Loss: 0.4441\n",
      "Loss: 0.5904\n",
      "Loss: 0.3510\n",
      "Loss: 0.5544\n",
      "Loss: 0.4808\n",
      "Loss: 0.5007\n",
      "Loss: 0.6934\n",
      "Loss: 0.4829\n",
      "Loss: 0.4490\n",
      "Loss: 0.4449\n",
      "Loss: 0.4712\n",
      "Loss: 0.6745\n",
      "Loss: 0.6663\n",
      "Loss: 0.3794\n",
      "Loss: 0.5584\n",
      "Loss: 0.3614\n",
      "Loss: 0.5854\n",
      "Loss: 0.6009\n",
      "Loss: 0.5545\n",
      "Loss: 0.4947\n",
      "Loss: 0.4786\n",
      "Loss: 0.4903\n",
      "Loss: 0.4182\n",
      "Loss: 0.2345\n",
      "Loss: 0.4146\n",
      "Loss: 0.5122\n",
      "Loss: 0.3512\n",
      "Loss: 0.6245\n",
      "Loss: 0.5976\n",
      "Loss: 0.6241\n",
      "Loss: 0.1946\n",
      "Loss: 0.6108\n",
      "Loss: 0.3374\n",
      "Loss: 0.3232\n",
      "Loss: 0.6088\n",
      "Loss: 0.6087\n",
      "Loss: 0.5332\n",
      "Loss: 0.4884\n",
      "Loss: 0.5782\n",
      "Loss: 0.4672\n",
      "Loss: 0.4896\n",
      "Loss: 0.4338\n",
      "Loss: 0.2458\n",
      "Loss: 0.5928\n",
      "Loss: 0.5548\n",
      "Loss: 0.3211\n",
      "Loss: 0.1714\n",
      "Loss: 0.3712\n",
      "Loss: 0.3658\n",
      "Loss: 0.4294\n",
      "Loss: 0.4848\n",
      "Loss: 0.3971\n",
      "Loss: 0.4812\n",
      "Loss: 0.3743\n",
      "Loss: 0.5546\n",
      "Loss: 0.6688\n",
      "Loss: 0.3815\n",
      "Loss: 0.4882\n",
      "Loss: 0.7311\n",
      "Loss: 0.5004\n",
      "Loss: 0.3023\n",
      "Loss: 0.3926\n",
      "Loss: 0.4089\n",
      "Loss: 0.3661\n",
      "Loss: 0.4979\n",
      "Loss: 0.2947\n",
      "Loss: 0.4486\n",
      "Loss: 0.5326\n",
      "Loss: 0.3260\n",
      "Loss: 0.5367\n",
      "Loss: 0.6906\n",
      "Loss: 0.5964\n",
      "Loss: 0.3636\n",
      "Loss: 0.5832\n",
      "Loss: 0.5638\n",
      "Loss: 0.4524\n",
      "Loss: 0.5058\n",
      "Loss: 0.4886\n",
      "Loss: 0.5324\n",
      "Loss: 0.4204\n",
      "Loss: 0.8415\n",
      "Loss: 0.2984\n",
      "Loss: 0.5630\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print('Loss: %.4f' % cost)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.235685110092163\n",
      "Loss: 2.6122305393218994\n",
      "Loss: 2.633882761001587\n",
      "Loss: 2.463536262512207\n",
      "Loss: 2.253995418548584\n",
      "Loss: 2.227691173553467\n",
      "Loss: 2.2037668228149414\n",
      "Loss: 2.2376036643981934\n",
      "Loss: 2.2737526893615723\n",
      "Loss: 2.1525075435638428\n",
      "Loss: 2.1420462131500244\n",
      "Loss: 2.104735851287842\n",
      "Loss: 2.2754225730895996\n",
      "Loss: 2.1633718013763428\n",
      "Loss: 2.124685525894165\n",
      "Loss: 2.085428237915039\n",
      "Loss: 2.080772876739502\n",
      "Loss: 1.9635969400405884\n",
      "Loss: 1.9688693284988403\n",
      "Loss: 2.071608543395996\n",
      "Loss: 1.9973366260528564\n",
      "Loss: 1.8948277235031128\n",
      "Loss: 1.8804954290390015\n",
      "Loss: 1.899318814277649\n",
      "Loss: 1.8611576557159424\n",
      "Loss: 1.9064974784851074\n",
      "Loss: 1.904478907585144\n",
      "Loss: 1.9299267530441284\n",
      "Loss: 1.7353285551071167\n",
      "Loss: 1.746910572052002\n",
      "Loss: 1.9866044521331787\n",
      "Loss: 1.872605323791504\n",
      "Loss: 1.9907395839691162\n",
      "Loss: 1.8297711610794067\n",
      "Loss: 1.760430932044983\n",
      "Loss: 1.9318454265594482\n",
      "Loss: 1.8175140619277954\n",
      "Loss: 1.8376332521438599\n",
      "Loss: 1.6989854574203491\n",
      "Loss: 1.582572340965271\n",
      "Loss: 1.8445755243301392\n",
      "Loss: 1.7279452085494995\n",
      "Loss: 1.8538498878479004\n",
      "Loss: 1.6030325889587402\n",
      "Loss: 1.6972392797470093\n",
      "Loss: 1.833166480064392\n",
      "Loss: 1.6762057542800903\n",
      "Loss: 1.7540795803070068\n",
      "Loss: 1.7157599925994873\n",
      "Loss: 1.5768641233444214\n",
      "Loss: 1.4103089570999146\n",
      "Loss: 1.5442253351211548\n",
      "Loss: 1.577795386314392\n",
      "Loss: 1.6203781366348267\n",
      "Loss: 1.6437221765518188\n",
      "Loss: 1.670914888381958\n",
      "Loss: 1.6315300464630127\n",
      "Loss: 1.7126332521438599\n",
      "Loss: 1.8169559240341187\n",
      "Loss: 1.4232901334762573\n",
      "Loss: 1.5889582633972168\n",
      "Loss: 1.4107666015625\n",
      "Loss: 1.5896573066711426\n",
      "Loss: 1.4864424467086792\n",
      "Loss: 1.431215524673462\n",
      "Loss: 1.6170637607574463\n",
      "Loss: 1.5263373851776123\n",
      "Loss: 1.5201152563095093\n",
      "Loss: 1.66840398311615\n",
      "Loss: 1.4270063638687134\n",
      "Loss: 1.4221171140670776\n",
      "Loss: 1.619748830795288\n",
      "Loss: 1.5797163248062134\n",
      "Loss: 1.518632411956787\n",
      "Loss: 1.5847445726394653\n",
      "Loss: 1.669315218925476\n",
      "Loss: 1.4403001070022583\n",
      "Loss: 1.5390825271606445\n",
      "Loss: 1.648163914680481\n",
      "Loss: 1.5826245546340942\n",
      "Loss: 1.3668900728225708\n",
      "Loss: 1.497296929359436\n",
      "Loss: 1.6096783876419067\n",
      "Loss: 1.7891534566879272\n",
      "Loss: 1.590016484260559\n",
      "Loss: 1.3957737684249878\n",
      "Loss: 1.5452258586883545\n",
      "Loss: 1.5920830965042114\n",
      "Loss: 1.62259840965271\n",
      "Loss: 1.413475751876831\n",
      "Loss: 1.4523564577102661\n",
      "Loss: 1.3758866786956787\n",
      "Loss: 1.5771470069885254\n",
      "Loss: 1.4463021755218506\n",
      "Loss: 1.3915131092071533\n",
      "Loss: 1.313422679901123\n",
      "Loss: 1.5220309495925903\n",
      "Loss: 1.5391067266464233\n",
      "Loss: 1.5877798795700073\n",
      "Loss: 1.497165322303772\n",
      "Loss: 1.3383429050445557\n",
      "Loss: 1.379634976387024\n",
      "Loss: 1.3977731466293335\n",
      "Loss: 1.4110832214355469\n",
      "Loss: 1.4959138631820679\n",
      "Loss: 1.4388912916183472\n",
      "Loss: 1.1972342729568481\n",
      "Loss: 1.4660602807998657\n",
      "Loss: 1.4974255561828613\n",
      "Loss: 1.2635573148727417\n",
      "Loss: 1.6518816947937012\n",
      "Loss: 1.2052518129348755\n",
      "Loss: 1.3269137144088745\n",
      "Loss: 1.4603450298309326\n",
      "Loss: 1.7873073816299438\n",
      "Loss: 1.3309401273727417\n",
      "Loss: 1.2543457746505737\n",
      "Loss: 1.335268497467041\n",
      "Loss: 1.3518387079238892\n",
      "Loss: 1.397591233253479\n",
      "Loss: 1.6119184494018555\n",
      "Loss: 1.6606438159942627\n",
      "Loss: 1.3041276931762695\n",
      "Loss: 1.3731377124786377\n",
      "Loss: 1.409706711769104\n",
      "Loss: 1.4399464130401611\n",
      "Loss: 1.3342857360839844\n",
      "Loss: 1.457720160484314\n",
      "Loss: 1.2296181917190552\n",
      "Loss: 1.4223198890686035\n",
      "Loss: 1.383384108543396\n",
      "Loss: 1.2598140239715576\n",
      "Loss: 1.2187786102294922\n",
      "Loss: 1.2607779502868652\n",
      "Loss: 1.0820035934448242\n",
      "Loss: 1.6282519102096558\n",
      "Loss: 1.3300622701644897\n",
      "Loss: 1.098014235496521\n",
      "Loss: 1.4603869915008545\n",
      "Loss: 1.431526780128479\n",
      "Loss: 1.1109938621520996\n",
      "Loss: 1.2632489204406738\n",
      "Loss: 1.4098953008651733\n",
      "Loss: 1.1122719049453735\n",
      "Loss: 1.2248601913452148\n",
      "Loss: 1.6237895488739014\n",
      "Loss: 1.763085126876831\n",
      "Loss: 1.632572889328003\n",
      "Loss: 1.3014280796051025\n",
      "Loss: 1.3012524843215942\n",
      "Loss: 1.250795841217041\n",
      "Loss: 1.4027079343795776\n",
      "Loss: 1.3468444347381592\n",
      "Loss: 1.5457476377487183\n",
      "Loss: 1.277178168296814\n",
      "Loss: 1.196873664855957\n",
      "Loss: 1.2334718704223633\n",
      "Loss: 1.0899207592010498\n",
      "Loss: 1.5114836692810059\n",
      "Loss: 1.340662956237793\n",
      "Loss: 1.253896951675415\n",
      "Loss: 1.2866239547729492\n",
      "Loss: 1.342124581336975\n",
      "Loss: 1.324912428855896\n",
      "Loss: 1.2508654594421387\n",
      "Loss: 1.2259007692337036\n",
      "Loss: 1.2639505863189697\n",
      "Loss: 1.392605185508728\n",
      "Loss: 1.2682194709777832\n",
      "Loss: 1.2202367782592773\n",
      "Loss: 1.1189669370651245\n",
      "Loss: 1.4634824991226196\n",
      "Loss: 1.1182035207748413\n",
      "Loss: 1.617924451828003\n",
      "Loss: 1.312849998474121\n",
      "Loss: 1.7567520141601562\n",
      "Loss: 1.2642279863357544\n",
      "Loss: 1.0016878843307495\n",
      "Loss: 1.278499722480774\n",
      "Loss: 1.3395122289657593\n",
      "Loss: 1.0473235845565796\n",
      "Loss: 1.303757667541504\n",
      "Loss: 1.0364866256713867\n",
      "Loss: 1.3158003091812134\n",
      "Loss: 1.0334320068359375\n",
      "Loss: 1.3454898595809937\n",
      "Loss: 1.610535740852356\n",
      "Loss: 1.117316484451294\n",
      "Loss: 1.3290739059448242\n",
      "Loss: 1.1589869260787964\n",
      "Loss: 1.2751556634902954\n",
      "Loss: 1.118372917175293\n",
      "Loss: 1.0789942741394043\n",
      "Loss: 1.2052178382873535\n",
      "Loss: 1.25667142868042\n",
      "Loss: 1.087365746498108\n",
      "Loss: 1.26752507686615\n",
      "Loss: 1.3001240491867065\n",
      "Loss: 1.2131319046020508\n",
      "Loss: 1.383064866065979\n",
      "Loss: 1.0269824266433716\n",
      "Loss: 1.267236351966858\n",
      "Loss: 1.2051447629928589\n",
      "Loss: 1.1581027507781982\n",
      "Loss: 1.2143549919128418\n",
      "Loss: 1.4053155183792114\n",
      "Loss: 1.2736320495605469\n",
      "Loss: 1.4560455083847046\n",
      "Loss: 1.3637226819992065\n",
      "Loss: 1.280386209487915\n",
      "Loss: 0.9875839352607727\n",
      "Loss: 1.3149687051773071\n",
      "Loss: 1.1509945392608643\n",
      "Loss: 1.4508002996444702\n",
      "Loss: 1.1669307947158813\n",
      "Loss: 1.1835060119628906\n",
      "Loss: 1.4548437595367432\n",
      "Loss: 1.2214583158493042\n",
      "Loss: 1.1926418542861938\n",
      "Loss: 1.3775122165679932\n",
      "Loss: 1.3205671310424805\n",
      "Loss: 1.2841286659240723\n",
      "Loss: 1.219007968902588\n",
      "Loss: 1.1335276365280151\n",
      "Loss: 1.2119208574295044\n",
      "Loss: 1.2257529497146606\n",
      "Loss: 1.2654953002929688\n",
      "Loss: 1.1596870422363281\n",
      "Loss: 0.9888041615486145\n",
      "Loss: 1.4068117141723633\n",
      "Loss: 1.2808972597122192\n",
      "Loss: 1.1286488771438599\n",
      "Loss: 1.3028886318206787\n",
      "Loss: 1.3703080415725708\n",
      "Loss: 1.261032223701477\n",
      "Loss: 1.4445573091506958\n",
      "Loss: 1.1982226371765137\n",
      "Loss: 1.2251136302947998\n",
      "Loss: 1.4952645301818848\n",
      "Loss: 0.9675335884094238\n",
      "Loss: 1.4960349798202515\n",
      "Loss: 1.3032640218734741\n",
      "Loss: 1.3615292310714722\n",
      "Loss: 1.1314189434051514\n",
      "Loss: 1.0800020694732666\n",
      "Loss: 1.1606308221817017\n",
      "Loss: 1.2875251770019531\n",
      "Loss: 0.9796814918518066\n",
      "Loss: 1.0468345880508423\n",
      "Loss: 1.3220371007919312\n",
      "Loss: 1.2171556949615479\n",
      "Loss: 1.4108604192733765\n",
      "Loss: 1.0667273998260498\n",
      "Loss: 1.2724387645721436\n",
      "Loss: 0.9733000993728638\n",
      "Loss: 1.1657841205596924\n",
      "Loss: 1.2085964679718018\n",
      "Loss: 1.4145327806472778\n",
      "Loss: 1.0316940546035767\n",
      "Loss: 1.2020612955093384\n",
      "Loss: 1.4044190645217896\n",
      "Loss: 1.2166486978530884\n",
      "Loss: 1.3140478134155273\n",
      "Loss: 1.1699028015136719\n",
      "Loss: 1.0640208721160889\n",
      "Loss: 1.1109665632247925\n",
      "Loss: 1.282559871673584\n",
      "Loss: 1.13416588306427\n",
      "Loss: 1.378733515739441\n",
      "Loss: 1.3009631633758545\n",
      "Loss: 1.1336040496826172\n",
      "Loss: 1.117401361465454\n",
      "Loss: 0.9894036054611206\n",
      "Loss: 1.2349382638931274\n",
      "Loss: 1.293837547302246\n",
      "Loss: 1.0066263675689697\n",
      "Loss: 1.144315481185913\n",
      "Loss: 1.2892123460769653\n",
      "Loss: 1.3642756938934326\n",
      "Loss: 1.1503819227218628\n",
      "Loss: 1.238848090171814\n",
      "Loss: 1.2947592735290527\n",
      "Loss: 1.2793198823928833\n",
      "Loss: 1.3867994546890259\n",
      "Loss: 1.1846293210983276\n",
      "Loss: 1.3020211458206177\n",
      "Loss: 1.2613575458526611\n",
      "Loss: 1.4629641771316528\n",
      "Loss: 1.1246036291122437\n",
      "Loss: 1.4205130338668823\n",
      "Loss: 1.4473727941513062\n",
      "Loss: 1.1376562118530273\n",
      "Loss: 1.229251503944397\n",
      "Loss: 1.0496493577957153\n",
      "Loss: 1.1700372695922852\n",
      "Loss: 1.2896748781204224\n",
      "Loss: 1.1218352317810059\n",
      "Loss: 1.2222869396209717\n",
      "Loss: 1.0251193046569824\n",
      "Loss: 1.4456483125686646\n",
      "Loss: 1.1176317930221558\n",
      "Loss: 1.35329270362854\n",
      "Loss: 1.3475489616394043\n",
      "Loss: 1.1959071159362793\n",
      "Loss: 1.2581703662872314\n",
      "Loss: 1.0993905067443848\n",
      "Loss: 0.9449941515922546\n",
      "Loss: 1.2014377117156982\n",
      "Loss: 1.462572455406189\n",
      "Loss: 1.3420929908752441\n",
      "Loss: 1.0339393615722656\n",
      "Loss: 1.534349799156189\n",
      "Loss: 1.1335688829421997\n",
      "Loss: 1.3005404472351074\n",
      "Loss: 1.113824486732483\n",
      "Loss: 1.1736772060394287\n",
      "Loss: 1.0841693878173828\n",
      "Loss: 1.008832573890686\n",
      "Loss: 1.2442015409469604\n",
      "Loss: 1.378468632698059\n",
      "Loss: 1.2424261569976807\n",
      "Loss: 1.1010956764221191\n",
      "Loss: 1.2528785467147827\n",
      "Loss: 1.4006870985031128\n",
      "Loss: 1.3451794385910034\n",
      "Loss: 1.1506346464157104\n",
      "Loss: 1.104378581047058\n",
      "Loss: 1.1331069469451904\n",
      "Loss: 1.5056166648864746\n",
      "Loss: 1.4190624952316284\n",
      "Loss: 1.3183910846710205\n",
      "Loss: 1.2393198013305664\n",
      "Loss: 1.3059238195419312\n",
      "Loss: 1.1074869632720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2563494443893433\n",
      "Loss: 1.403342843055725\n",
      "Loss: 1.1448407173156738\n",
      "Loss: 1.2700246572494507\n",
      "Loss: 1.197383999824524\n",
      "Loss: 1.1524964570999146\n",
      "Loss: 1.1833837032318115\n",
      "Loss: 1.2829405069351196\n",
      "Loss: 1.2575676441192627\n",
      "Loss: 1.3691112995147705\n",
      "Loss: 1.1953160762786865\n",
      "Loss: 0.9843840003013611\n",
      "Loss: 1.049376130104065\n",
      "Loss: 1.2238192558288574\n",
      "Loss: 1.0818769931793213\n",
      "Loss: 1.221265435218811\n",
      "Loss: 1.296981692314148\n",
      "Loss: 1.2105437517166138\n",
      "Loss: 1.3517882823944092\n",
      "Loss: 1.4353647232055664\n",
      "Loss: 1.157196044921875\n",
      "Loss: 1.3252780437469482\n",
      "Loss: 1.3122432231903076\n",
      "Loss: 1.1853066682815552\n",
      "Loss: 1.2519562244415283\n",
      "Loss: 1.3399854898452759\n",
      "Loss: 1.3301687240600586\n",
      "Loss: 1.2446739673614502\n",
      "Loss: 0.9673069715499878\n",
      "Loss: 1.179593563079834\n",
      "Loss: 1.198248028755188\n",
      "Loss: 1.2492250204086304\n",
      "Loss: 1.1838258504867554\n",
      "Loss: 1.1154186725616455\n",
      "Loss: 1.2435503005981445\n",
      "Loss: 1.1733181476593018\n",
      "Loss: 1.2816259860992432\n",
      "Loss: 1.4226664304733276\n",
      "Loss: 1.1587389707565308\n",
      "Loss: 1.1353973150253296\n",
      "Loss: 1.1618744134902954\n",
      "Loss: 1.3599145412445068\n",
      "Loss: 1.1938884258270264\n",
      "Loss: 1.4208449125289917\n",
      "Loss: 1.2440860271453857\n",
      "Loss: 1.2013260126113892\n",
      "Loss: 1.167707085609436\n",
      "Loss: 1.190779685974121\n",
      "Loss: 1.1260404586791992\n",
      "Loss: 1.0278937816619873\n",
      "Loss: 1.303391456604004\n",
      "Loss: 1.3113423585891724\n",
      "Loss: 1.2229841947555542\n",
      "Loss: 1.0783264636993408\n",
      "Loss: 1.2692532539367676\n",
      "Loss: 1.2664270401000977\n",
      "Loss: 1.1705702543258667\n",
      "Loss: 1.1029691696166992\n",
      "Loss: 1.2340068817138672\n",
      "Loss: 1.0087275505065918\n",
      "Loss: 1.2008581161499023\n",
      "Loss: 1.1323069334030151\n",
      "Loss: 1.0741431713104248\n",
      "Loss: 1.0341225862503052\n",
      "Loss: 1.0548293590545654\n",
      "Loss: 1.1281077861785889\n",
      "Loss: 1.0293493270874023\n",
      "Loss: 1.1280714273452759\n",
      "Loss: 1.2192203998565674\n",
      "Loss: 1.3690423965454102\n",
      "Loss: 1.1918632984161377\n",
      "Loss: 1.2687883377075195\n",
      "Loss: 1.3266428709030151\n",
      "Loss: 1.1690545082092285\n",
      "Loss: 1.0636903047561646\n",
      "Loss: 1.2885087728500366\n",
      "Loss: 1.456192970275879\n",
      "Loss: 1.339510440826416\n",
      "Loss: 1.3222934007644653\n",
      "Loss: 1.2157245874404907\n",
      "Loss: 1.2957513332366943\n",
      "Loss: 1.1628409624099731\n",
      "Loss: 0.9602407217025757\n",
      "Loss: 1.3418344259262085\n",
      "Loss: 1.2332618236541748\n",
      "Loss: 1.0941405296325684\n",
      "Loss: 1.273746132850647\n",
      "Loss: 1.234391450881958\n",
      "Loss: 1.2861526012420654\n",
      "Loss: 1.3526252508163452\n",
      "Loss: 1.0530059337615967\n",
      "Loss: 1.195702075958252\n",
      "Loss: 1.2838160991668701\n",
      "Loss: 1.321865200996399\n",
      "Loss: 1.2850500345230103\n",
      "Loss: 1.1863939762115479\n",
      "Loss: 1.2922964096069336\n",
      "Loss: 0.9848041534423828\n",
      "Loss: 1.1725280284881592\n",
      "Loss: 1.1318854093551636\n",
      "Loss: 1.270263433456421\n",
      "Loss: 1.2737932205200195\n",
      "Loss: 1.2582436800003052\n",
      "Loss: 1.2190213203430176\n",
      "Loss: 1.478855848312378\n",
      "Loss: 1.263303518295288\n",
      "Loss: 1.0482492446899414\n",
      "Loss: 1.1727783679962158\n",
      "Loss: 1.2216111421585083\n",
      "Loss: 0.8183339834213257\n",
      "Loss: 1.4877914190292358\n",
      "Loss: 1.1857587099075317\n",
      "Loss: 0.9796085357666016\n",
      "Loss: 1.1918102502822876\n",
      "Loss: 1.1566601991653442\n",
      "Loss: 1.1214625835418701\n",
      "Loss: 1.2897753715515137\n",
      "Loss: 1.393506646156311\n",
      "Loss: 1.1876639127731323\n",
      "Loss: 1.3913214206695557\n",
      "Loss: 1.1665016412734985\n",
      "Loss: 1.1178789138793945\n",
      "Loss: 1.231469988822937\n",
      "Loss: 1.1592686176300049\n",
      "Loss: 1.1318601369857788\n",
      "Loss: 1.157965898513794\n",
      "Loss: 1.1896733045578003\n",
      "Loss: 1.0742554664611816\n",
      "Loss: 1.121546983718872\n",
      "Loss: 1.0540724992752075\n",
      "Loss: 1.296531081199646\n",
      "Loss: 1.1609470844268799\n",
      "Loss: 1.66414213180542\n",
      "Loss: 1.3626017570495605\n",
      "Loss: 1.2246931791305542\n",
      "Loss: 1.3278210163116455\n",
      "Loss: 0.998862087726593\n",
      "Loss: 1.0043548345565796\n",
      "Loss: 1.2588485479354858\n",
      "Loss: 1.3776724338531494\n",
      "Loss: 1.0444905757904053\n",
      "Loss: 1.3926353454589844\n",
      "Loss: 1.283640742301941\n",
      "Loss: 1.3935601711273193\n",
      "Loss: 1.2700749635696411\n",
      "Loss: 1.4010580778121948\n",
      "Loss: 1.2938357591629028\n",
      "Loss: 1.2393853664398193\n",
      "Loss: 1.1966496706008911\n",
      "Loss: 1.2760347127914429\n",
      "Loss: 1.4307186603546143\n",
      "Loss: 1.2587332725524902\n",
      "Loss: 1.5049831867218018\n",
      "Loss: 1.1174768209457397\n",
      "Loss: 1.3139582872390747\n",
      "Loss: 1.2776484489440918\n",
      "Loss: 1.1688232421875\n",
      "Loss: 1.2090564966201782\n",
      "Loss: 1.3832415342330933\n",
      "Loss: 1.4675743579864502\n",
      "Loss: 1.2586028575897217\n",
      "Loss: 1.2986814975738525\n",
      "Loss: 1.0805147886276245\n",
      "Loss: 1.240971326828003\n",
      "Loss: 1.2335131168365479\n",
      "Loss: 1.1076582670211792\n",
      "Loss: 1.2298142910003662\n",
      "Loss: 1.3393030166625977\n",
      "Loss: 1.3650034666061401\n",
      "Loss: 1.1393321752548218\n",
      "Loss: 1.3616055250167847\n",
      "Loss: 1.1057307720184326\n",
      "Loss: 1.307349443435669\n",
      "Loss: 1.2807817459106445\n",
      "Loss: 1.2801460027694702\n",
      "Loss: 1.2532916069030762\n",
      "Loss: 1.2900947332382202\n",
      "Loss: 1.0855380296707153\n",
      "Loss: 1.379518747329712\n",
      "Loss: 1.1467593908309937\n",
      "Loss: 0.9852449893951416\n",
      "Loss: 1.4831522703170776\n",
      "Loss: 1.1293200254440308\n",
      "Loss: 1.4231446981430054\n",
      "Loss: 1.0936334133148193\n",
      "Loss: 1.4027410745620728\n",
      "Loss: 1.1791912317276\n",
      "Loss: 1.1081740856170654\n",
      "Loss: 1.1920348405838013\n",
      "Loss: 1.1149975061416626\n",
      "Loss: 1.2038017511367798\n",
      "Loss: 1.0106333494186401\n",
      "Loss: 1.4339001178741455\n",
      "Loss: 1.1490801572799683\n",
      "Loss: 1.4813343286514282\n",
      "Loss: 1.8436601161956787\n",
      "Loss: 0.992838442325592\n",
      "Loss: 1.2004950046539307\n",
      "Loss: 1.2700743675231934\n",
      "Loss: 1.0762827396392822\n",
      "Loss: 1.3768911361694336\n",
      "Loss: 1.4488389492034912\n",
      "Loss: 1.3022938966751099\n",
      "Loss: 1.3164743185043335\n",
      "Loss: 1.0845494270324707\n",
      "Loss: 1.231898307800293\n",
      "Loss: 1.4370489120483398\n",
      "Loss: 1.406612515449524\n",
      "Loss: 1.1785396337509155\n",
      "Loss: 1.197891354560852\n",
      "Loss: 1.1143451929092407\n",
      "Loss: 1.0594983100891113\n",
      "Loss: 1.1966118812561035\n",
      "Loss: 1.439035177230835\n",
      "Loss: 0.9877829551696777\n",
      "Loss: 1.245133638381958\n",
      "Loss: 1.376039981842041\n",
      "Loss: 1.07768976688385\n",
      "Loss: 1.2206538915634155\n",
      "Loss: 1.2591651678085327\n",
      "Loss: 1.369421124458313\n",
      "Loss: 1.1184288263320923\n",
      "Loss: 1.0475637912750244\n",
      "Loss: 0.930880069732666\n",
      "Loss: 1.2883648872375488\n",
      "Loss: 1.217452883720398\n",
      "Loss: 1.0205053091049194\n",
      "Loss: 1.0486727952957153\n",
      "Loss: 1.3300988674163818\n",
      "Loss: 1.0711673498153687\n",
      "Loss: 1.2951669692993164\n",
      "Loss: 1.0698494911193848\n",
      "Loss: 1.7003238201141357\n",
      "Loss: 1.296053409576416\n",
      "Loss: 1.2219146490097046\n",
      "Loss: 1.202470302581787\n",
      "Loss: 1.3263145685195923\n",
      "Loss: 1.121506929397583\n",
      "Loss: 1.2151283025741577\n",
      "Loss: 1.3277678489685059\n",
      "Loss: 1.4927781820297241\n",
      "Loss: 1.1611453294754028\n",
      "Loss: 1.3179692029953003\n",
      "Loss: 1.4605231285095215\n",
      "Loss: 1.1721575260162354\n",
      "Loss: 1.158571481704712\n",
      "Loss: 1.1376765966415405\n",
      "Loss: 1.0068988800048828\n",
      "Loss: 1.4054572582244873\n",
      "Loss: 1.2447326183319092\n",
      "Loss: 1.4498543739318848\n",
      "Loss: 1.3311924934387207\n",
      "Loss: 1.0564143657684326\n",
      "Loss: 1.1576443910598755\n",
      "Loss: 1.2988070249557495\n",
      "Loss: 1.1543501615524292\n",
      "Loss: 1.1974117755889893\n",
      "Loss: 1.5257419347763062\n",
      "Loss: 1.471794605255127\n",
      "Loss: 1.1776920557022095\n",
      "Loss: 1.3780800104141235\n",
      "Loss: 1.1459885835647583\n",
      "Loss: 1.1982214450836182\n",
      "Loss: 1.676869511604309\n",
      "Loss: 1.2468976974487305\n",
      "Loss: 1.1324021816253662\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 3.4085\n",
      "a: -0.1000\n",
      "step:  1\n",
      "loss: 2.5221\n",
      "a: -0.0968\n",
      "step:  2\n",
      "loss: 2.5012\n",
      "a: -0.0941\n",
      "step:  3\n",
      "loss: 2.6047\n",
      "a: -0.0919\n",
      "step:  4\n",
      "loss: 2.3506\n",
      "a: -0.0891\n",
      "step:  5\n",
      "loss: 2.4400\n",
      "a: -0.0866\n",
      "step:  6\n",
      "loss: 2.3091\n",
      "a: -0.0840\n",
      "step:  7\n",
      "loss: 2.3205\n",
      "a: -0.0816\n",
      "step:  8\n",
      "loss: 2.3555\n",
      "a: -0.0792\n",
      "step:  9\n",
      "loss: 2.3413\n",
      "a: -0.0770\n",
      "step:  10\n",
      "loss: 2.3310\n",
      "a: -0.0750\n",
      "step:  11\n",
      "loss: 2.4262\n",
      "a: -0.0729\n",
      "step:  12\n",
      "loss: 2.2930\n",
      "a: -0.0709\n",
      "step:  13\n",
      "loss: 2.3100\n",
      "a: -0.0690\n",
      "step:  14\n",
      "loss: 2.2143\n",
      "a: -0.0672\n",
      "step:  15\n",
      "loss: 2.2310\n",
      "a: -0.0655\n",
      "step:  16\n",
      "loss: 2.2795\n",
      "a: -0.0639\n",
      "step:  17\n",
      "loss: 2.3911\n",
      "a: -0.0623\n",
      "step:  18\n",
      "loss: 2.2091\n",
      "a: -0.0607\n",
      "step:  19\n",
      "loss: 2.1138\n",
      "a: -0.0591\n",
      "step:  20\n",
      "loss: 2.1172\n",
      "a: -0.0576\n",
      "step:  21\n",
      "loss: 2.2578\n",
      "a: -0.0561\n",
      "step:  22\n",
      "loss: 2.2086\n",
      "a: -0.0546\n",
      "step:  23\n",
      "loss: 2.1716\n",
      "a: -0.0532\n",
      "step:  24\n",
      "loss: 2.0940\n",
      "a: -0.0518\n",
      "step:  25\n",
      "loss: 2.1407\n",
      "a: -0.0503\n",
      "step:  26\n",
      "loss: 2.1106\n",
      "a: -0.0489\n",
      "step:  27\n",
      "loss: 2.0607\n",
      "a: -0.0474\n",
      "step:  28\n",
      "loss: 2.1161\n",
      "a: -0.0460\n",
      "step:  29\n",
      "loss: 1.9676\n",
      "a: -0.0447\n",
      "step:  30\n",
      "loss: 2.1163\n",
      "a: -0.0434\n",
      "step:  31\n",
      "loss: 1.9599\n",
      "a: -0.0420\n",
      "step:  32\n",
      "loss: 2.1428\n",
      "a: -0.0407\n",
      "step:  33\n",
      "loss: 2.0046\n",
      "a: -0.0394\n",
      "step:  34\n",
      "loss: 1.9258\n",
      "a: -0.0381\n",
      "step:  35\n",
      "loss: 1.9724\n",
      "a: -0.0368\n",
      "step:  36\n",
      "loss: 1.8967\n",
      "a: -0.0355\n",
      "step:  37\n",
      "loss: 1.9229\n",
      "a: -0.0342\n",
      "step:  38\n",
      "loss: 1.8829\n",
      "a: -0.0329\n",
      "step:  39\n",
      "loss: 1.8738\n",
      "a: -0.0316\n",
      "step:  40\n",
      "loss: 1.9295\n",
      "a: -0.0304\n",
      "step:  41\n",
      "loss: 1.7957\n",
      "a: -0.0292\n",
      "step:  42\n",
      "loss: 1.8951\n",
      "a: -0.0280\n",
      "step:  43\n",
      "loss: 1.7749\n",
      "a: -0.0268\n",
      "step:  44\n",
      "loss: 1.9031\n",
      "a: -0.0256\n",
      "step:  45\n",
      "loss: 1.7559\n",
      "a: -0.0244\n",
      "step:  46\n",
      "loss: 1.8441\n",
      "a: -0.0233\n",
      "step:  47\n",
      "loss: 1.7691\n",
      "a: -0.0222\n",
      "step:  48\n",
      "loss: 1.6500\n",
      "a: -0.0210\n",
      "step:  49\n",
      "loss: 1.8786\n",
      "a: -0.0198\n",
      "step:  50\n",
      "loss: 1.6281\n",
      "a: -0.0186\n",
      "step:  51\n",
      "loss: 1.9056\n",
      "a: -0.0174\n",
      "step:  52\n",
      "loss: 1.9222\n",
      "a: -0.0162\n",
      "step:  53\n",
      "loss: 1.5522\n",
      "a: -0.0150\n",
      "step:  54\n",
      "loss: 1.7388\n",
      "a: -0.0138\n",
      "step:  55\n",
      "loss: 1.7017\n",
      "a: -0.0127\n",
      "step:  56\n",
      "loss: 1.8135\n",
      "a: -0.0116\n",
      "step:  57\n",
      "loss: 1.7568\n",
      "a: -0.0105\n",
      "step:  58\n",
      "loss: 1.5650\n",
      "a: -0.0095\n",
      "step:  59\n",
      "loss: 1.6894\n",
      "a: -0.0084\n",
      "step:  60\n",
      "loss: 1.7822\n",
      "a: -0.0074\n",
      "step:  61\n",
      "loss: 1.8146\n",
      "a: -0.0064\n",
      "step:  62\n",
      "loss: 1.7770\n",
      "a: -0.0055\n",
      "step:  63\n",
      "loss: 1.7493\n",
      "a: -0.0045\n",
      "step:  64\n",
      "loss: 1.6809\n",
      "a: -0.0035\n",
      "step:  65\n",
      "loss: 1.5123\n",
      "a: -0.0026\n",
      "step:  66\n",
      "loss: 1.4301\n",
      "a: -0.0017\n",
      "step:  67\n",
      "loss: 1.6585\n",
      "a: -0.0008\n",
      "step:  68\n",
      "loss: 1.6485\n",
      "a: -0.0001\n",
      "step:  69\n",
      "loss: 1.8119\n",
      "a: 0.0007\n",
      "step:  70\n",
      "loss: 1.3545\n",
      "a: 0.0014\n",
      "step:  71\n",
      "loss: 1.7212\n",
      "a: 0.0021\n",
      "step:  72\n",
      "loss: 1.7668\n",
      "a: 0.0025\n",
      "step:  73\n",
      "loss: 1.5602\n",
      "a: 0.0028\n",
      "step:  74\n",
      "loss: 1.8980\n",
      "a: 0.0030\n",
      "step:  75\n",
      "loss: 1.6417\n",
      "a: 0.0031\n",
      "step:  76\n",
      "loss: 1.6261\n",
      "a: 0.0029\n",
      "step:  77\n",
      "loss: 1.8146\n",
      "a: 0.0026\n",
      "step:  78\n",
      "loss: 1.9130\n",
      "a: 0.0020\n",
      "step:  79\n",
      "loss: 1.6634\n",
      "a: 0.0011\n",
      "step:  80\n",
      "loss: 1.7140\n",
      "a: 0.0001\n",
      "step:  81\n",
      "loss: 1.6001\n",
      "a: -0.0009\n",
      "step:  82\n",
      "loss: 1.5459\n",
      "a: -0.0019\n",
      "step:  83\n",
      "loss: 1.7669\n",
      "a: -0.0030\n",
      "step:  84\n",
      "loss: 1.6140\n",
      "a: -0.0040\n",
      "step:  85\n",
      "loss: 1.5271\n",
      "a: -0.0051\n",
      "step:  86\n",
      "loss: 1.4989\n",
      "a: -0.0062\n",
      "step:  87\n",
      "loss: 1.5702\n",
      "a: -0.0072\n",
      "step:  88\n",
      "loss: 1.6632\n",
      "a: -0.0082\n",
      "step:  89\n",
      "loss: 1.7257\n",
      "a: -0.0090\n",
      "step:  90\n",
      "loss: 1.8331\n",
      "a: -0.0096\n",
      "step:  91\n",
      "loss: 1.7898\n",
      "a: -0.0101\n",
      "step:  92\n",
      "loss: 1.5673\n",
      "a: -0.0104\n",
      "step:  93\n",
      "loss: 1.6740\n",
      "a: -0.0103\n",
      "step:  94\n",
      "loss: 1.6874\n",
      "a: -0.0100\n",
      "step:  95\n",
      "loss: 1.5068\n",
      "a: -0.0093\n",
      "step:  96\n",
      "loss: 1.7248\n",
      "a: -0.0085\n",
      "step:  97\n",
      "loss: 1.6170\n",
      "a: -0.0075\n",
      "step:  98\n",
      "loss: 1.5882\n",
      "a: -0.0063\n",
      "step:  99\n",
      "loss: 1.5802\n",
      "a: -0.0052\n",
      "step:  100\n",
      "loss: 1.6569\n",
      "a: -0.0041\n",
      "step:  101\n",
      "loss: 1.7392\n",
      "a: -0.0029\n",
      "step:  102\n",
      "loss: 1.5889\n",
      "a: -0.0020\n",
      "step:  103\n",
      "loss: 1.6225\n",
      "a: -0.0010\n",
      "step:  104\n",
      "loss: 1.5066\n",
      "a: -0.0003\n",
      "step:  105\n",
      "loss: 1.8229\n",
      "a: 0.0004\n",
      "step:  106\n",
      "loss: 1.6268\n",
      "a: 0.0010\n",
      "step:  107\n",
      "loss: 1.4473\n",
      "a: 0.0012\n",
      "step:  108\n",
      "loss: 1.5546\n",
      "a: 0.0013\n",
      "step:  109\n",
      "loss: 1.5561\n",
      "a: 0.0010\n",
      "step:  110\n",
      "loss: 1.5924\n",
      "a: 0.0002\n",
      "step:  111\n",
      "loss: 1.2335\n",
      "a: -0.0006\n",
      "step:  112\n",
      "loss: 1.6826\n",
      "a: -0.0015\n",
      "step:  113\n",
      "loss: 1.6981\n",
      "a: -0.0025\n",
      "step:  114\n",
      "loss: 1.3215\n",
      "a: -0.0034\n",
      "step:  115\n",
      "loss: 1.4294\n",
      "a: -0.0038\n",
      "step:  116\n",
      "loss: 1.6396\n",
      "a: -0.0041\n",
      "step:  117\n",
      "loss: 1.7817\n",
      "a: -0.0046\n",
      "step:  118\n",
      "loss: 1.7511\n",
      "a: -0.0051\n",
      "step:  119\n",
      "loss: 1.7832\n",
      "a: -0.0050\n",
      "step:  120\n",
      "loss: 1.6281\n",
      "a: -0.0046\n",
      "step:  121\n",
      "loss: 1.5801\n",
      "a: -0.0036\n",
      "step:  122\n",
      "loss: 1.4008\n",
      "a: -0.0029\n",
      "step:  123\n",
      "loss: 1.4286\n",
      "a: -0.0028\n",
      "step:  124\n",
      "loss: 1.6410\n",
      "a: -0.0028\n",
      "step:  125\n",
      "loss: 1.3077\n",
      "a: -0.0030\n",
      "step:  126\n",
      "loss: 1.6435\n",
      "a: -0.0031\n",
      "step:  127\n",
      "loss: 1.4600\n",
      "a: -0.0035\n",
      "step:  128\n",
      "loss: 1.7056\n",
      "a: -0.0044\n",
      "step:  129\n",
      "loss: 1.7571\n",
      "a: -0.0057\n",
      "step:  130\n",
      "loss: 1.4880\n",
      "a: -0.0061\n",
      "step:  131\n",
      "loss: 1.5708\n",
      "a: -0.0063\n",
      "step:  132\n",
      "loss: 1.5376\n",
      "a: -0.0054\n",
      "step:  133\n",
      "loss: 1.4449\n",
      "a: -0.0041\n",
      "step:  134\n",
      "loss: 1.6267\n",
      "a: -0.0026\n",
      "step:  135\n",
      "loss: 1.4549\n",
      "a: -0.0010\n",
      "step:  136\n",
      "loss: 1.5073\n",
      "a: 0.0003\n",
      "step:  137\n",
      "loss: 1.7053\n",
      "a: 0.0006\n",
      "step:  138\n",
      "loss: 1.4918\n",
      "a: 0.0003\n",
      "step:  139\n",
      "loss: 1.6660\n",
      "a: 0.0000\n",
      "step:  140\n",
      "loss: 1.4423\n",
      "a: 0.0002\n",
      "step:  141\n",
      "loss: 1.5049\n",
      "a: 0.0009\n",
      "step:  142\n",
      "loss: 1.5169\n",
      "a: 0.0013\n",
      "step:  143\n",
      "loss: 1.5914\n",
      "a: 0.0005\n",
      "step:  144\n",
      "loss: 1.5522\n",
      "a: -0.0002\n",
      "step:  145\n",
      "loss: 1.5344\n",
      "a: -0.0012\n",
      "step:  146\n",
      "loss: 1.4068\n",
      "a: -0.0027\n",
      "step:  147\n",
      "loss: 1.6969\n",
      "a: -0.0042\n",
      "step:  148\n",
      "loss: 1.6349\n",
      "a: -0.0054\n",
      "step:  149\n",
      "loss: 1.5064\n",
      "a: -0.0052\n",
      "step:  150\n",
      "loss: 1.4180\n",
      "a: -0.0037\n",
      "step:  151\n",
      "loss: 1.5857\n",
      "a: -0.0020\n",
      "step:  152\n",
      "loss: 1.4785\n",
      "a: -0.0012\n",
      "step:  153\n",
      "loss: 1.4036\n",
      "a: -0.0012\n",
      "step:  154\n",
      "loss: 1.4994\n",
      "a: -0.0025\n",
      "step:  155\n",
      "loss: 1.4950\n",
      "a: -0.0034\n",
      "step:  156\n",
      "loss: 1.4671\n",
      "a: -0.0043\n",
      "step:  157\n",
      "loss: 1.3805\n",
      "a: -0.0048\n",
      "step:  158\n",
      "loss: 1.6333\n",
      "a: -0.0048\n",
      "step:  159\n",
      "loss: 1.6569\n",
      "a: -0.0030\n",
      "step:  160\n",
      "loss: 1.4015\n",
      "a: -0.0014\n",
      "step:  161\n",
      "loss: 1.4531\n",
      "a: -0.0007\n",
      "step:  162\n",
      "loss: 1.5482\n",
      "a: -0.0009\n",
      "step:  163\n",
      "loss: 1.5637\n",
      "a: -0.0024\n",
      "step:  164\n",
      "loss: 1.5310\n",
      "a: -0.0034\n",
      "step:  165\n",
      "loss: 1.5554\n",
      "a: -0.0045\n",
      "step:  166\n",
      "loss: 1.5648\n",
      "a: -0.0047\n",
      "step:  167\n",
      "loss: 1.4782\n",
      "a: -0.0043\n",
      "step:  168\n",
      "loss: 1.4542\n",
      "a: -0.0036\n",
      "step:  169\n",
      "loss: 1.5651\n",
      "a: -0.0028\n",
      "step:  170\n",
      "loss: 1.6215\n",
      "a: -0.0008\n",
      "step:  171\n",
      "loss: 1.9141\n",
      "a: 0.0011\n",
      "step:  172\n",
      "loss: 1.5872\n",
      "a: -0.0013\n",
      "step:  173\n",
      "loss: 1.6786\n",
      "a: -0.0034\n",
      "step:  174\n",
      "loss: 1.4823\n",
      "a: -0.0051\n",
      "step:  175\n",
      "loss: 1.4915\n",
      "a: -0.0064\n",
      "step:  176\n",
      "loss: 1.7009\n",
      "a: -0.0071\n",
      "step:  177\n",
      "loss: 1.7038\n",
      "a: -0.0071\n",
      "step:  178\n",
      "loss: 1.5468\n",
      "a: -0.0060\n",
      "step:  179\n",
      "loss: 1.4559\n",
      "a: -0.0043\n",
      "step:  180\n",
      "loss: 1.4176\n",
      "a: -0.0026\n",
      "step:  181\n",
      "loss: 1.8261\n",
      "a: -0.0012\n",
      "step:  182\n",
      "loss: 1.7024\n",
      "a: -0.0000\n",
      "step:  183\n",
      "loss: 1.4918\n",
      "a: 0.0004\n",
      "step:  184\n",
      "loss: 1.4905\n",
      "a: 0.0003\n",
      "step:  185\n",
      "loss: 1.3890\n",
      "a: -0.0000\n",
      "step:  186\n",
      "loss: 1.4765\n",
      "a: -0.0009\n",
      "step:  187\n",
      "loss: 1.5421\n",
      "a: -0.0017\n",
      "step:  188\n",
      "loss: 1.2771\n",
      "a: -0.0026\n",
      "step:  189\n",
      "loss: 1.4485\n",
      "a: -0.0035\n",
      "step:  190\n",
      "loss: 1.4907\n",
      "a: -0.0041\n",
      "step:  191\n",
      "loss: 1.1742\n",
      "a: -0.0045\n",
      "step:  192\n",
      "loss: 1.6574\n",
      "a: -0.0045\n",
      "step:  193\n",
      "loss: 1.2833\n",
      "a: -0.0037\n",
      "step:  194\n",
      "loss: 1.5668\n",
      "a: -0.0030\n",
      "step:  195\n",
      "loss: 1.5464\n",
      "a: -0.0023\n",
      "step:  196\n",
      "loss: 1.5504\n",
      "a: -0.0014\n",
      "step:  197\n",
      "loss: 1.5030\n",
      "a: -0.0005\n",
      "step:  198\n",
      "loss: 1.6402\n",
      "a: -0.0003\n",
      "step:  199\n",
      "loss: 1.4054\n",
      "a: -0.0010\n",
      "step:  200\n",
      "loss: 1.3490\n",
      "a: -0.0025\n",
      "step:  201\n",
      "loss: 1.5555\n",
      "a: -0.0039\n",
      "step:  202\n",
      "loss: 1.2723\n",
      "a: -0.0054\n",
      "step:  203\n",
      "loss: 1.2722\n",
      "a: -0.0063\n",
      "step:  204\n",
      "loss: 1.5342\n",
      "a: -0.0073\n",
      "step:  205\n",
      "loss: 1.4231\n",
      "a: -0.0076\n",
      "step:  206\n",
      "loss: 1.4120\n",
      "a: -0.0072\n",
      "step:  207\n",
      "loss: 1.7237\n",
      "a: -0.0063\n",
      "step:  208\n",
      "loss: 1.7755\n",
      "a: -0.0051\n",
      "step:  209\n",
      "loss: 1.3786\n",
      "a: -0.0033\n",
      "step:  210\n",
      "loss: 1.5147\n",
      "a: -0.0017\n",
      "step:  211\n",
      "loss: 1.7378\n",
      "a: -0.0004\n",
      "step:  212\n",
      "loss: 1.5177\n",
      "a: 0.0004\n",
      "step:  213\n",
      "loss: 1.4615\n",
      "a: -0.0005\n",
      "step:  214\n",
      "loss: 1.4202\n",
      "a: -0.0017\n",
      "step:  215\n",
      "loss: 1.3972\n",
      "a: -0.0033\n",
      "step:  216\n",
      "loss: 1.3857\n",
      "a: -0.0048\n",
      "step:  217\n",
      "loss: 1.6005\n",
      "a: -0.0062\n",
      "step:  218\n",
      "loss: 1.5054\n",
      "a: -0.0073\n",
      "step:  219\n",
      "loss: 1.8596\n",
      "a: -0.0082\n",
      "step:  220\n",
      "loss: 1.1927\n",
      "a: -0.0086\n",
      "step:  221\n",
      "loss: 1.6698\n",
      "a: -0.0087\n",
      "step:  222\n",
      "loss: 1.6810\n",
      "a: -0.0080\n",
      "step:  223\n",
      "loss: 1.3340\n",
      "a: -0.0071\n",
      "step:  224\n",
      "loss: 1.5324\n",
      "a: -0.0058\n",
      "step:  225\n",
      "loss: 1.3293\n",
      "a: -0.0046\n",
      "step:  226\n",
      "loss: 1.4565\n",
      "a: -0.0033\n",
      "step:  227\n",
      "loss: 1.5513\n",
      "a: -0.0021\n",
      "step:  228\n",
      "loss: 1.6032\n",
      "a: -0.0014\n",
      "step:  229\n",
      "loss: 1.5490\n",
      "a: -0.0013\n",
      "step:  230\n",
      "loss: 1.4824\n",
      "a: -0.0018\n",
      "step:  231\n",
      "loss: 1.5488\n",
      "a: -0.0027\n",
      "step:  232\n",
      "loss: 1.3808\n",
      "a: -0.0035\n",
      "step:  233\n",
      "loss: 1.5430\n",
      "a: -0.0044\n",
      "step:  234\n",
      "loss: 1.4282\n",
      "a: -0.0052\n",
      "step:  235\n",
      "loss: 1.6667\n",
      "a: -0.0056\n",
      "step:  236\n",
      "loss: 1.5187\n",
      "a: -0.0058\n",
      "step:  237\n",
      "loss: 1.3366\n",
      "a: -0.0056\n",
      "step:  238\n",
      "loss: 1.5825\n",
      "a: -0.0054\n",
      "step:  239\n",
      "loss: 1.5864\n",
      "a: -0.0046\n",
      "step:  240\n",
      "loss: 1.6622\n",
      "a: -0.0037\n",
      "step:  241\n",
      "loss: 1.4211\n",
      "a: -0.0033\n",
      "step:  242\n",
      "loss: 1.5797\n",
      "a: -0.0030\n",
      "step:  243\n",
      "loss: 1.6200\n",
      "a: -0.0025\n",
      "step:  244\n",
      "loss: 1.1813\n",
      "a: -0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  245\n",
      "loss: 1.5209\n",
      "a: -0.0017\n",
      "step:  246\n",
      "loss: 1.5846\n",
      "a: -0.0019\n",
      "step:  247\n",
      "loss: 1.3325\n",
      "a: -0.0028\n",
      "step:  248\n",
      "loss: 1.3627\n",
      "a: -0.0039\n",
      "step:  249\n",
      "loss: 1.6347\n",
      "a: -0.0049\n",
      "step:  250\n",
      "loss: 1.4698\n",
      "a: -0.0052\n",
      "step:  251\n",
      "loss: 1.5196\n",
      "a: -0.0057\n",
      "step:  252\n",
      "loss: 1.4727\n",
      "a: -0.0055\n",
      "step:  253\n",
      "loss: 1.4236\n",
      "a: -0.0046\n",
      "step:  254\n",
      "loss: 1.3833\n",
      "a: -0.0031\n",
      "step:  255\n",
      "loss: 1.2631\n",
      "a: -0.0018\n",
      "step:  256\n",
      "loss: 1.5503\n",
      "a: -0.0013\n",
      "step:  257\n",
      "loss: 1.7177\n",
      "a: -0.0021\n",
      "step:  258\n",
      "loss: 1.5474\n",
      "a: -0.0038\n",
      "step:  259\n",
      "loss: 1.6490\n",
      "a: -0.0051\n",
      "step:  260\n",
      "loss: 1.3808\n",
      "a: -0.0059\n",
      "step:  261\n",
      "loss: 1.5072\n",
      "a: -0.0061\n",
      "step:  262\n",
      "loss: 1.4202\n",
      "a: -0.0058\n",
      "step:  263\n",
      "loss: 1.4471\n",
      "a: -0.0049\n",
      "step:  264\n",
      "loss: 1.4630\n",
      "a: -0.0037\n",
      "step:  265\n",
      "loss: 1.2740\n",
      "a: -0.0028\n",
      "step:  266\n",
      "loss: 1.6468\n",
      "a: -0.0021\n",
      "step:  267\n",
      "loss: 1.3712\n",
      "a: -0.0020\n",
      "step:  268\n",
      "loss: 1.4291\n",
      "a: -0.0029\n",
      "step:  269\n",
      "loss: 1.7333\n",
      "a: -0.0041\n",
      "step:  270\n",
      "loss: 1.5332\n",
      "a: -0.0057\n",
      "step:  271\n",
      "loss: 1.3080\n",
      "a: -0.0071\n",
      "step:  272\n",
      "loss: 1.6563\n",
      "a: -0.0082\n",
      "step:  273\n",
      "loss: 1.3753\n",
      "a: -0.0089\n",
      "step:  274\n",
      "loss: 1.5225\n",
      "a: -0.0095\n",
      "step:  275\n",
      "loss: 1.7651\n",
      "a: -0.0097\n",
      "step:  276\n",
      "loss: 1.4318\n",
      "a: -0.0091\n",
      "step:  277\n",
      "loss: 1.5283\n",
      "a: -0.0082\n",
      "step:  278\n",
      "loss: 1.3391\n",
      "a: -0.0069\n",
      "step:  279\n",
      "loss: 1.4175\n",
      "a: -0.0056\n",
      "step:  280\n",
      "loss: 1.2237\n",
      "a: -0.0042\n",
      "step:  281\n",
      "loss: 1.4786\n",
      "a: -0.0030\n",
      "step:  282\n",
      "loss: 1.6658\n",
      "a: -0.0023\n",
      "step:  283\n",
      "loss: 1.6725\n",
      "a: -0.0022\n",
      "step:  284\n",
      "loss: 1.4878\n",
      "a: -0.0030\n",
      "step:  285\n",
      "loss: 1.3781\n",
      "a: -0.0043\n",
      "step:  286\n",
      "loss: 1.3871\n",
      "a: -0.0055\n",
      "step:  287\n",
      "loss: 1.6144\n",
      "a: -0.0066\n",
      "step:  288\n",
      "loss: 1.4349\n",
      "a: -0.0078\n",
      "step:  289\n",
      "loss: 1.6048\n",
      "a: -0.0087\n",
      "step:  290\n",
      "loss: 1.4849\n",
      "a: -0.0093\n",
      "step:  291\n",
      "loss: 1.3999\n",
      "a: -0.0093\n",
      "step:  292\n",
      "loss: 1.5296\n",
      "a: -0.0088\n",
      "step:  293\n",
      "loss: 1.5687\n",
      "a: -0.0079\n",
      "step:  294\n",
      "loss: 1.3354\n",
      "a: -0.0065\n",
      "step:  295\n",
      "loss: 1.3680\n",
      "a: -0.0051\n",
      "step:  296\n",
      "loss: 1.2096\n",
      "a: -0.0038\n",
      "step:  297\n",
      "loss: 1.4351\n",
      "a: -0.0026\n",
      "step:  298\n",
      "loss: 1.5171\n",
      "a: -0.0015\n",
      "step:  299\n",
      "loss: 1.5058\n",
      "a: -0.0008\n",
      "step:  300\n",
      "loss: 1.6863\n",
      "a: -0.0019\n",
      "step:  301\n",
      "loss: 1.4714\n",
      "a: -0.0039\n",
      "step:  302\n",
      "loss: 1.3872\n",
      "a: -0.0059\n",
      "step:  303\n",
      "loss: 1.4142\n",
      "a: -0.0077\n",
      "step:  304\n",
      "loss: 1.5576\n",
      "a: -0.0091\n",
      "step:  305\n",
      "loss: 1.4400\n",
      "a: -0.0102\n",
      "step:  306\n",
      "loss: 1.7764\n",
      "a: -0.0110\n",
      "step:  307\n",
      "loss: 1.7344\n",
      "a: -0.0115\n",
      "step:  308\n",
      "loss: 1.6417\n",
      "a: -0.0113\n",
      "step:  309\n",
      "loss: 1.5149\n",
      "a: -0.0106\n",
      "step:  310\n",
      "loss: 1.4317\n",
      "a: -0.0094\n",
      "step:  311\n",
      "loss: 1.4205\n",
      "a: -0.0081\n",
      "step:  312\n",
      "loss: 1.4711\n",
      "a: -0.0067\n",
      "step:  313\n",
      "loss: 1.3685\n",
      "a: -0.0054\n",
      "step:  314\n",
      "loss: 1.3144\n",
      "a: -0.0041\n",
      "step:  315\n",
      "loss: 1.4795\n",
      "a: -0.0029\n",
      "step:  316\n",
      "loss: 1.4846\n",
      "a: -0.0020\n",
      "step:  317\n",
      "loss: 1.3948\n",
      "a: -0.0013\n",
      "step:  318\n",
      "loss: 1.5317\n",
      "a: -0.0010\n",
      "step:  319\n",
      "loss: 1.4584\n",
      "a: -0.0018\n",
      "step:  320\n",
      "loss: 1.3177\n",
      "a: -0.0030\n",
      "step:  321\n",
      "loss: 1.5132\n",
      "a: -0.0043\n",
      "step:  322\n",
      "loss: 1.4429\n",
      "a: -0.0054\n",
      "step:  323\n",
      "loss: 1.3838\n",
      "a: -0.0065\n",
      "step:  324\n",
      "loss: 1.3738\n",
      "a: -0.0073\n",
      "step:  325\n",
      "loss: 1.3955\n",
      "a: -0.0079\n",
      "step:  326\n",
      "loss: 1.5592\n",
      "a: -0.0084\n",
      "step:  327\n",
      "loss: 1.6626\n",
      "a: -0.0087\n",
      "step:  328\n",
      "loss: 1.6361\n",
      "a: -0.0089\n",
      "step:  329\n",
      "loss: 1.3335\n",
      "a: -0.0086\n",
      "step:  330\n",
      "loss: 1.4324\n",
      "a: -0.0081\n",
      "step:  331\n",
      "loss: 1.3349\n",
      "a: -0.0073\n",
      "step:  332\n",
      "loss: 1.3686\n",
      "a: -0.0063\n",
      "step:  333\n",
      "loss: 1.4735\n",
      "a: -0.0050\n",
      "step:  334\n",
      "loss: 1.2658\n",
      "a: -0.0036\n",
      "step:  335\n",
      "loss: 1.7588\n",
      "a: -0.0023\n",
      "step:  336\n",
      "loss: 1.3266\n",
      "a: -0.0017\n",
      "step:  337\n",
      "loss: 1.5655\n",
      "a: -0.0017\n",
      "step:  338\n",
      "loss: 1.3494\n",
      "a: -0.0024\n",
      "step:  339\n",
      "loss: 1.4804\n",
      "a: -0.0035\n",
      "step:  340\n",
      "loss: 1.5241\n",
      "a: -0.0045\n",
      "step:  341\n",
      "loss: 1.5804\n",
      "a: -0.0054\n",
      "step:  342\n",
      "loss: 1.3673\n",
      "a: -0.0061\n",
      "step:  343\n",
      "loss: 1.5484\n",
      "a: -0.0064\n",
      "step:  344\n",
      "loss: 1.5041\n",
      "a: -0.0066\n",
      "step:  345\n",
      "loss: 1.3817\n",
      "a: -0.0064\n",
      "step:  346\n",
      "loss: 1.3506\n",
      "a: -0.0059\n",
      "step:  347\n",
      "loss: 1.4246\n",
      "a: -0.0050\n",
      "step:  348\n",
      "loss: 1.3054\n",
      "a: -0.0038\n",
      "step:  349\n",
      "loss: 1.3737\n",
      "a: -0.0029\n",
      "step:  350\n",
      "loss: 1.5774\n",
      "a: -0.0025\n",
      "step:  351\n",
      "loss: 1.4159\n",
      "a: -0.0028\n",
      "step:  352\n",
      "loss: 1.4766\n",
      "a: -0.0029\n",
      "step:  353\n",
      "loss: 1.5666\n",
      "a: -0.0026\n",
      "step:  354\n",
      "loss: 1.5964\n",
      "a: -0.0029\n",
      "step:  355\n",
      "loss: 1.5003\n",
      "a: -0.0032\n",
      "step:  356\n",
      "loss: 1.2437\n",
      "a: -0.0035\n",
      "step:  357\n",
      "loss: 1.2803\n",
      "a: -0.0039\n",
      "step:  358\n",
      "loss: 1.5494\n",
      "a: -0.0038\n",
      "step:  359\n",
      "loss: 1.2628\n",
      "a: -0.0041\n",
      "step:  360\n",
      "loss: 1.4170\n",
      "a: -0.0046\n",
      "step:  361\n",
      "loss: 1.4638\n",
      "a: -0.0054\n",
      "step:  362\n",
      "loss: 1.6000\n",
      "a: -0.0060\n",
      "step:  363\n",
      "loss: 1.4271\n",
      "a: -0.0066\n",
      "step:  364\n",
      "loss: 1.3582\n",
      "a: -0.0062\n",
      "step:  365\n",
      "loss: 1.4264\n",
      "a: -0.0050\n",
      "step:  366\n",
      "loss: 1.5278\n",
      "a: -0.0033\n",
      "step:  367\n",
      "loss: 1.3411\n",
      "a: -0.0019\n",
      "step:  368\n",
      "loss: 1.7706\n",
      "a: -0.0013\n",
      "step:  369\n",
      "loss: 1.4430\n",
      "a: -0.0034\n",
      "step:  370\n",
      "loss: 1.4483\n",
      "a: -0.0055\n",
      "step:  371\n",
      "loss: 1.5394\n",
      "a: -0.0072\n",
      "step:  372\n",
      "loss: 1.6347\n",
      "a: -0.0081\n",
      "step:  373\n",
      "loss: 1.4695\n",
      "a: -0.0083\n",
      "step:  374\n",
      "loss: 1.5499\n",
      "a: -0.0080\n",
      "step:  375\n",
      "loss: 1.6720\n",
      "a: -0.0067\n",
      "step:  376\n",
      "loss: 1.1936\n",
      "a: -0.0049\n",
      "step:  377\n",
      "loss: 1.4614\n",
      "a: -0.0032\n",
      "step:  378\n",
      "loss: 1.3751\n",
      "a: -0.0015\n",
      "step:  379\n",
      "loss: 1.8183\n",
      "a: 0.0001\n",
      "step:  380\n",
      "loss: 1.1914\n",
      "a: -0.0024\n",
      "step:  381\n",
      "loss: 1.3997\n",
      "a: -0.0045\n",
      "step:  382\n",
      "loss: 1.3573\n",
      "a: -0.0063\n",
      "step:  383\n",
      "loss: 1.7872\n",
      "a: -0.0079\n",
      "step:  384\n",
      "loss: 1.4352\n",
      "a: -0.0093\n",
      "step:  385\n",
      "loss: 1.5285\n",
      "a: -0.0105\n",
      "step:  386\n",
      "loss: 1.7676\n",
      "a: -0.0115\n",
      "step:  387\n",
      "loss: 1.4668\n",
      "a: -0.0122\n",
      "step:  388\n",
      "loss: 1.8264\n",
      "a: -0.0127\n",
      "step:  389\n",
      "loss: 1.7786\n",
      "a: -0.0129\n",
      "step:  390\n",
      "loss: 1.8431\n",
      "a: -0.0128\n",
      "step:  391\n",
      "loss: 1.8258\n",
      "a: -0.0124\n",
      "step:  392\n",
      "loss: 1.7676\n",
      "a: -0.0116\n",
      "step:  393\n",
      "loss: 1.2490\n",
      "a: -0.0104\n",
      "step:  394\n",
      "loss: 1.5124\n",
      "a: -0.0092\n",
      "step:  395\n",
      "loss: 1.4598\n",
      "a: -0.0079\n",
      "step:  396\n",
      "loss: 1.4803\n",
      "a: -0.0067\n",
      "step:  397\n",
      "loss: 1.5367\n",
      "a: -0.0055\n",
      "step:  398\n",
      "loss: 1.7900\n",
      "a: -0.0044\n",
      "step:  399\n",
      "loss: 1.4096\n",
      "a: -0.0035\n",
      "step:  400\n",
      "loss: 1.3900\n",
      "a: -0.0027\n",
      "step:  401\n",
      "loss: 1.7690\n",
      "a: -0.0022\n",
      "step:  402\n",
      "loss: 1.6105\n",
      "a: -0.0020\n",
      "step:  403\n",
      "loss: 1.6909\n",
      "a: -0.0021\n",
      "step:  404\n",
      "loss: 1.6312\n",
      "a: -0.0027\n",
      "step:  405\n",
      "loss: 1.7770\n",
      "a: -0.0035\n",
      "step:  406\n",
      "loss: 1.5975\n",
      "a: -0.0045\n",
      "step:  407\n",
      "loss: 1.5194\n",
      "a: -0.0056\n",
      "step:  408\n",
      "loss: 1.6813\n",
      "a: -0.0068\n",
      "step:  409\n",
      "loss: 1.4421\n",
      "a: -0.0079\n",
      "step:  410\n",
      "loss: 1.5364\n",
      "a: -0.0090\n",
      "step:  411\n",
      "loss: 1.6065\n",
      "a: -0.0099\n",
      "step:  412\n",
      "loss: 1.4862\n",
      "a: -0.0107\n",
      "step:  413\n",
      "loss: 1.4724\n",
      "a: -0.0114\n",
      "step:  414\n",
      "loss: 1.6989\n",
      "a: -0.0118\n",
      "step:  415\n",
      "loss: 1.8300\n",
      "a: -0.0118\n",
      "step:  416\n",
      "loss: 1.5716\n",
      "a: -0.0115\n",
      "step:  417\n",
      "loss: 1.3857\n",
      "a: -0.0110\n",
      "step:  418\n",
      "loss: 1.6650\n",
      "a: -0.0102\n",
      "step:  419\n",
      "loss: 1.6840\n",
      "a: -0.0092\n",
      "step:  420\n",
      "loss: 1.5038\n",
      "a: -0.0081\n",
      "step:  421\n",
      "loss: 1.6525\n",
      "a: -0.0069\n",
      "step:  422\n",
      "loss: 1.4078\n",
      "a: -0.0056\n",
      "step:  423\n",
      "loss: 1.5590\n",
      "a: -0.0043\n",
      "step:  424\n",
      "loss: 1.3243\n",
      "a: -0.0031\n",
      "step:  425\n",
      "loss: 1.3945\n",
      "a: -0.0018\n",
      "step:  426\n",
      "loss: 1.6448\n",
      "a: -0.0008\n",
      "step:  427\n",
      "loss: 1.9456\n",
      "a: 0.0001\n",
      "step:  428\n",
      "loss: 1.5604\n",
      "a: 0.0003\n",
      "step:  429\n",
      "loss: 1.3839\n",
      "a: -0.0024\n",
      "step:  430\n",
      "loss: 1.5211\n",
      "a: -0.0045\n",
      "step:  431\n",
      "loss: 1.7385\n",
      "a: -0.0064\n",
      "step:  432\n",
      "loss: 1.3746\n",
      "a: -0.0080\n",
      "step:  433\n",
      "loss: 1.5746\n",
      "a: -0.0095\n",
      "step:  434\n",
      "loss: 1.8084\n",
      "a: -0.0109\n",
      "step:  435\n",
      "loss: 1.8677\n",
      "a: -0.0121\n",
      "step:  436\n",
      "loss: 1.7727\n",
      "a: -0.0132\n",
      "step:  437\n",
      "loss: 1.9030\n",
      "a: -0.0141\n",
      "step:  438\n",
      "loss: 2.2071\n",
      "a: -0.0150\n",
      "step:  439\n",
      "loss: 1.9243\n",
      "a: -0.0157\n",
      "step:  440\n",
      "loss: 2.1869\n",
      "a: -0.0162\n",
      "step:  441\n",
      "loss: 2.0041\n",
      "a: -0.0165\n",
      "step:  442\n",
      "loss: 2.3599\n",
      "a: -0.0167\n",
      "step:  443\n",
      "loss: 1.9475\n",
      "a: -0.0167\n",
      "step:  444\n",
      "loss: 1.9053\n",
      "a: -0.0165\n",
      "step:  445\n",
      "loss: 1.7523\n",
      "a: -0.0161\n",
      "step:  446\n",
      "loss: 2.0476\n",
      "a: -0.0155\n",
      "step:  447\n",
      "loss: 2.0135\n",
      "a: -0.0147\n",
      "step:  448\n",
      "loss: 1.8224\n",
      "a: -0.0137\n",
      "step:  449\n",
      "loss: 1.9502\n",
      "a: -0.0126\n",
      "step:  450\n",
      "loss: 1.6249\n",
      "a: -0.0115\n",
      "step:  451\n",
      "loss: 1.4393\n",
      "a: -0.0104\n",
      "step:  452\n",
      "loss: 1.7461\n",
      "a: -0.0092\n",
      "step:  453\n",
      "loss: 1.6359\n",
      "a: -0.0081\n",
      "step:  454\n",
      "loss: 1.6725\n",
      "a: -0.0069\n",
      "step:  455\n",
      "loss: 1.4999\n",
      "a: -0.0058\n",
      "step:  456\n",
      "loss: 1.6318\n",
      "a: -0.0047\n",
      "step:  457\n",
      "loss: 1.3595\n",
      "a: -0.0036\n",
      "step:  458\n",
      "loss: 1.4765\n",
      "a: -0.0026\n",
      "step:  459\n",
      "loss: 1.8408\n",
      "a: -0.0016\n",
      "step:  460\n",
      "loss: 1.6302\n",
      "a: -0.0006\n",
      "step:  461\n",
      "loss: 1.6242\n",
      "a: 0.0002\n",
      "step:  462\n",
      "loss: 1.9497\n",
      "a: 0.0004\n",
      "step:  463\n",
      "loss: 1.5056\n",
      "a: -0.0012\n",
      "step:  464\n",
      "loss: 1.4580\n",
      "a: -0.0025\n",
      "step:  465\n",
      "loss: 1.6800\n",
      "a: -0.0039\n",
      "step:  466\n",
      "loss: 1.5558\n",
      "a: -0.0052\n",
      "step:  467\n",
      "loss: 1.5933\n",
      "a: -0.0065\n",
      "step:  468\n",
      "loss: 1.5844\n",
      "a: -0.0077\n",
      "step:  469\n",
      "loss: 1.5516\n",
      "a: -0.0089\n",
      "step:  470\n",
      "loss: 1.4709\n",
      "a: -0.0100\n",
      "step:  471\n",
      "loss: 1.5399\n",
      "a: -0.0111\n",
      "step:  472\n",
      "loss: 1.5777\n",
      "a: -0.0121\n",
      "step:  473\n",
      "loss: 1.8503\n",
      "a: -0.0130\n",
      "step:  474\n",
      "loss: 1.8718\n",
      "a: -0.0139\n",
      "step:  475\n",
      "loss: 1.9781\n",
      "a: -0.0147\n",
      "step:  476\n",
      "loss: 1.9116\n",
      "a: -0.0153\n",
      "step:  477\n",
      "loss: 2.1566\n",
      "a: -0.0158\n",
      "step:  478\n",
      "loss: 2.1755\n",
      "a: -0.0161\n",
      "step:  479\n",
      "loss: 1.7344\n",
      "a: -0.0162\n",
      "step:  480\n",
      "loss: 2.3249\n",
      "a: -0.0162\n",
      "step:  481\n",
      "loss: 2.1301\n",
      "a: -0.0160\n",
      "step:  482\n",
      "loss: 1.9009\n",
      "a: -0.0156\n",
      "step:  483\n",
      "loss: 2.2466\n",
      "a: -0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  484\n",
      "loss: 1.8223\n",
      "a: -0.0142\n",
      "step:  485\n",
      "loss: 1.6479\n",
      "a: -0.0133\n",
      "step:  486\n",
      "loss: 1.8781\n",
      "a: -0.0122\n",
      "step:  487\n",
      "loss: 1.7053\n",
      "a: -0.0110\n",
      "step:  488\n",
      "loss: 1.6862\n",
      "a: -0.0098\n",
      "step:  489\n",
      "loss: 1.5665\n",
      "a: -0.0085\n",
      "step:  490\n",
      "loss: 1.5441\n",
      "a: -0.0072\n",
      "step:  491\n",
      "loss: 1.5239\n",
      "a: -0.0060\n",
      "step:  492\n",
      "loss: 1.6709\n",
      "a: -0.0048\n",
      "step:  493\n",
      "loss: 1.4837\n",
      "a: -0.0037\n",
      "step:  494\n",
      "loss: 1.5473\n",
      "a: -0.0026\n",
      "step:  495\n",
      "loss: 1.5519\n",
      "a: -0.0015\n",
      "step:  496\n",
      "loss: 1.8529\n",
      "a: -0.0005\n",
      "step:  497\n",
      "loss: 1.7404\n",
      "a: 0.0002\n",
      "step:  498\n",
      "loss: 1.6038\n",
      "a: -0.0001\n",
      "step:  499\n",
      "loss: 1.6841\n",
      "a: -0.0006\n",
      "step:  500\n",
      "loss: 1.5672\n",
      "a: -0.0014\n",
      "step:  501\n",
      "loss: 1.6632\n",
      "a: -0.0022\n",
      "step:  502\n",
      "loss: 1.6321\n",
      "a: -0.0032\n",
      "step:  503\n",
      "loss: 1.4383\n",
      "a: -0.0041\n",
      "step:  504\n",
      "loss: 1.6570\n",
      "a: -0.0051\n",
      "step:  505\n",
      "loss: 1.4947\n",
      "a: -0.0061\n",
      "step:  506\n",
      "loss: 1.6434\n",
      "a: -0.0070\n",
      "step:  507\n",
      "loss: 1.5852\n",
      "a: -0.0079\n",
      "step:  508\n",
      "loss: 1.6863\n",
      "a: -0.0088\n",
      "step:  509\n",
      "loss: 1.5260\n",
      "a: -0.0097\n",
      "step:  510\n",
      "loss: 1.6556\n",
      "a: -0.0104\n",
      "step:  511\n",
      "loss: 1.5281\n",
      "a: -0.0112\n",
      "step:  512\n",
      "loss: 1.5763\n",
      "a: -0.0118\n",
      "step:  513\n",
      "loss: 1.9250\n",
      "a: -0.0124\n",
      "step:  514\n",
      "loss: 1.7211\n",
      "a: -0.0128\n",
      "step:  515\n",
      "loss: 1.5164\n",
      "a: -0.0132\n",
      "step:  516\n",
      "loss: 1.9687\n",
      "a: -0.0133\n",
      "step:  517\n",
      "loss: 1.7745\n",
      "a: -0.0133\n",
      "step:  518\n",
      "loss: 1.6445\n",
      "a: -0.0129\n",
      "step:  519\n",
      "loss: 1.7095\n",
      "a: -0.0125\n",
      "step:  520\n",
      "loss: 1.7216\n",
      "a: -0.0118\n",
      "step:  521\n",
      "loss: 1.7386\n",
      "a: -0.0111\n",
      "step:  522\n",
      "loss: 1.8622\n",
      "a: -0.0101\n",
      "step:  523\n",
      "loss: 1.6694\n",
      "a: -0.0091\n",
      "step:  524\n",
      "loss: 1.6177\n",
      "a: -0.0080\n",
      "step:  525\n",
      "loss: 1.5338\n",
      "a: -0.0069\n",
      "step:  526\n",
      "loss: 1.5866\n",
      "a: -0.0057\n",
      "step:  527\n",
      "loss: 1.6463\n",
      "a: -0.0044\n",
      "step:  528\n",
      "loss: 1.5078\n",
      "a: -0.0032\n",
      "step:  529\n",
      "loss: 1.8282\n",
      "a: -0.0021\n",
      "step:  530\n",
      "loss: 1.5575\n",
      "a: -0.0011\n",
      "step:  531\n",
      "loss: 1.6924\n",
      "a: -0.0002\n",
      "step:  532\n",
      "loss: 2.4011\n",
      "a: 0.0006\n",
      "step:  533\n",
      "loss: 1.6422\n",
      "a: -0.0020\n",
      "step:  534\n",
      "loss: 1.4662\n",
      "a: -0.0041\n",
      "step:  535\n",
      "loss: 1.2930\n",
      "a: -0.0060\n",
      "step:  536\n",
      "loss: 1.5241\n",
      "a: -0.0076\n",
      "step:  537\n",
      "loss: 1.6975\n",
      "a: -0.0091\n",
      "step:  538\n",
      "loss: 1.4229\n",
      "a: -0.0104\n",
      "step:  539\n",
      "loss: 1.7580\n",
      "a: -0.0117\n",
      "step:  540\n",
      "loss: 1.7183\n",
      "a: -0.0129\n",
      "step:  541\n",
      "loss: 2.0004\n",
      "a: -0.0141\n",
      "step:  542\n",
      "loss: 2.1851\n",
      "a: -0.0152\n",
      "step:  543\n",
      "loss: 2.1807\n",
      "a: -0.0162\n",
      "step:  544\n",
      "loss: 2.2652\n",
      "a: -0.0171\n",
      "step:  545\n",
      "loss: 2.1195\n",
      "a: -0.0179\n",
      "step:  546\n",
      "loss: 2.0966\n",
      "a: -0.0187\n",
      "step:  547\n",
      "loss: 2.4934\n",
      "a: -0.0193\n",
      "step:  548\n",
      "loss: 2.6251\n",
      "a: -0.0199\n",
      "step:  549\n",
      "loss: 2.4641\n",
      "a: -0.0203\n",
      "step:  550\n",
      "loss: 2.4123\n",
      "a: -0.0205\n",
      "step:  551\n",
      "loss: 2.4635\n",
      "a: -0.0207\n",
      "step:  552\n",
      "loss: 2.4450\n",
      "a: -0.0207\n",
      "step:  553\n",
      "loss: 2.2607\n",
      "a: -0.0205\n",
      "step:  554\n",
      "loss: 2.2623\n",
      "a: -0.0201\n",
      "step:  555\n",
      "loss: 2.1236\n",
      "a: -0.0196\n",
      "step:  556\n",
      "loss: 1.9308\n",
      "a: -0.0189\n",
      "step:  557\n",
      "loss: 2.1605\n",
      "a: -0.0180\n",
      "step:  558\n",
      "loss: 2.3417\n",
      "a: -0.0170\n",
      "step:  559\n",
      "loss: 1.7416\n",
      "a: -0.0158\n",
      "step:  560\n",
      "loss: 1.9004\n",
      "a: -0.0146\n",
      "step:  561\n",
      "loss: 1.7424\n",
      "a: -0.0133\n",
      "step:  562\n",
      "loss: 1.6182\n",
      "a: -0.0120\n",
      "step:  563\n",
      "loss: 1.4844\n",
      "a: -0.0108\n",
      "step:  564\n",
      "loss: 1.7367\n",
      "a: -0.0095\n",
      "step:  565\n",
      "loss: 1.6736\n",
      "a: -0.0083\n",
      "step:  566\n",
      "loss: 1.8476\n",
      "a: -0.0071\n",
      "step:  567\n",
      "loss: 1.8394\n",
      "a: -0.0060\n",
      "step:  568\n",
      "loss: 1.6672\n",
      "a: -0.0048\n",
      "step:  569\n",
      "loss: 1.8599\n",
      "a: -0.0038\n",
      "step:  570\n",
      "loss: 2.3641\n",
      "a: -0.0028\n",
      "step:  571\n",
      "loss: 1.8664\n",
      "a: -0.0019\n",
      "step:  572\n",
      "loss: 1.6074\n",
      "a: -0.0010\n",
      "step:  573\n",
      "loss: 1.3696\n",
      "a: -0.0002\n",
      "step:  574\n",
      "loss: 2.2987\n",
      "a: 0.0005\n",
      "step:  575\n",
      "loss: 2.0533\n",
      "a: 0.0003\n",
      "step:  576\n",
      "loss: 1.4549\n",
      "a: -0.0008\n",
      "step:  577\n",
      "loss: 1.7564\n",
      "a: -0.0020\n",
      "step:  578\n",
      "loss: 1.6598\n",
      "a: -0.0032\n",
      "step:  579\n",
      "loss: 1.5558\n",
      "a: -0.0044\n",
      "step:  580\n",
      "loss: 1.6511\n",
      "a: -0.0056\n",
      "step:  581\n",
      "loss: 1.5704\n",
      "a: -0.0068\n",
      "step:  582\n",
      "loss: 1.4829\n",
      "a: -0.0080\n",
      "step:  583\n",
      "loss: 1.5268\n",
      "a: -0.0090\n",
      "step:  584\n",
      "loss: 1.5883\n",
      "a: -0.0101\n",
      "step:  585\n",
      "loss: 1.7334\n",
      "a: -0.0111\n",
      "step:  586\n",
      "loss: 2.0150\n",
      "a: -0.0121\n",
      "step:  587\n",
      "loss: 1.7514\n",
      "a: -0.0130\n",
      "step:  588\n",
      "loss: 2.1706\n",
      "a: -0.0139\n",
      "step:  589\n",
      "loss: 1.9335\n",
      "a: -0.0147\n",
      "step:  590\n",
      "loss: 2.0530\n",
      "a: -0.0154\n",
      "step:  591\n",
      "loss: 2.1806\n",
      "a: -0.0160\n",
      "step:  592\n",
      "loss: 2.2779\n",
      "a: -0.0165\n",
      "step:  593\n",
      "loss: 2.2316\n",
      "a: -0.0169\n",
      "step:  594\n",
      "loss: 2.4358\n",
      "a: -0.0172\n",
      "step:  595\n",
      "loss: 2.4354\n",
      "a: -0.0173\n",
      "step:  596\n",
      "loss: 2.0021\n",
      "a: -0.0173\n",
      "step:  597\n",
      "loss: 2.2460\n",
      "a: -0.0171\n",
      "step:  598\n",
      "loss: 1.9036\n",
      "a: -0.0167\n",
      "step:  599\n",
      "loss: 2.0858\n",
      "a: -0.0162\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code snippets to build the convolutional network:\n",
    "\n",
    "```python\n",
    "    from torch . nn . functional import conv2d , max_pool2d\n",
    "    convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "    subsampleing_layer = max_pool_2d ( convolutional_layer , (2 , 2) ) # reduces window 2x2 to 1 pixel\n",
    "    out_layer = dropout ( subsample_layer , p_drop_input )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels = 128 (?)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "activation = 'relu'\n",
    "\n",
    "number_of_output_pixels = 128 \n",
    "print('number of output pixels = 128 (?)')\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([-0.1], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o, a])\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o])\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        output = softmax(pre_softmax)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_loader, test_loader, num_epochs, log_interval):\n",
    "    # run training\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, epoch, log_interval)\n",
    "        test(test_loader)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.385042\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.321151\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.371151\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.351151\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.401151\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.361151\n",
      "\n",
      "Test set: Average loss: 169524.3906, Accuracy: 1028/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.351151\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.321151\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.351151\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.361151\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.321151\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.391151\n",
      "\n",
      "Test set: Average loss: 4954480.0000, Accuracy: 1028/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.341151\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.351151\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.391151\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.341151\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.381151\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 2.311151\n",
      "\n",
      "Test set: Average loss: 36804872.0000, Accuracy: 1028/10000 (10%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.321151\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 2.361151\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 2.371151\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 2.301151\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 2.331151\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 2.321151\n",
      "\n",
      "Test set: Average loss: 153331488.0000, Accuracy: 1028/10000 (10%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.381151\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.321151\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 2.331151\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.381151\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 2.401151\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 2.341151\n",
      "\n",
      "Test set: Average loss: 465278080.0000, Accuracy: 1028/10000 (10%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.381151\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 2.341151\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 2.371151\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 2.301151\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 2.351151\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 2.301151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/robin/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d46b835d8601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-3f284de02341>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(train_loader, test_loader, num_epochs, log_interval)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-0099dcf04934>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, log_interval)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_epochs = 10\n",
    "log_interval = 100\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues:\n",
    " - softmax and cross_entropy applied correctly?\n",
    " - validation score not working correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8749\n",
      "loss: 1.8950\n",
      "loss: 1.8771\n",
      "loss: 1.8343\n",
      "loss: 1.9477\n",
      "loss: 1.9044\n",
      "loss: 1.8572\n",
      "loss: 1.9090\n",
      "loss: 1.8267\n",
      "loss: 1.8636\n",
      "loss: 1.9367\n",
      "loss: 1.9012\n",
      "loss: 1.9369\n",
      "loss: 1.8933\n",
      "loss: 1.8630\n",
      "loss: 1.8968\n",
      "loss: 1.9009\n",
      "loss: 1.8796\n",
      "loss: 1.8150\n",
      "loss: 1.9533\n",
      "loss: 1.8822\n",
      "loss: 1.9460\n",
      "loss: 1.8386\n",
      "loss: 1.9072\n",
      "loss: 1.8292\n",
      "loss: 1.9125\n",
      "loss: 1.8710\n",
      "loss: 1.8571\n",
      "loss: 1.9005\n",
      "loss: 1.8392\n",
      "loss: 1.8970\n",
      "loss: 1.8447\n",
      "loss: 1.8780\n",
      "loss: 1.8806\n",
      "loss: 1.8873\n",
      "loss: 1.8552\n",
      "loss: 1.8339\n",
      "loss: 1.8392\n",
      "loss: 1.8212\n",
      "loss: 1.8735\n",
      "loss: 1.8874\n",
      "loss: 1.8615\n",
      "loss: 1.9124\n",
      "loss: 1.8411\n",
      "loss: 1.8206\n",
      "loss: 1.8935\n",
      "loss: 1.8432\n",
      "loss: 1.9238\n",
      "loss: 1.9352\n",
      "loss: 1.8857\n",
      "loss: 1.8065\n",
      "loss: 1.8790\n",
      "loss: 1.8132\n",
      "loss: 1.8741\n",
      "loss: 1.9009\n",
      "loss: 1.8336\n",
      "loss: 1.8713\n",
      "loss: 1.8421\n",
      "loss: 1.7888\n",
      "loss: 1.7799\n",
      "loss: 1.7837\n",
      "loss: 1.8092\n",
      "loss: 1.8242\n",
      "loss: 1.8140\n",
      "loss: 1.8656\n",
      "loss: 1.8403\n",
      "loss: 1.8153\n",
      "loss: 1.8496\n",
      "loss: 1.8439\n",
      "loss: 1.8918\n",
      "loss: 1.8485\n",
      "loss: 1.8378\n",
      "loss: 1.8581\n",
      "loss: 1.7738\n",
      "loss: 1.8080\n",
      "loss: 1.8133\n",
      "loss: 1.8092\n",
      "loss: 1.8143\n",
      "loss: 1.8490\n",
      "loss: 1.7667\n",
      "loss: 1.7212\n",
      "loss: 1.8062\n",
      "loss: 1.7978\n",
      "loss: 1.7868\n",
      "loss: 1.7989\n",
      "loss: 1.8277\n",
      "loss: 1.7998\n",
      "loss: 1.8413\n",
      "loss: 1.8210\n",
      "loss: 1.8264\n",
      "loss: 1.8087\n",
      "loss: 1.7470\n",
      "loss: 1.7644\n",
      "loss: 1.8610\n",
      "loss: 1.8446\n",
      "loss: 1.7186\n",
      "loss: 1.8349\n",
      "loss: 1.8434\n",
      "loss: 1.8812\n",
      "loss: 1.8365\n",
      "loss: 1.7569\n",
      "loss: 1.8365\n",
      "loss: 1.8170\n",
      "loss: 1.8175\n",
      "loss: 1.8149\n",
      "loss: 1.8459\n",
      "loss: 1.8367\n",
      "loss: 1.8607\n",
      "loss: 1.7893\n",
      "loss: 1.7572\n",
      "loss: 1.8453\n",
      "loss: 1.7716\n",
      "loss: 1.8107\n",
      "loss: 1.7477\n",
      "loss: 1.7281\n",
      "loss: 1.8440\n",
      "loss: 1.8014\n",
      "loss: 1.7704\n",
      "loss: 1.7524\n",
      "loss: 1.7771\n",
      "loss: 1.8088\n",
      "loss: 1.8187\n",
      "loss: 1.7149\n",
      "loss: 1.7950\n",
      "loss: 1.7776\n",
      "loss: 1.7011\n",
      "loss: 1.7546\n",
      "loss: 1.8485\n",
      "loss: 1.7178\n",
      "loss: 1.8066\n",
      "loss: 1.8186\n",
      "loss: 1.8279\n",
      "loss: 1.7982\n",
      "loss: 1.8055\n",
      "loss: 1.8498\n",
      "loss: 1.8045\n",
      "loss: 1.7782\n",
      "loss: 1.8023\n",
      "loss: 1.7961\n",
      "loss: 1.8064\n",
      "loss: 1.7273\n",
      "loss: 1.8281\n",
      "loss: 1.7170\n",
      "loss: 1.7340\n",
      "loss: 1.8307\n",
      "loss: 1.7604\n",
      "loss: 1.8114\n",
      "loss: 1.8612\n",
      "loss: 1.8577\n",
      "loss: 1.7611\n",
      "loss: 1.7591\n",
      "loss: 1.7715\n",
      "loss: 1.7855\n",
      "loss: 1.8118\n",
      "loss: 1.8048\n",
      "loss: 1.8655\n",
      "loss: 1.7945\n",
      "loss: 1.7455\n",
      "loss: 1.8127\n",
      "loss: 1.8343\n",
      "loss: 1.7884\n",
      "loss: 1.7463\n",
      "loss: 1.8200\n",
      "loss: 1.7785\n",
      "loss: 1.8280\n",
      "loss: 1.7588\n",
      "loss: 1.8705\n",
      "loss: 1.7862\n",
      "loss: 1.7775\n",
      "loss: 1.7717\n",
      "loss: 1.8224\n",
      "loss: 1.8200\n",
      "loss: 1.8641\n",
      "loss: 1.8545\n",
      "loss: 1.7639\n",
      "loss: 1.7736\n",
      "loss: 1.8280\n",
      "loss: 1.7653\n",
      "loss: 1.7755\n",
      "loss: 1.8216\n",
      "loss: 1.7840\n",
      "loss: 1.7251\n",
      "loss: 1.7466\n",
      "loss: 1.8783\n",
      "loss: 1.7765\n",
      "loss: 1.7904\n",
      "loss: 1.8268\n",
      "loss: 1.7991\n",
      "loss: 1.7047\n",
      "loss: 1.7155\n",
      "loss: 1.7693\n",
      "loss: 1.8160\n",
      "loss: 1.8804\n",
      "loss: 1.7812\n",
      "loss: 1.8191\n",
      "loss: 1.7796\n",
      "loss: 1.7781\n",
      "loss: 1.8009\n",
      "loss: 1.8103\n",
      "loss: 1.7777\n",
      "loss: 1.7534\n",
      "loss: 1.7748\n",
      "loss: 1.7492\n",
      "loss: 1.7470\n",
      "loss: 1.7840\n",
      "loss: 1.8245\n",
      "loss: 1.7622\n",
      "loss: 1.7846\n",
      "loss: 1.7340\n",
      "loss: 1.7478\n",
      "loss: 1.7943\n",
      "loss: 1.7740\n",
      "loss: 1.8417\n",
      "loss: 1.7471\n",
      "loss: 1.7688\n",
      "loss: 1.7131\n",
      "loss: 1.8301\n",
      "loss: 1.8089\n",
      "loss: 1.7332\n",
      "loss: 1.8105\n",
      "loss: 1.8188\n",
      "loss: 1.8181\n",
      "loss: 1.7643\n",
      "loss: 1.7466\n",
      "loss: 1.8221\n",
      "loss: 1.8020\n",
      "loss: 1.7842\n",
      "loss: 1.8433\n",
      "loss: 1.8490\n",
      "loss: 1.8741\n",
      "loss: 1.8447\n",
      "loss: 1.7906\n",
      "loss: 1.8628\n",
      "loss: 1.8161\n",
      "loss: 1.8070\n",
      "loss: 1.8369\n",
      "loss: 1.8441\n",
      "loss: 1.8109\n",
      "loss: 1.7542\n",
      "loss: 1.7959\n",
      "loss: 1.8048\n",
      "loss: 1.8066\n",
      "loss: 1.7989\n",
      "loss: 1.7936\n",
      "loss: 1.8734\n",
      "loss: 1.8398\n",
      "loss: 1.8219\n",
      "loss: 1.8411\n",
      "loss: 1.7869\n",
      "loss: 1.7899\n",
      "loss: 1.8381\n",
      "loss: 1.7723\n",
      "loss: 1.7982\n",
      "loss: 1.8288\n",
      "loss: 1.7167\n",
      "loss: 1.8450\n",
      "loss: 1.7911\n",
      "loss: 1.7450\n",
      "loss: 1.7723\n",
      "loss: 1.7922\n",
      "loss: 1.8678\n",
      "loss: 1.7755\n",
      "loss: 1.7794\n",
      "loss: 1.6412\n",
      "loss: 1.9281\n",
      "loss: 1.8779\n",
      "loss: 1.7925\n",
      "loss: 1.7298\n",
      "loss: 1.8836\n",
      "loss: 1.8198\n",
      "loss: 1.7939\n",
      "loss: 1.8496\n",
      "loss: 1.7610\n",
      "loss: 1.8913\n",
      "loss: 1.7776\n",
      "loss: 1.8266\n",
      "loss: 1.7586\n",
      "loss: 1.7662\n",
      "loss: 1.8084\n",
      "loss: 1.7702\n",
      "loss: 1.7902\n",
      "loss: 1.8589\n",
      "loss: 1.7464\n",
      "loss: 1.7878\n",
      "loss: 1.8075\n",
      "loss: 1.8407\n",
      "loss: 1.8290\n",
      "loss: 1.7582\n",
      "loss: 1.7793\n",
      "loss: 1.7678\n",
      "loss: 1.8200\n",
      "loss: 1.7218\n",
      "loss: 1.8373\n",
      "loss: 1.8145\n",
      "loss: 1.8507\n",
      "loss: 1.8251\n",
      "loss: 1.9087\n",
      "loss: 1.7484\n",
      "loss: 1.8171\n",
      "loss: 1.8695\n",
      "loss: 1.7410\n",
      "loss: 1.8256\n",
      "loss: 1.8205\n",
      "loss: 1.8125\n",
      "loss: 1.8320\n",
      "loss: 1.8050\n",
      "loss: 1.7816\n",
      "loss: 1.8397\n",
      "loss: 1.7914\n",
      "loss: 1.8188\n",
      "loss: 1.8953\n",
      "loss: 1.8480\n",
      "loss: 1.7792\n",
      "loss: 1.8490\n",
      "loss: 1.8800\n",
      "loss: 1.7964\n",
      "loss: 1.8012\n",
      "loss: 1.7894\n",
      "loss: 1.8295\n",
      "loss: 1.8902\n",
      "loss: 1.8457\n",
      "loss: 1.7190\n",
      "loss: 1.7975\n",
      "loss: 1.9044\n",
      "loss: 1.7509\n",
      "loss: 1.8612\n",
      "loss: 1.7978\n",
      "loss: 1.8799\n",
      "loss: 1.8971\n",
      "loss: 1.7890\n",
      "loss: 1.9192\n",
      "loss: 1.7682\n",
      "loss: 1.8399\n",
      "loss: 1.7142\n",
      "loss: 1.8036\n",
      "loss: 1.8707\n",
      "loss: 1.7872\n",
      "loss: 1.8330\n",
      "loss: 1.8408\n",
      "loss: 1.8175\n",
      "loss: 1.8606\n",
      "loss: 1.7996\n",
      "loss: 1.8416\n",
      "loss: 1.8000\n",
      "loss: 1.8197\n",
      "loss: 1.9092\n",
      "loss: 1.7510\n",
      "loss: 1.9104\n",
      "loss: 1.8390\n",
      "loss: 1.9024\n",
      "loss: 1.8326\n",
      "loss: 1.8876\n",
      "loss: 1.8811\n",
      "loss: 1.8803\n",
      "loss: 1.8247\n",
      "loss: 1.8194\n",
      "loss: 1.8450\n",
      "loss: 1.7881\n",
      "loss: 1.8109\n",
      "loss: 1.7511\n",
      "loss: 1.7579\n",
      "loss: 1.7928\n",
      "loss: 1.8366\n",
      "loss: 1.8197\n",
      "loss: 1.7807\n",
      "loss: 1.8777\n",
      "loss: 1.8579\n",
      "loss: 1.7091\n",
      "loss: 1.7865\n",
      "loss: 1.8069\n",
      "loss: 1.8695\n",
      "loss: 1.8174\n",
      "loss: 1.8359\n",
      "loss: 1.8685\n",
      "loss: 1.8590\n",
      "loss: 1.9053\n",
      "loss: 1.8096\n",
      "loss: 1.7708\n",
      "loss: 1.8523\n",
      "loss: 1.8528\n",
      "loss: 1.8006\n",
      "loss: 1.9262\n",
      "loss: 1.8191\n",
      "loss: 1.7312\n",
      "loss: 1.7474\n",
      "loss: 1.8787\n",
      "loss: 1.9004\n",
      "loss: 1.7030\n",
      "loss: 1.7880\n",
      "loss: 1.7983\n",
      "loss: 1.8116\n",
      "loss: 1.8654\n",
      "loss: 1.8240\n",
      "loss: 1.8907\n",
      "loss: 1.8595\n",
      "loss: 1.7887\n",
      "loss: 1.6936\n",
      "loss: 1.8318\n",
      "loss: 1.7596\n",
      "loss: 1.8079\n",
      "loss: 1.8583\n",
      "loss: 1.7744\n",
      "loss: 1.8237\n",
      "loss: 1.8457\n",
      "loss: 1.8197\n",
      "loss: 1.8240\n",
      "loss: 1.8513\n",
      "loss: 1.7964\n",
      "loss: 1.9070\n",
      "loss: 1.8379\n",
      "loss: 1.8250\n",
      "loss: 1.8903\n",
      "loss: 1.8895\n",
      "loss: 1.7750\n",
      "loss: 1.8647\n",
      "loss: 1.8887\n",
      "loss: 1.8680\n",
      "loss: 1.8611\n",
      "loss: 1.8477\n",
      "loss: 1.8178\n",
      "loss: 1.8308\n",
      "loss: 1.9180\n",
      "loss: 1.9132\n",
      "loss: 1.8377\n",
      "loss: 1.8714\n",
      "loss: 1.8597\n",
      "loss: 1.8395\n",
      "loss: 1.9639\n",
      "loss: 1.8779\n",
      "loss: 1.8013\n",
      "loss: 1.8341\n",
      "loss: 1.8429\n",
      "loss: 1.8681\n",
      "loss: 1.9464\n",
      "loss: 1.8337\n",
      "loss: 1.8956\n",
      "loss: 1.8506\n",
      "loss: 1.8610\n",
      "loss: 1.9570\n",
      "loss: 1.9643\n",
      "loss: 1.9479\n",
      "loss: 1.8247\n",
      "loss: 1.8872\n",
      "loss: 1.8454\n",
      "loss: 1.8677\n",
      "loss: 1.9196\n",
      "loss: 1.8001\n",
      "loss: 1.8962\n",
      "loss: 1.8564\n",
      "loss: 1.9101\n",
      "loss: 1.8372\n",
      "loss: 1.8536\n",
      "loss: 1.8662\n",
      "loss: 1.8796\n",
      "loss: 1.8664\n",
      "loss: 1.8706\n",
      "loss: 1.8926\n",
      "loss: 1.8132\n",
      "loss: 1.7903\n",
      "loss: 1.8482\n",
      "loss: 1.8755\n",
      "loss: 1.9097\n",
      "loss: 1.8250\n",
      "loss: 1.7914\n",
      "loss: 1.8496\n",
      "loss: 1.8396\n",
      "loss: 1.8373\n",
      "loss: 1.8996\n",
      "loss: 1.8618\n",
      "loss: 1.8980\n",
      "loss: 1.8784\n",
      "loss: 1.8960\n",
      "loss: 1.9312\n",
      "loss: 1.8420\n",
      "loss: 1.7614\n",
      "loss: 1.8708\n",
      "loss: 1.8778\n",
      "loss: 1.7880\n",
      "loss: 1.7894\n",
      "loss: 1.7596\n",
      "loss: 1.8789\n",
      "loss: 1.8510\n",
      "loss: 1.8396\n",
      "loss: 1.9093\n",
      "loss: 1.8768\n",
      "loss: 1.9065\n",
      "loss: 1.8188\n",
      "loss: 1.8332\n",
      "loss: 1.7512\n",
      "loss: 1.9112\n",
      "loss: 1.8296\n",
      "loss: 1.8829\n",
      "loss: 1.8580\n",
      "loss: 1.8196\n",
      "loss: 1.8199\n",
      "loss: 1.8412\n",
      "loss: 1.8104\n",
      "loss: 1.8592\n",
      "loss: 1.8213\n",
      "loss: 1.9555\n",
      "loss: 1.8996\n",
      "loss: 1.8192\n",
      "loss: 1.7613\n",
      "loss: 1.8315\n",
      "loss: 1.8493\n",
      "loss: 1.8774\n",
      "loss: 1.7684\n",
      "loss: 1.8280\n",
      "loss: 1.8292\n",
      "loss: 1.7982\n",
      "loss: 1.8405\n",
      "loss: 1.7708\n",
      "loss: 1.8412\n",
      "loss: 1.8245\n",
      "loss: 1.7511\n",
      "loss: 1.8509\n",
      "loss: 1.8581\n",
      "loss: 1.8394\n",
      "loss: 1.8112\n",
      "loss: 1.8312\n",
      "loss: 1.8402\n",
      "loss: 1.8119\n",
      "loss: 1.8212\n",
      "loss: 1.8812\n",
      "loss: 1.8581\n",
      "loss: 1.8996\n",
      "loss: 1.8406\n",
      "loss: 1.7811\n",
      "loss: 1.7894\n",
      "loss: 1.8417\n",
      "loss: 1.8196\n",
      "loss: 1.8609\n",
      "loss: 1.7296\n",
      "loss: 1.8693\n",
      "loss: 1.8561\n",
      "loss: 1.8718\n",
      "loss: 1.8196\n",
      "loss: 1.8211\n",
      "loss: 1.8908\n",
      "loss: 1.8625\n",
      "loss: 1.7848\n",
      "loss: 1.8624\n",
      "loss: 1.8296\n",
      "loss: 1.8513\n",
      "loss: 1.7712\n",
      "loss: 1.8308\n",
      "loss: 1.7712\n",
      "loss: 1.8980\n",
      "loss: 1.8812\n",
      "loss: 1.8695\n",
      "loss: 1.8193\n",
      "loss: 1.8609\n",
      "loss: 1.8211\n",
      "loss: 1.8411\n",
      "loss: 1.7812\n",
      "loss: 1.8776\n",
      "loss: 1.9009\n",
      "loss: 1.7996\n",
      "loss: 1.9612\n",
      "loss: 1.8603\n",
      "loss: 1.9412\n",
      "loss: 1.7711\n",
      "loss: 1.8312\n",
      "loss: 1.9496\n",
      "loss: 1.8211\n",
      "loss: 1.8596\n",
      "loss: 1.8112\n",
      "loss: 1.8097\n",
      "loss: 1.9252\n",
      "loss: 1.8612\n",
      "loss: 1.8093\n",
      "loss: 1.7812\n",
      "loss: 1.8912\n",
      "loss: 1.8612\n",
      "loss: 1.8783\n",
      "loss: 1.9696\n",
      "loss: 1.9312\n",
      "loss: 1.9196\n",
      "loss: 1.8296\n",
      "loss: 1.8294\n",
      "loss: 1.8512\n",
      "loss: 1.9596\n",
      "loss: 1.9238\n",
      "loss: 1.9210\n",
      "loss: 1.9808\n",
      "loss: 1.9493\n",
      "loss: 1.7996\n",
      "loss: 1.8694\n",
      "loss: 1.8712\n",
      "loss: 1.9412\n",
      "loss: 1.9112\n",
      "loss: 1.8593\n",
      "loss: 1.8812\n",
      "loss: 1.8812\n",
      "loss: 1.8812\n",
      "loss: 1.8512\n",
      "loss: 1.8211\n",
      "loss: 1.8312\n",
      "loss: 1.8612\n",
      "loss: 1.9412\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(trainloader, 0):\n",
    "    pre_softmax = cnn(X.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "    output = softmax(pre_softmax)\n",
    "    cost = torch.nn.functional.cross_entropy(output, y)\n",
    "    cost.backward()\n",
    "    print('loss: %.4f' % cost)\n",
    "    #print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_pre(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 1, 1])\n",
      "torch.Size([100, 128])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 10])\n",
      "----------\n",
      "tensor([  -32.1970, -1772.8342,   271.9752,  -784.3312, -1048.0977,\n",
      "          589.0019,  -326.3761,  -128.4423,    18.9680,   281.6931])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "# get output size\n",
    "X = cnn_pre(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.8)\n",
    "print(X.size())\n",
    "# reshape\n",
    "X = X.reshape(mb_size, number_of_output_pixels)\n",
    "print(X.size())\n",
    "X_test = torch.randn((mb_size, 1, 28, 28))\n",
    "pre_soft = cnn(X_test, w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "print(pre_soft.size())\n",
    "# apply softmax\n",
    "print(softmax(pre_soft).size())\n",
    "print('----------')\n",
    "print(pre_soft[0])\n",
    "print(softmax(pre_soft)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for _ in trainloader:\n",
    "    count += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

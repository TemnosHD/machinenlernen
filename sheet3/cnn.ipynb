{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 50 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset in trainset and testset. The trainset consists of 60000 images, the testset of 10000 imgs.\n",
    "\n",
    "trainset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "testset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classnames = [str(i) for i in range(10)]\n",
    "\n",
    "def imshow(img, title=\"\", cmap = \"Greys_r\"): #convert tensor to image\n",
    "    plt.title(title)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    numpyimg = img.numpy()[0]\n",
    "    plt.imshow(numpyimg, cmap = cmap)\n",
    "    \n",
    "\n",
    "def display_10_images_from_dataset(dataset, class_names):\n",
    "    \"\"\"\n",
    "    plots 10 randomly chosen images from a given dataset\n",
    "    \"\"\"\n",
    "    display = [] #holds tuples of image and respective label\n",
    "    for _ in range(10):\n",
    "        index = np.random.randint(0,len(dataset)+1)\n",
    "        display.append(dataset[index])\n",
    "    \n",
    "    nr = 1\n",
    "    fig, axes = plt.subplots(2,5,sharex='col',sharey='row', figsize = (14,9))\n",
    "    for image, label in display:\n",
    "        axes[(nr-1)//5][(nr-1)%5] = plt.subplot(2,5,nr)\n",
    "        plt.title(class_names[label], fontsize = 16)\n",
    "        imshow(image, title = class_names[label])\n",
    "        nr+=1\n",
    "    fig.subplots_adjust(hspace=-0.4)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[0:5]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in fig.axes[1:5]+fig.axes[6:]], visible=False)\n",
    "    plt.show()\n",
    "    plt.savefig(\"previewMNIST.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAFtCAYAAADccl8mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa004ae0668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9fa03b4dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_10_images_from_dataset(testset, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4616\n",
      "Loss: 2.2623\n",
      "Loss: 2.2687\n",
      "Loss: 2.3062\n",
      "Loss: 1.9459\n",
      "Loss: 1.8585\n",
      "Loss: 1.7999\n",
      "Loss: 1.8672\n",
      "Loss: 1.5682\n",
      "Loss: 1.6160\n",
      "Loss: 1.5095\n",
      "Loss: 1.4167\n",
      "Loss: 1.3357\n",
      "Loss: 1.2428\n",
      "Loss: 1.2929\n",
      "Loss: 1.2513\n",
      "Loss: 1.0737\n",
      "Loss: 1.0431\n",
      "Loss: 0.9244\n",
      "Loss: 0.8819\n",
      "Loss: 0.8752\n",
      "Loss: 0.9034\n",
      "Loss: 0.9561\n",
      "Loss: 0.9420\n",
      "Loss: 0.6420\n",
      "Loss: 0.8193\n",
      "Loss: 0.9044\n",
      "Loss: 0.8192\n",
      "Loss: 0.6635\n",
      "Loss: 0.9786\n",
      "Loss: 0.8285\n",
      "Loss: 0.8810\n",
      "Loss: 0.7102\n",
      "Loss: 0.8722\n",
      "Loss: 0.7622\n",
      "Loss: 0.9330\n",
      "Loss: 0.8628\n",
      "Loss: 0.5893\n",
      "Loss: 0.8791\n",
      "Loss: 0.8498\n",
      "Loss: 0.6238\n",
      "Loss: 0.8118\n",
      "Loss: 0.6374\n",
      "Loss: 0.6403\n",
      "Loss: 0.5777\n",
      "Loss: 0.6154\n",
      "Loss: 0.8124\n",
      "Loss: 0.7551\n",
      "Loss: 0.4743\n",
      "Loss: 0.9171\n",
      "Loss: 0.6680\n",
      "Loss: 0.4804\n",
      "Loss: 0.4202\n",
      "Loss: 0.7785\n",
      "Loss: 0.7798\n",
      "Loss: 0.6439\n",
      "Loss: 0.7925\n",
      "Loss: 0.5243\n",
      "Loss: 0.8232\n",
      "Loss: 1.1416\n",
      "Loss: 0.9142\n",
      "Loss: 0.5445\n",
      "Loss: 0.5664\n",
      "Loss: 0.5694\n",
      "Loss: 0.7296\n",
      "Loss: 0.7483\n",
      "Loss: 0.6519\n",
      "Loss: 0.6084\n",
      "Loss: 0.5755\n",
      "Loss: 0.6159\n",
      "Loss: 0.7813\n",
      "Loss: 0.8101\n",
      "Loss: 0.8068\n",
      "Loss: 1.1041\n",
      "Loss: 0.6524\n",
      "Loss: 0.7702\n",
      "Loss: 0.5993\n",
      "Loss: 0.5065\n",
      "Loss: 0.6113\n",
      "Loss: 0.6083\n",
      "Loss: 0.7676\n",
      "Loss: 0.5566\n",
      "Loss: 0.8223\n",
      "Loss: 0.8087\n",
      "Loss: 0.4615\n",
      "Loss: 0.7688\n",
      "Loss: 0.8530\n",
      "Loss: 0.6136\n",
      "Loss: 0.3724\n",
      "Loss: 0.5497\n",
      "Loss: 0.6407\n",
      "Loss: 0.4824\n",
      "Loss: 0.5681\n",
      "Loss: 0.4612\n",
      "Loss: 0.4049\n",
      "Loss: 0.5048\n",
      "Loss: 0.5059\n",
      "Loss: 0.3631\n",
      "Loss: 0.4935\n",
      "Loss: 0.4644\n",
      "Loss: 0.4627\n",
      "Loss: 0.5506\n",
      "Loss: 0.3552\n",
      "Loss: 0.3457\n",
      "Loss: 0.4223\n",
      "Loss: 0.3638\n",
      "Loss: 0.4555\n",
      "Loss: 0.5683\n",
      "Loss: 0.5084\n",
      "Loss: 0.6139\n",
      "Loss: 0.3561\n",
      "Loss: 0.3619\n",
      "Loss: 0.5608\n",
      "Loss: 0.4616\n",
      "Loss: 0.3960\n",
      "Loss: 0.5241\n",
      "Loss: 0.6519\n",
      "Loss: 0.4764\n",
      "Loss: 0.4469\n",
      "Loss: 0.3596\n",
      "Loss: 0.5653\n",
      "Loss: 0.5156\n",
      "Loss: 0.7987\n",
      "Loss: 0.6775\n",
      "Loss: 0.8012\n",
      "Loss: 0.4932\n",
      "Loss: 0.5695\n",
      "Loss: 0.5412\n",
      "Loss: 0.3161\n",
      "Loss: 0.4107\n",
      "Loss: 0.3143\n",
      "Loss: 0.6117\n",
      "Loss: 0.5519\n",
      "Loss: 0.4310\n",
      "Loss: 0.4528\n",
      "Loss: 0.6208\n",
      "Loss: 0.6337\n",
      "Loss: 0.5806\n",
      "Loss: 0.6459\n",
      "Loss: 0.5501\n",
      "Loss: 1.0889\n",
      "Loss: 0.6188\n",
      "Loss: 0.3678\n",
      "Loss: 0.6154\n",
      "Loss: 0.5195\n",
      "Loss: 0.6008\n",
      "Loss: 0.5994\n",
      "Loss: 0.4818\n",
      "Loss: 0.3397\n",
      "Loss: 0.5009\n",
      "Loss: 0.5956\n",
      "Loss: 0.6631\n",
      "Loss: 0.3033\n",
      "Loss: 0.3443\n",
      "Loss: 0.3661\n",
      "Loss: 0.5735\n",
      "Loss: 0.8253\n",
      "Loss: 0.5399\n",
      "Loss: 0.2460\n",
      "Loss: 0.6103\n",
      "Loss: 0.5278\n",
      "Loss: 0.3016\n",
      "Loss: 0.7616\n",
      "Loss: 0.3581\n",
      "Loss: 0.6241\n",
      "Loss: 0.6071\n",
      "Loss: 0.2660\n",
      "Loss: 0.5084\n",
      "Loss: 0.6929\n",
      "Loss: 0.6686\n",
      "Loss: 0.5319\n",
      "Loss: 0.5198\n",
      "Loss: 0.4971\n",
      "Loss: 0.3130\n",
      "Loss: 0.3810\n",
      "Loss: 0.4776\n",
      "Loss: 0.3751\n",
      "Loss: 0.5262\n",
      "Loss: 0.5847\n",
      "Loss: 0.4147\n",
      "Loss: 1.0081\n",
      "Loss: 0.6562\n",
      "Loss: 0.7714\n",
      "Loss: 0.4018\n",
      "Loss: 0.3387\n",
      "Loss: 0.5060\n",
      "Loss: 0.6864\n",
      "Loss: 0.3463\n",
      "Loss: 0.9258\n",
      "Loss: 0.5689\n",
      "Loss: 0.5029\n",
      "Loss: 0.2791\n",
      "Loss: 0.5064\n",
      "Loss: 0.7162\n",
      "Loss: 0.9503\n",
      "Loss: 0.7706\n",
      "Loss: 0.5334\n",
      "Loss: 0.7142\n",
      "Loss: 0.5468\n",
      "Loss: 0.4680\n",
      "Loss: 0.5346\n",
      "Loss: 0.3577\n",
      "Loss: 0.4539\n",
      "Loss: 0.5988\n",
      "Loss: 0.8486\n",
      "Loss: 0.5251\n",
      "Loss: 0.3513\n",
      "Loss: 0.8071\n",
      "Loss: 0.3385\n",
      "Loss: 0.3458\n",
      "Loss: 0.7672\n",
      "Loss: 0.5986\n",
      "Loss: 0.3748\n",
      "Loss: 0.5346\n",
      "Loss: 0.6651\n",
      "Loss: 0.7060\n",
      "Loss: 0.3089\n",
      "Loss: 0.3123\n",
      "Loss: 0.4823\n",
      "Loss: 0.5656\n",
      "Loss: 0.5826\n",
      "Loss: 0.6617\n",
      "Loss: 0.7189\n",
      "Loss: 0.5287\n",
      "Loss: 1.0457\n",
      "Loss: 0.5443\n",
      "Loss: 0.5592\n",
      "Loss: 0.4268\n",
      "Loss: 0.5496\n",
      "Loss: 0.8281\n",
      "Loss: 0.2701\n",
      "Loss: 0.4742\n",
      "Loss: 0.7734\n",
      "Loss: 0.5355\n",
      "Loss: 0.5746\n",
      "Loss: 0.5162\n",
      "Loss: 0.5794\n",
      "Loss: 0.4154\n",
      "Loss: 0.5395\n",
      "Loss: 0.4341\n",
      "Loss: 0.3951\n",
      "Loss: 0.7457\n",
      "Loss: 0.5920\n",
      "Loss: 0.5922\n",
      "Loss: 0.5333\n",
      "Loss: 0.3329\n",
      "Loss: 0.9013\n",
      "Loss: 0.3802\n",
      "Loss: 0.5442\n",
      "Loss: 0.4862\n",
      "Loss: 0.4633\n",
      "Loss: 0.5156\n",
      "Loss: 0.7939\n",
      "Loss: 0.7227\n",
      "Loss: 0.4730\n",
      "Loss: 0.3706\n",
      "Loss: 0.6891\n",
      "Loss: 0.4992\n",
      "Loss: 0.5920\n",
      "Loss: 0.4109\n",
      "Loss: 0.5846\n",
      "Loss: 0.4111\n",
      "Loss: 0.4905\n",
      "Loss: 0.4662\n",
      "Loss: 0.4490\n",
      "Loss: 0.3806\n",
      "Loss: 0.3043\n",
      "Loss: 0.2153\n",
      "Loss: 0.4967\n",
      "Loss: 0.6346\n",
      "Loss: 0.4570\n",
      "Loss: 0.6542\n",
      "Loss: 0.4453\n",
      "Loss: 0.4115\n",
      "Loss: 0.3210\n",
      "Loss: 0.3969\n",
      "Loss: 0.4671\n",
      "Loss: 0.4463\n",
      "Loss: 0.2722\n",
      "Loss: 0.6704\n",
      "Loss: 0.5603\n",
      "Loss: 0.4002\n",
      "Loss: 0.6814\n",
      "Loss: 0.3246\n",
      "Loss: 0.3658\n",
      "Loss: 0.4499\n",
      "Loss: 0.6866\n",
      "Loss: 0.5176\n",
      "Loss: 0.5008\n",
      "Loss: 0.3147\n",
      "Loss: 0.5580\n",
      "Loss: 0.5748\n",
      "Loss: 0.3781\n",
      "Loss: 0.2691\n",
      "Loss: 0.5608\n",
      "Loss: 0.4585\n",
      "Loss: 0.6194\n",
      "Loss: 0.3689\n",
      "Loss: 0.4253\n",
      "Loss: 0.3593\n",
      "Loss: 0.6162\n",
      "Loss: 0.3170\n",
      "Loss: 0.5060\n",
      "Loss: 0.4654\n",
      "Loss: 0.4376\n",
      "Loss: 0.8769\n",
      "Loss: 0.3793\n",
      "Loss: 0.3859\n",
      "Loss: 0.5717\n",
      "Loss: 0.3544\n",
      "Loss: 0.6459\n",
      "Loss: 0.4938\n",
      "Loss: 0.2960\n",
      "Loss: 0.4165\n",
      "Loss: 0.3877\n",
      "Loss: 0.3235\n",
      "Loss: 0.5462\n",
      "Loss: 0.5003\n",
      "Loss: 0.3738\n",
      "Loss: 0.4763\n",
      "Loss: 0.3834\n",
      "Loss: 0.5366\n",
      "Loss: 0.4890\n",
      "Loss: 0.5048\n",
      "Loss: 0.3665\n",
      "Loss: 0.5615\n",
      "Loss: 0.4951\n",
      "Loss: 0.3332\n",
      "Loss: 0.5571\n",
      "Loss: 0.3276\n",
      "Loss: 0.4268\n",
      "Loss: 0.5432\n",
      "Loss: 0.3775\n",
      "Loss: 0.4099\n",
      "Loss: 0.5498\n",
      "Loss: 0.5800\n",
      "Loss: 0.5582\n",
      "Loss: 0.3626\n",
      "Loss: 0.5827\n",
      "Loss: 0.3552\n",
      "Loss: 0.7910\n",
      "Loss: 0.4607\n",
      "Loss: 0.4372\n",
      "Loss: 0.2572\n",
      "Loss: 0.6701\n",
      "Loss: 0.5336\n",
      "Loss: 0.6299\n",
      "Loss: 0.2954\n",
      "Loss: 0.4850\n",
      "Loss: 0.4365\n",
      "Loss: 0.3882\n",
      "Loss: 0.4287\n",
      "Loss: 0.5315\n",
      "Loss: 0.6359\n",
      "Loss: 0.4263\n",
      "Loss: 0.4065\n",
      "Loss: 0.3427\n",
      "Loss: 0.5349\n",
      "Loss: 0.3398\n",
      "Loss: 0.4635\n",
      "Loss: 0.5358\n",
      "Loss: 0.4481\n",
      "Loss: 0.3800\n",
      "Loss: 0.5976\n",
      "Loss: 0.4899\n",
      "Loss: 0.4744\n",
      "Loss: 0.3063\n",
      "Loss: 0.5163\n",
      "Loss: 0.4555\n",
      "Loss: 0.5230\n",
      "Loss: 0.3794\n",
      "Loss: 0.7661\n",
      "Loss: 0.4034\n",
      "Loss: 0.4500\n",
      "Loss: 0.4440\n",
      "Loss: 0.4394\n",
      "Loss: 0.4078\n",
      "Loss: 0.5002\n",
      "Loss: 0.3511\n",
      "Loss: 0.4337\n",
      "Loss: 0.2991\n",
      "Loss: 0.7771\n",
      "Loss: 0.3169\n",
      "Loss: 0.2788\n",
      "Loss: 0.7220\n",
      "Loss: 0.2496\n",
      "Loss: 0.7759\n",
      "Loss: 0.4751\n",
      "Loss: 0.4411\n",
      "Loss: 0.4566\n",
      "Loss: 0.4143\n",
      "Loss: 0.4321\n",
      "Loss: 0.4727\n",
      "Loss: 0.5029\n",
      "Loss: 0.5534\n",
      "Loss: 0.3229\n",
      "Loss: 0.5087\n",
      "Loss: 0.4032\n",
      "Loss: 0.3273\n",
      "Loss: 0.3290\n",
      "Loss: 0.3601\n",
      "Loss: 0.4091\n",
      "Loss: 0.3301\n",
      "Loss: 0.5504\n",
      "Loss: 0.3214\n",
      "Loss: 0.4117\n",
      "Loss: 0.4683\n",
      "Loss: 0.5451\n",
      "Loss: 0.5662\n",
      "Loss: 0.4219\n",
      "Loss: 0.4620\n",
      "Loss: 0.4889\n",
      "Loss: 0.4099\n",
      "Loss: 0.3905\n",
      "Loss: 0.2928\n",
      "Loss: 0.2827\n",
      "Loss: 0.4515\n",
      "Loss: 0.3770\n",
      "Loss: 0.7293\n",
      "Loss: 0.6723\n",
      "Loss: 0.5357\n",
      "Loss: 0.4687\n",
      "Loss: 0.7209\n",
      "Loss: 0.4211\n",
      "Loss: 0.5916\n",
      "Loss: 0.3764\n",
      "Loss: 0.6307\n",
      "Loss: 0.4605\n",
      "Loss: 0.5263\n",
      "Loss: 0.3403\n",
      "Loss: 0.5405\n",
      "Loss: 0.2212\n",
      "Loss: 0.2955\n",
      "Loss: 0.5630\n",
      "Loss: 0.5846\n",
      "Loss: 0.4816\n",
      "Loss: 0.8059\n",
      "Loss: 0.4175\n",
      "Loss: 0.4718\n",
      "Loss: 0.4672\n",
      "Loss: 0.3800\n",
      "Loss: 0.5999\n",
      "Loss: 0.5009\n",
      "Loss: 0.4884\n",
      "Loss: 0.4238\n",
      "Loss: 0.3324\n",
      "Loss: 0.2835\n",
      "Loss: 0.2854\n",
      "Loss: 0.3697\n",
      "Loss: 0.2945\n",
      "Loss: 0.4416\n",
      "Loss: 0.3408\n",
      "Loss: 0.2571\n",
      "Loss: 0.3184\n",
      "Loss: 0.5421\n",
      "Loss: 0.3616\n",
      "Loss: 0.4156\n",
      "Loss: 0.3264\n",
      "Loss: 0.5469\n",
      "Loss: 0.3640\n",
      "Loss: 0.1283\n",
      "Loss: 0.2844\n",
      "Loss: 0.3389\n",
      "Loss: 0.6840\n",
      "Loss: 0.3857\n",
      "Loss: 0.3060\n",
      "Loss: 0.4610\n",
      "Loss: 0.4309\n",
      "Loss: 0.3358\n",
      "Loss: 0.3594\n",
      "Loss: 0.2930\n",
      "Loss: 0.2050\n",
      "Loss: 0.6327\n",
      "Loss: 0.3841\n",
      "Loss: 0.3991\n",
      "Loss: 0.6249\n",
      "Loss: 0.7920\n",
      "Loss: 0.8226\n",
      "Loss: 0.2881\n",
      "Loss: 0.4108\n",
      "Loss: 0.2964\n",
      "Loss: 0.4324\n",
      "Loss: 0.7763\n",
      "Loss: 0.5533\n",
      "Loss: 0.5627\n",
      "Loss: 0.2879\n",
      "Loss: 0.3243\n",
      "Loss: 0.3827\n",
      "Loss: 0.2166\n",
      "Loss: 0.2598\n",
      "Loss: 0.3612\n",
      "Loss: 0.4857\n",
      "Loss: 0.6551\n",
      "Loss: 0.3499\n",
      "Loss: 0.3770\n",
      "Loss: 0.4011\n",
      "Loss: 0.3225\n",
      "Loss: 0.6388\n",
      "Loss: 0.2935\n",
      "Loss: 0.2533\n",
      "Loss: 0.5639\n",
      "Loss: 0.6246\n",
      "Loss: 0.5892\n",
      "Loss: 0.4082\n",
      "Loss: 0.7108\n",
      "Loss: 0.4996\n",
      "Loss: 0.4141\n",
      "Loss: 0.3437\n",
      "Loss: 0.3003\n",
      "Loss: 0.2630\n",
      "Loss: 0.2989\n",
      "Loss: 0.4879\n",
      "Loss: 0.4748\n",
      "Loss: 0.2661\n",
      "Loss: 0.2179\n",
      "Loss: 0.5489\n",
      "Loss: 0.6702\n",
      "Loss: 0.4912\n",
      "Loss: 0.5852\n",
      "Loss: 0.5248\n",
      "Loss: 0.4798\n",
      "Loss: 0.3958\n",
      "Loss: 0.4397\n",
      "Loss: 0.2556\n",
      "Loss: 0.6624\n",
      "Loss: 0.3231\n",
      "Loss: 0.3485\n",
      "Loss: 0.4423\n",
      "Loss: 0.3493\n",
      "Loss: 0.3847\n",
      "Loss: 0.5707\n",
      "Loss: 0.4374\n",
      "Loss: 0.6499\n",
      "Loss: 0.4231\n",
      "Loss: 0.3916\n",
      "Loss: 0.2718\n",
      "Loss: 0.3710\n",
      "Loss: 0.3987\n",
      "Loss: 0.6048\n",
      "Loss: 0.2581\n",
      "Loss: 0.3456\n",
      "Loss: 0.3289\n",
      "Loss: 0.3561\n",
      "Loss: 0.3654\n",
      "Loss: 0.5201\n",
      "Loss: 0.2678\n",
      "Loss: 0.3805\n",
      "Loss: 0.1776\n",
      "Loss: 0.5063\n",
      "Loss: 0.3214\n",
      "Loss: 0.3035\n",
      "Loss: 0.4356\n",
      "Loss: 0.6392\n",
      "Loss: 0.5581\n",
      "Loss: 0.4488\n",
      "Loss: 0.3308\n",
      "Loss: 0.5436\n",
      "Loss: 0.4916\n",
      "Loss: 0.8371\n",
      "Loss: 0.4399\n",
      "Loss: 0.7166\n",
      "Loss: 0.3481\n",
      "Loss: 0.5592\n",
      "Loss: 0.1957\n",
      "Loss: 0.6562\n",
      "Loss: 0.2271\n",
      "Loss: 0.3201\n",
      "Loss: 0.4854\n",
      "Loss: 0.5390\n",
      "Loss: 0.3106\n",
      "Loss: 0.6640\n",
      "Loss: 0.4056\n",
      "Loss: 0.4467\n",
      "Loss: 0.4425\n",
      "Loss: 0.3170\n",
      "Loss: 0.4059\n",
      "Loss: 0.2320\n",
      "Loss: 0.3860\n",
      "Loss: 0.4788\n",
      "Loss: 0.4850\n",
      "Loss: 0.3389\n",
      "Loss: 0.4528\n",
      "Loss: 0.4506\n",
      "Loss: 0.4980\n",
      "Loss: 0.3779\n",
      "Loss: 0.7441\n",
      "Loss: 0.6545\n",
      "Loss: 0.3650\n",
      "Loss: 0.2083\n",
      "Loss: 0.5279\n",
      "Loss: 0.5429\n",
      "Loss: 0.2747\n",
      "Loss: 0.5620\n",
      "Loss: 0.4168\n",
      "Loss: 0.4494\n",
      "Loss: 0.4217\n",
      "Loss: 0.2789\n",
      "Loss: 0.3679\n",
      "Loss: 0.4033\n",
      "Loss: 0.3569\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print('Loss: %.4f' % cost)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.235685110092163\n",
      "Loss: 2.6122305393218994\n",
      "Loss: 2.633882761001587\n",
      "Loss: 2.463536262512207\n",
      "Loss: 2.253995418548584\n",
      "Loss: 2.227691173553467\n",
      "Loss: 2.2037668228149414\n",
      "Loss: 2.2376036643981934\n",
      "Loss: 2.2737526893615723\n",
      "Loss: 2.1525075435638428\n",
      "Loss: 2.1420462131500244\n",
      "Loss: 2.104735851287842\n",
      "Loss: 2.2754225730895996\n",
      "Loss: 2.1633718013763428\n",
      "Loss: 2.124685525894165\n",
      "Loss: 2.085428237915039\n",
      "Loss: 2.080772876739502\n",
      "Loss: 1.9635969400405884\n",
      "Loss: 1.9688693284988403\n",
      "Loss: 2.071608543395996\n",
      "Loss: 1.9973366260528564\n",
      "Loss: 1.8948277235031128\n",
      "Loss: 1.8804954290390015\n",
      "Loss: 1.899318814277649\n",
      "Loss: 1.8611576557159424\n",
      "Loss: 1.9064974784851074\n",
      "Loss: 1.904478907585144\n",
      "Loss: 1.9299267530441284\n",
      "Loss: 1.7353285551071167\n",
      "Loss: 1.746910572052002\n",
      "Loss: 1.9866044521331787\n",
      "Loss: 1.872605323791504\n",
      "Loss: 1.9907395839691162\n",
      "Loss: 1.8297711610794067\n",
      "Loss: 1.760430932044983\n",
      "Loss: 1.9318454265594482\n",
      "Loss: 1.8175140619277954\n",
      "Loss: 1.8376332521438599\n",
      "Loss: 1.6989854574203491\n",
      "Loss: 1.582572340965271\n",
      "Loss: 1.8445755243301392\n",
      "Loss: 1.7279452085494995\n",
      "Loss: 1.8538498878479004\n",
      "Loss: 1.6030325889587402\n",
      "Loss: 1.6972392797470093\n",
      "Loss: 1.833166480064392\n",
      "Loss: 1.6762057542800903\n",
      "Loss: 1.7540795803070068\n",
      "Loss: 1.7157599925994873\n",
      "Loss: 1.5768641233444214\n",
      "Loss: 1.4103089570999146\n",
      "Loss: 1.5442253351211548\n",
      "Loss: 1.577795386314392\n",
      "Loss: 1.6203781366348267\n",
      "Loss: 1.6437221765518188\n",
      "Loss: 1.670914888381958\n",
      "Loss: 1.6315300464630127\n",
      "Loss: 1.7126332521438599\n",
      "Loss: 1.8169559240341187\n",
      "Loss: 1.4232901334762573\n",
      "Loss: 1.5889582633972168\n",
      "Loss: 1.4107666015625\n",
      "Loss: 1.5896573066711426\n",
      "Loss: 1.4864424467086792\n",
      "Loss: 1.431215524673462\n",
      "Loss: 1.6170637607574463\n",
      "Loss: 1.5263373851776123\n",
      "Loss: 1.5201152563095093\n",
      "Loss: 1.66840398311615\n",
      "Loss: 1.4270063638687134\n",
      "Loss: 1.4221171140670776\n",
      "Loss: 1.619748830795288\n",
      "Loss: 1.5797163248062134\n",
      "Loss: 1.518632411956787\n",
      "Loss: 1.5847445726394653\n",
      "Loss: 1.669315218925476\n",
      "Loss: 1.4403001070022583\n",
      "Loss: 1.5390825271606445\n",
      "Loss: 1.648163914680481\n",
      "Loss: 1.5826245546340942\n",
      "Loss: 1.3668900728225708\n",
      "Loss: 1.497296929359436\n",
      "Loss: 1.6096783876419067\n",
      "Loss: 1.7891534566879272\n",
      "Loss: 1.590016484260559\n",
      "Loss: 1.3957737684249878\n",
      "Loss: 1.5452258586883545\n",
      "Loss: 1.5920830965042114\n",
      "Loss: 1.62259840965271\n",
      "Loss: 1.413475751876831\n",
      "Loss: 1.4523564577102661\n",
      "Loss: 1.3758866786956787\n",
      "Loss: 1.5771470069885254\n",
      "Loss: 1.4463021755218506\n",
      "Loss: 1.3915131092071533\n",
      "Loss: 1.313422679901123\n",
      "Loss: 1.5220309495925903\n",
      "Loss: 1.5391067266464233\n",
      "Loss: 1.5877798795700073\n",
      "Loss: 1.497165322303772\n",
      "Loss: 1.3383429050445557\n",
      "Loss: 1.379634976387024\n",
      "Loss: 1.3977731466293335\n",
      "Loss: 1.4110832214355469\n",
      "Loss: 1.4959138631820679\n",
      "Loss: 1.4388912916183472\n",
      "Loss: 1.1972342729568481\n",
      "Loss: 1.4660602807998657\n",
      "Loss: 1.4974255561828613\n",
      "Loss: 1.2635573148727417\n",
      "Loss: 1.6518816947937012\n",
      "Loss: 1.2052518129348755\n",
      "Loss: 1.3269137144088745\n",
      "Loss: 1.4603450298309326\n",
      "Loss: 1.7873073816299438\n",
      "Loss: 1.3309401273727417\n",
      "Loss: 1.2543457746505737\n",
      "Loss: 1.335268497467041\n",
      "Loss: 1.3518387079238892\n",
      "Loss: 1.397591233253479\n",
      "Loss: 1.6119184494018555\n",
      "Loss: 1.6606438159942627\n",
      "Loss: 1.3041276931762695\n",
      "Loss: 1.3731377124786377\n",
      "Loss: 1.409706711769104\n",
      "Loss: 1.4399464130401611\n",
      "Loss: 1.3342857360839844\n",
      "Loss: 1.457720160484314\n",
      "Loss: 1.2296181917190552\n",
      "Loss: 1.4223198890686035\n",
      "Loss: 1.383384108543396\n",
      "Loss: 1.2598140239715576\n",
      "Loss: 1.2187786102294922\n",
      "Loss: 1.2607779502868652\n",
      "Loss: 1.0820035934448242\n",
      "Loss: 1.6282519102096558\n",
      "Loss: 1.3300622701644897\n",
      "Loss: 1.098014235496521\n",
      "Loss: 1.4603869915008545\n",
      "Loss: 1.431526780128479\n",
      "Loss: 1.1109938621520996\n",
      "Loss: 1.2632489204406738\n",
      "Loss: 1.4098953008651733\n",
      "Loss: 1.1122719049453735\n",
      "Loss: 1.2248601913452148\n",
      "Loss: 1.6237895488739014\n",
      "Loss: 1.763085126876831\n",
      "Loss: 1.632572889328003\n",
      "Loss: 1.3014280796051025\n",
      "Loss: 1.3012524843215942\n",
      "Loss: 1.250795841217041\n",
      "Loss: 1.4027079343795776\n",
      "Loss: 1.3468444347381592\n",
      "Loss: 1.5457476377487183\n",
      "Loss: 1.277178168296814\n",
      "Loss: 1.196873664855957\n",
      "Loss: 1.2334718704223633\n",
      "Loss: 1.0899207592010498\n",
      "Loss: 1.5114836692810059\n",
      "Loss: 1.340662956237793\n",
      "Loss: 1.253896951675415\n",
      "Loss: 1.2866239547729492\n",
      "Loss: 1.342124581336975\n",
      "Loss: 1.324912428855896\n",
      "Loss: 1.2508654594421387\n",
      "Loss: 1.2259007692337036\n",
      "Loss: 1.2639505863189697\n",
      "Loss: 1.392605185508728\n",
      "Loss: 1.2682194709777832\n",
      "Loss: 1.2202367782592773\n",
      "Loss: 1.1189669370651245\n",
      "Loss: 1.4634824991226196\n",
      "Loss: 1.1182035207748413\n",
      "Loss: 1.617924451828003\n",
      "Loss: 1.312849998474121\n",
      "Loss: 1.7567520141601562\n",
      "Loss: 1.2642279863357544\n",
      "Loss: 1.0016878843307495\n",
      "Loss: 1.278499722480774\n",
      "Loss: 1.3395122289657593\n",
      "Loss: 1.0473235845565796\n",
      "Loss: 1.303757667541504\n",
      "Loss: 1.0364866256713867\n",
      "Loss: 1.3158003091812134\n",
      "Loss: 1.0334320068359375\n",
      "Loss: 1.3454898595809937\n",
      "Loss: 1.610535740852356\n",
      "Loss: 1.117316484451294\n",
      "Loss: 1.3290739059448242\n",
      "Loss: 1.1589869260787964\n",
      "Loss: 1.2751556634902954\n",
      "Loss: 1.118372917175293\n",
      "Loss: 1.0789942741394043\n",
      "Loss: 1.2052178382873535\n",
      "Loss: 1.25667142868042\n",
      "Loss: 1.087365746498108\n",
      "Loss: 1.26752507686615\n",
      "Loss: 1.3001240491867065\n",
      "Loss: 1.2131319046020508\n",
      "Loss: 1.383064866065979\n",
      "Loss: 1.0269824266433716\n",
      "Loss: 1.267236351966858\n",
      "Loss: 1.2051447629928589\n",
      "Loss: 1.1581027507781982\n",
      "Loss: 1.2143549919128418\n",
      "Loss: 1.4053155183792114\n",
      "Loss: 1.2736320495605469\n",
      "Loss: 1.4560455083847046\n",
      "Loss: 1.3637226819992065\n",
      "Loss: 1.280386209487915\n",
      "Loss: 0.9875839352607727\n",
      "Loss: 1.3149687051773071\n",
      "Loss: 1.1509945392608643\n",
      "Loss: 1.4508002996444702\n",
      "Loss: 1.1669307947158813\n",
      "Loss: 1.1835060119628906\n",
      "Loss: 1.4548437595367432\n",
      "Loss: 1.2214583158493042\n",
      "Loss: 1.1926418542861938\n",
      "Loss: 1.3775122165679932\n",
      "Loss: 1.3205671310424805\n",
      "Loss: 1.2841286659240723\n",
      "Loss: 1.219007968902588\n",
      "Loss: 1.1335276365280151\n",
      "Loss: 1.2119208574295044\n",
      "Loss: 1.2257529497146606\n",
      "Loss: 1.2654953002929688\n",
      "Loss: 1.1596870422363281\n",
      "Loss: 0.9888041615486145\n",
      "Loss: 1.4068117141723633\n",
      "Loss: 1.2808972597122192\n",
      "Loss: 1.1286488771438599\n",
      "Loss: 1.3028886318206787\n",
      "Loss: 1.3703080415725708\n",
      "Loss: 1.261032223701477\n",
      "Loss: 1.4445573091506958\n",
      "Loss: 1.1982226371765137\n",
      "Loss: 1.2251136302947998\n",
      "Loss: 1.4952645301818848\n",
      "Loss: 0.9675335884094238\n",
      "Loss: 1.4960349798202515\n",
      "Loss: 1.3032640218734741\n",
      "Loss: 1.3615292310714722\n",
      "Loss: 1.1314189434051514\n",
      "Loss: 1.0800020694732666\n",
      "Loss: 1.1606308221817017\n",
      "Loss: 1.2875251770019531\n",
      "Loss: 0.9796814918518066\n",
      "Loss: 1.0468345880508423\n",
      "Loss: 1.3220371007919312\n",
      "Loss: 1.2171556949615479\n",
      "Loss: 1.4108604192733765\n",
      "Loss: 1.0667273998260498\n",
      "Loss: 1.2724387645721436\n",
      "Loss: 0.9733000993728638\n",
      "Loss: 1.1657841205596924\n",
      "Loss: 1.2085964679718018\n",
      "Loss: 1.4145327806472778\n",
      "Loss: 1.0316940546035767\n",
      "Loss: 1.2020612955093384\n",
      "Loss: 1.4044190645217896\n",
      "Loss: 1.2166486978530884\n",
      "Loss: 1.3140478134155273\n",
      "Loss: 1.1699028015136719\n",
      "Loss: 1.0640208721160889\n",
      "Loss: 1.1109665632247925\n",
      "Loss: 1.282559871673584\n",
      "Loss: 1.13416588306427\n",
      "Loss: 1.378733515739441\n",
      "Loss: 1.3009631633758545\n",
      "Loss: 1.1336040496826172\n",
      "Loss: 1.117401361465454\n",
      "Loss: 0.9894036054611206\n",
      "Loss: 1.2349382638931274\n",
      "Loss: 1.293837547302246\n",
      "Loss: 1.0066263675689697\n",
      "Loss: 1.144315481185913\n",
      "Loss: 1.2892123460769653\n",
      "Loss: 1.3642756938934326\n",
      "Loss: 1.1503819227218628\n",
      "Loss: 1.238848090171814\n",
      "Loss: 1.2947592735290527\n",
      "Loss: 1.2793198823928833\n",
      "Loss: 1.3867994546890259\n",
      "Loss: 1.1846293210983276\n",
      "Loss: 1.3020211458206177\n",
      "Loss: 1.2613575458526611\n",
      "Loss: 1.4629641771316528\n",
      "Loss: 1.1246036291122437\n",
      "Loss: 1.4205130338668823\n",
      "Loss: 1.4473727941513062\n",
      "Loss: 1.1376562118530273\n",
      "Loss: 1.229251503944397\n",
      "Loss: 1.0496493577957153\n",
      "Loss: 1.1700372695922852\n",
      "Loss: 1.2896748781204224\n",
      "Loss: 1.1218352317810059\n",
      "Loss: 1.2222869396209717\n",
      "Loss: 1.0251193046569824\n",
      "Loss: 1.4456483125686646\n",
      "Loss: 1.1176317930221558\n",
      "Loss: 1.35329270362854\n",
      "Loss: 1.3475489616394043\n",
      "Loss: 1.1959071159362793\n",
      "Loss: 1.2581703662872314\n",
      "Loss: 1.0993905067443848\n",
      "Loss: 0.9449941515922546\n",
      "Loss: 1.2014377117156982\n",
      "Loss: 1.462572455406189\n",
      "Loss: 1.3420929908752441\n",
      "Loss: 1.0339393615722656\n",
      "Loss: 1.534349799156189\n",
      "Loss: 1.1335688829421997\n",
      "Loss: 1.3005404472351074\n",
      "Loss: 1.113824486732483\n",
      "Loss: 1.1736772060394287\n",
      "Loss: 1.0841693878173828\n",
      "Loss: 1.008832573890686\n",
      "Loss: 1.2442015409469604\n",
      "Loss: 1.378468632698059\n",
      "Loss: 1.2424261569976807\n",
      "Loss: 1.1010956764221191\n",
      "Loss: 1.2528785467147827\n",
      "Loss: 1.4006870985031128\n",
      "Loss: 1.3451794385910034\n",
      "Loss: 1.1506346464157104\n",
      "Loss: 1.104378581047058\n",
      "Loss: 1.1331069469451904\n",
      "Loss: 1.5056166648864746\n",
      "Loss: 1.4190624952316284\n",
      "Loss: 1.3183910846710205\n",
      "Loss: 1.2393198013305664\n",
      "Loss: 1.3059238195419312\n",
      "Loss: 1.1074869632720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2563494443893433\n",
      "Loss: 1.403342843055725\n",
      "Loss: 1.1448407173156738\n",
      "Loss: 1.2700246572494507\n",
      "Loss: 1.197383999824524\n",
      "Loss: 1.1524964570999146\n",
      "Loss: 1.1833837032318115\n",
      "Loss: 1.2829405069351196\n",
      "Loss: 1.2575676441192627\n",
      "Loss: 1.3691112995147705\n",
      "Loss: 1.1953160762786865\n",
      "Loss: 0.9843840003013611\n",
      "Loss: 1.049376130104065\n",
      "Loss: 1.2238192558288574\n",
      "Loss: 1.0818769931793213\n",
      "Loss: 1.221265435218811\n",
      "Loss: 1.296981692314148\n",
      "Loss: 1.2105437517166138\n",
      "Loss: 1.3517882823944092\n",
      "Loss: 1.4353647232055664\n",
      "Loss: 1.157196044921875\n",
      "Loss: 1.3252780437469482\n",
      "Loss: 1.3122432231903076\n",
      "Loss: 1.1853066682815552\n",
      "Loss: 1.2519562244415283\n",
      "Loss: 1.3399854898452759\n",
      "Loss: 1.3301687240600586\n",
      "Loss: 1.2446739673614502\n",
      "Loss: 0.9673069715499878\n",
      "Loss: 1.179593563079834\n",
      "Loss: 1.198248028755188\n",
      "Loss: 1.2492250204086304\n",
      "Loss: 1.1838258504867554\n",
      "Loss: 1.1154186725616455\n",
      "Loss: 1.2435503005981445\n",
      "Loss: 1.1733181476593018\n",
      "Loss: 1.2816259860992432\n",
      "Loss: 1.4226664304733276\n",
      "Loss: 1.1587389707565308\n",
      "Loss: 1.1353973150253296\n",
      "Loss: 1.1618744134902954\n",
      "Loss: 1.3599145412445068\n",
      "Loss: 1.1938884258270264\n",
      "Loss: 1.4208449125289917\n",
      "Loss: 1.2440860271453857\n",
      "Loss: 1.2013260126113892\n",
      "Loss: 1.167707085609436\n",
      "Loss: 1.190779685974121\n",
      "Loss: 1.1260404586791992\n",
      "Loss: 1.0278937816619873\n",
      "Loss: 1.303391456604004\n",
      "Loss: 1.3113423585891724\n",
      "Loss: 1.2229841947555542\n",
      "Loss: 1.0783264636993408\n",
      "Loss: 1.2692532539367676\n",
      "Loss: 1.2664270401000977\n",
      "Loss: 1.1705702543258667\n",
      "Loss: 1.1029691696166992\n",
      "Loss: 1.2340068817138672\n",
      "Loss: 1.0087275505065918\n",
      "Loss: 1.2008581161499023\n",
      "Loss: 1.1323069334030151\n",
      "Loss: 1.0741431713104248\n",
      "Loss: 1.0341225862503052\n",
      "Loss: 1.0548293590545654\n",
      "Loss: 1.1281077861785889\n",
      "Loss: 1.0293493270874023\n",
      "Loss: 1.1280714273452759\n",
      "Loss: 1.2192203998565674\n",
      "Loss: 1.3690423965454102\n",
      "Loss: 1.1918632984161377\n",
      "Loss: 1.2687883377075195\n",
      "Loss: 1.3266428709030151\n",
      "Loss: 1.1690545082092285\n",
      "Loss: 1.0636903047561646\n",
      "Loss: 1.2885087728500366\n",
      "Loss: 1.456192970275879\n",
      "Loss: 1.339510440826416\n",
      "Loss: 1.3222934007644653\n",
      "Loss: 1.2157245874404907\n",
      "Loss: 1.2957513332366943\n",
      "Loss: 1.1628409624099731\n",
      "Loss: 0.9602407217025757\n",
      "Loss: 1.3418344259262085\n",
      "Loss: 1.2332618236541748\n",
      "Loss: 1.0941405296325684\n",
      "Loss: 1.273746132850647\n",
      "Loss: 1.234391450881958\n",
      "Loss: 1.2861526012420654\n",
      "Loss: 1.3526252508163452\n",
      "Loss: 1.0530059337615967\n",
      "Loss: 1.195702075958252\n",
      "Loss: 1.2838160991668701\n",
      "Loss: 1.321865200996399\n",
      "Loss: 1.2850500345230103\n",
      "Loss: 1.1863939762115479\n",
      "Loss: 1.2922964096069336\n",
      "Loss: 0.9848041534423828\n",
      "Loss: 1.1725280284881592\n",
      "Loss: 1.1318854093551636\n",
      "Loss: 1.270263433456421\n",
      "Loss: 1.2737932205200195\n",
      "Loss: 1.2582436800003052\n",
      "Loss: 1.2190213203430176\n",
      "Loss: 1.478855848312378\n",
      "Loss: 1.263303518295288\n",
      "Loss: 1.0482492446899414\n",
      "Loss: 1.1727783679962158\n",
      "Loss: 1.2216111421585083\n",
      "Loss: 0.8183339834213257\n",
      "Loss: 1.4877914190292358\n",
      "Loss: 1.1857587099075317\n",
      "Loss: 0.9796085357666016\n",
      "Loss: 1.1918102502822876\n",
      "Loss: 1.1566601991653442\n",
      "Loss: 1.1214625835418701\n",
      "Loss: 1.2897753715515137\n",
      "Loss: 1.393506646156311\n",
      "Loss: 1.1876639127731323\n",
      "Loss: 1.3913214206695557\n",
      "Loss: 1.1665016412734985\n",
      "Loss: 1.1178789138793945\n",
      "Loss: 1.231469988822937\n",
      "Loss: 1.1592686176300049\n",
      "Loss: 1.1318601369857788\n",
      "Loss: 1.157965898513794\n",
      "Loss: 1.1896733045578003\n",
      "Loss: 1.0742554664611816\n",
      "Loss: 1.121546983718872\n",
      "Loss: 1.0540724992752075\n",
      "Loss: 1.296531081199646\n",
      "Loss: 1.1609470844268799\n",
      "Loss: 1.66414213180542\n",
      "Loss: 1.3626017570495605\n",
      "Loss: 1.2246931791305542\n",
      "Loss: 1.3278210163116455\n",
      "Loss: 0.998862087726593\n",
      "Loss: 1.0043548345565796\n",
      "Loss: 1.2588485479354858\n",
      "Loss: 1.3776724338531494\n",
      "Loss: 1.0444905757904053\n",
      "Loss: 1.3926353454589844\n",
      "Loss: 1.283640742301941\n",
      "Loss: 1.3935601711273193\n",
      "Loss: 1.2700749635696411\n",
      "Loss: 1.4010580778121948\n",
      "Loss: 1.2938357591629028\n",
      "Loss: 1.2393853664398193\n",
      "Loss: 1.1966496706008911\n",
      "Loss: 1.2760347127914429\n",
      "Loss: 1.4307186603546143\n",
      "Loss: 1.2587332725524902\n",
      "Loss: 1.5049831867218018\n",
      "Loss: 1.1174768209457397\n",
      "Loss: 1.3139582872390747\n",
      "Loss: 1.2776484489440918\n",
      "Loss: 1.1688232421875\n",
      "Loss: 1.2090564966201782\n",
      "Loss: 1.3832415342330933\n",
      "Loss: 1.4675743579864502\n",
      "Loss: 1.2586028575897217\n",
      "Loss: 1.2986814975738525\n",
      "Loss: 1.0805147886276245\n",
      "Loss: 1.240971326828003\n",
      "Loss: 1.2335131168365479\n",
      "Loss: 1.1076582670211792\n",
      "Loss: 1.2298142910003662\n",
      "Loss: 1.3393030166625977\n",
      "Loss: 1.3650034666061401\n",
      "Loss: 1.1393321752548218\n",
      "Loss: 1.3616055250167847\n",
      "Loss: 1.1057307720184326\n",
      "Loss: 1.307349443435669\n",
      "Loss: 1.2807817459106445\n",
      "Loss: 1.2801460027694702\n",
      "Loss: 1.2532916069030762\n",
      "Loss: 1.2900947332382202\n",
      "Loss: 1.0855380296707153\n",
      "Loss: 1.379518747329712\n",
      "Loss: 1.1467593908309937\n",
      "Loss: 0.9852449893951416\n",
      "Loss: 1.4831522703170776\n",
      "Loss: 1.1293200254440308\n",
      "Loss: 1.4231446981430054\n",
      "Loss: 1.0936334133148193\n",
      "Loss: 1.4027410745620728\n",
      "Loss: 1.1791912317276\n",
      "Loss: 1.1081740856170654\n",
      "Loss: 1.1920348405838013\n",
      "Loss: 1.1149975061416626\n",
      "Loss: 1.2038017511367798\n",
      "Loss: 1.0106333494186401\n",
      "Loss: 1.4339001178741455\n",
      "Loss: 1.1490801572799683\n",
      "Loss: 1.4813343286514282\n",
      "Loss: 1.8436601161956787\n",
      "Loss: 0.992838442325592\n",
      "Loss: 1.2004950046539307\n",
      "Loss: 1.2700743675231934\n",
      "Loss: 1.0762827396392822\n",
      "Loss: 1.3768911361694336\n",
      "Loss: 1.4488389492034912\n",
      "Loss: 1.3022938966751099\n",
      "Loss: 1.3164743185043335\n",
      "Loss: 1.0845494270324707\n",
      "Loss: 1.231898307800293\n",
      "Loss: 1.4370489120483398\n",
      "Loss: 1.406612515449524\n",
      "Loss: 1.1785396337509155\n",
      "Loss: 1.197891354560852\n",
      "Loss: 1.1143451929092407\n",
      "Loss: 1.0594983100891113\n",
      "Loss: 1.1966118812561035\n",
      "Loss: 1.439035177230835\n",
      "Loss: 0.9877829551696777\n",
      "Loss: 1.245133638381958\n",
      "Loss: 1.376039981842041\n",
      "Loss: 1.07768976688385\n",
      "Loss: 1.2206538915634155\n",
      "Loss: 1.2591651678085327\n",
      "Loss: 1.369421124458313\n",
      "Loss: 1.1184288263320923\n",
      "Loss: 1.0475637912750244\n",
      "Loss: 0.930880069732666\n",
      "Loss: 1.2883648872375488\n",
      "Loss: 1.217452883720398\n",
      "Loss: 1.0205053091049194\n",
      "Loss: 1.0486727952957153\n",
      "Loss: 1.3300988674163818\n",
      "Loss: 1.0711673498153687\n",
      "Loss: 1.2951669692993164\n",
      "Loss: 1.0698494911193848\n",
      "Loss: 1.7003238201141357\n",
      "Loss: 1.296053409576416\n",
      "Loss: 1.2219146490097046\n",
      "Loss: 1.202470302581787\n",
      "Loss: 1.3263145685195923\n",
      "Loss: 1.121506929397583\n",
      "Loss: 1.2151283025741577\n",
      "Loss: 1.3277678489685059\n",
      "Loss: 1.4927781820297241\n",
      "Loss: 1.1611453294754028\n",
      "Loss: 1.3179692029953003\n",
      "Loss: 1.4605231285095215\n",
      "Loss: 1.1721575260162354\n",
      "Loss: 1.158571481704712\n",
      "Loss: 1.1376765966415405\n",
      "Loss: 1.0068988800048828\n",
      "Loss: 1.4054572582244873\n",
      "Loss: 1.2447326183319092\n",
      "Loss: 1.4498543739318848\n",
      "Loss: 1.3311924934387207\n",
      "Loss: 1.0564143657684326\n",
      "Loss: 1.1576443910598755\n",
      "Loss: 1.2988070249557495\n",
      "Loss: 1.1543501615524292\n",
      "Loss: 1.1974117755889893\n",
      "Loss: 1.5257419347763062\n",
      "Loss: 1.471794605255127\n",
      "Loss: 1.1776920557022095\n",
      "Loss: 1.3780800104141235\n",
      "Loss: 1.1459885835647583\n",
      "Loss: 1.1982214450836182\n",
      "Loss: 1.676869511604309\n",
      "Loss: 1.2468976974487305\n",
      "Loss: 1.1324021816253662\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 3.4085\n",
      "a: -0.1000\n",
      "step:  1\n",
      "loss: 2.5221\n",
      "a: -0.0968\n",
      "step:  2\n",
      "loss: 2.5012\n",
      "a: -0.0941\n",
      "step:  3\n",
      "loss: 2.6047\n",
      "a: -0.0919\n",
      "step:  4\n",
      "loss: 2.3506\n",
      "a: -0.0891\n",
      "step:  5\n",
      "loss: 2.4400\n",
      "a: -0.0866\n",
      "step:  6\n",
      "loss: 2.3091\n",
      "a: -0.0840\n",
      "step:  7\n",
      "loss: 2.3205\n",
      "a: -0.0816\n",
      "step:  8\n",
      "loss: 2.3555\n",
      "a: -0.0792\n",
      "step:  9\n",
      "loss: 2.3413\n",
      "a: -0.0770\n",
      "step:  10\n",
      "loss: 2.3310\n",
      "a: -0.0750\n",
      "step:  11\n",
      "loss: 2.4262\n",
      "a: -0.0729\n",
      "step:  12\n",
      "loss: 2.2930\n",
      "a: -0.0709\n",
      "step:  13\n",
      "loss: 2.3100\n",
      "a: -0.0690\n",
      "step:  14\n",
      "loss: 2.2143\n",
      "a: -0.0672\n",
      "step:  15\n",
      "loss: 2.2310\n",
      "a: -0.0655\n",
      "step:  16\n",
      "loss: 2.2795\n",
      "a: -0.0639\n",
      "step:  17\n",
      "loss: 2.3911\n",
      "a: -0.0623\n",
      "step:  18\n",
      "loss: 2.2091\n",
      "a: -0.0607\n",
      "step:  19\n",
      "loss: 2.1138\n",
      "a: -0.0591\n",
      "step:  20\n",
      "loss: 2.1172\n",
      "a: -0.0576\n",
      "step:  21\n",
      "loss: 2.2578\n",
      "a: -0.0561\n",
      "step:  22\n",
      "loss: 2.2086\n",
      "a: -0.0546\n",
      "step:  23\n",
      "loss: 2.1716\n",
      "a: -0.0532\n",
      "step:  24\n",
      "loss: 2.0940\n",
      "a: -0.0518\n",
      "step:  25\n",
      "loss: 2.1407\n",
      "a: -0.0503\n",
      "step:  26\n",
      "loss: 2.1106\n",
      "a: -0.0489\n",
      "step:  27\n",
      "loss: 2.0607\n",
      "a: -0.0474\n",
      "step:  28\n",
      "loss: 2.1161\n",
      "a: -0.0460\n",
      "step:  29\n",
      "loss: 1.9676\n",
      "a: -0.0447\n",
      "step:  30\n",
      "loss: 2.1163\n",
      "a: -0.0434\n",
      "step:  31\n",
      "loss: 1.9599\n",
      "a: -0.0420\n",
      "step:  32\n",
      "loss: 2.1428\n",
      "a: -0.0407\n",
      "step:  33\n",
      "loss: 2.0046\n",
      "a: -0.0394\n",
      "step:  34\n",
      "loss: 1.9258\n",
      "a: -0.0381\n",
      "step:  35\n",
      "loss: 1.9724\n",
      "a: -0.0368\n",
      "step:  36\n",
      "loss: 1.8967\n",
      "a: -0.0355\n",
      "step:  37\n",
      "loss: 1.9229\n",
      "a: -0.0342\n",
      "step:  38\n",
      "loss: 1.8829\n",
      "a: -0.0329\n",
      "step:  39\n",
      "loss: 1.8738\n",
      "a: -0.0316\n",
      "step:  40\n",
      "loss: 1.9295\n",
      "a: -0.0304\n",
      "step:  41\n",
      "loss: 1.7957\n",
      "a: -0.0292\n",
      "step:  42\n",
      "loss: 1.8951\n",
      "a: -0.0280\n",
      "step:  43\n",
      "loss: 1.7749\n",
      "a: -0.0268\n",
      "step:  44\n",
      "loss: 1.9031\n",
      "a: -0.0256\n",
      "step:  45\n",
      "loss: 1.7559\n",
      "a: -0.0244\n",
      "step:  46\n",
      "loss: 1.8441\n",
      "a: -0.0233\n",
      "step:  47\n",
      "loss: 1.7691\n",
      "a: -0.0222\n",
      "step:  48\n",
      "loss: 1.6500\n",
      "a: -0.0210\n",
      "step:  49\n",
      "loss: 1.8786\n",
      "a: -0.0198\n",
      "step:  50\n",
      "loss: 1.6281\n",
      "a: -0.0186\n",
      "step:  51\n",
      "loss: 1.9056\n",
      "a: -0.0174\n",
      "step:  52\n",
      "loss: 1.9222\n",
      "a: -0.0162\n",
      "step:  53\n",
      "loss: 1.5522\n",
      "a: -0.0150\n",
      "step:  54\n",
      "loss: 1.7388\n",
      "a: -0.0138\n",
      "step:  55\n",
      "loss: 1.7017\n",
      "a: -0.0127\n",
      "step:  56\n",
      "loss: 1.8135\n",
      "a: -0.0116\n",
      "step:  57\n",
      "loss: 1.7568\n",
      "a: -0.0105\n",
      "step:  58\n",
      "loss: 1.5650\n",
      "a: -0.0095\n",
      "step:  59\n",
      "loss: 1.6894\n",
      "a: -0.0084\n",
      "step:  60\n",
      "loss: 1.7822\n",
      "a: -0.0074\n",
      "step:  61\n",
      "loss: 1.8146\n",
      "a: -0.0064\n",
      "step:  62\n",
      "loss: 1.7770\n",
      "a: -0.0055\n",
      "step:  63\n",
      "loss: 1.7493\n",
      "a: -0.0045\n",
      "step:  64\n",
      "loss: 1.6809\n",
      "a: -0.0035\n",
      "step:  65\n",
      "loss: 1.5123\n",
      "a: -0.0026\n",
      "step:  66\n",
      "loss: 1.4301\n",
      "a: -0.0017\n",
      "step:  67\n",
      "loss: 1.6585\n",
      "a: -0.0008\n",
      "step:  68\n",
      "loss: 1.6485\n",
      "a: -0.0001\n",
      "step:  69\n",
      "loss: 1.8119\n",
      "a: 0.0007\n",
      "step:  70\n",
      "loss: 1.3545\n",
      "a: 0.0014\n",
      "step:  71\n",
      "loss: 1.7212\n",
      "a: 0.0021\n",
      "step:  72\n",
      "loss: 1.7668\n",
      "a: 0.0025\n",
      "step:  73\n",
      "loss: 1.5602\n",
      "a: 0.0028\n",
      "step:  74\n",
      "loss: 1.8980\n",
      "a: 0.0030\n",
      "step:  75\n",
      "loss: 1.6417\n",
      "a: 0.0031\n",
      "step:  76\n",
      "loss: 1.6261\n",
      "a: 0.0029\n",
      "step:  77\n",
      "loss: 1.8146\n",
      "a: 0.0026\n",
      "step:  78\n",
      "loss: 1.9130\n",
      "a: 0.0020\n",
      "step:  79\n",
      "loss: 1.6634\n",
      "a: 0.0011\n",
      "step:  80\n",
      "loss: 1.7140\n",
      "a: 0.0001\n",
      "step:  81\n",
      "loss: 1.6001\n",
      "a: -0.0009\n",
      "step:  82\n",
      "loss: 1.5459\n",
      "a: -0.0019\n",
      "step:  83\n",
      "loss: 1.7669\n",
      "a: -0.0030\n",
      "step:  84\n",
      "loss: 1.6140\n",
      "a: -0.0040\n",
      "step:  85\n",
      "loss: 1.5271\n",
      "a: -0.0051\n",
      "step:  86\n",
      "loss: 1.4989\n",
      "a: -0.0062\n",
      "step:  87\n",
      "loss: 1.5702\n",
      "a: -0.0072\n",
      "step:  88\n",
      "loss: 1.6632\n",
      "a: -0.0082\n",
      "step:  89\n",
      "loss: 1.7257\n",
      "a: -0.0090\n",
      "step:  90\n",
      "loss: 1.8331\n",
      "a: -0.0096\n",
      "step:  91\n",
      "loss: 1.7898\n",
      "a: -0.0101\n",
      "step:  92\n",
      "loss: 1.5673\n",
      "a: -0.0104\n",
      "step:  93\n",
      "loss: 1.6740\n",
      "a: -0.0103\n",
      "step:  94\n",
      "loss: 1.6874\n",
      "a: -0.0100\n",
      "step:  95\n",
      "loss: 1.5068\n",
      "a: -0.0093\n",
      "step:  96\n",
      "loss: 1.7248\n",
      "a: -0.0085\n",
      "step:  97\n",
      "loss: 1.6170\n",
      "a: -0.0075\n",
      "step:  98\n",
      "loss: 1.5882\n",
      "a: -0.0063\n",
      "step:  99\n",
      "loss: 1.5802\n",
      "a: -0.0052\n",
      "step:  100\n",
      "loss: 1.6569\n",
      "a: -0.0041\n",
      "step:  101\n",
      "loss: 1.7392\n",
      "a: -0.0029\n",
      "step:  102\n",
      "loss: 1.5889\n",
      "a: -0.0020\n",
      "step:  103\n",
      "loss: 1.6225\n",
      "a: -0.0010\n",
      "step:  104\n",
      "loss: 1.5066\n",
      "a: -0.0003\n",
      "step:  105\n",
      "loss: 1.8229\n",
      "a: 0.0004\n",
      "step:  106\n",
      "loss: 1.6268\n",
      "a: 0.0010\n",
      "step:  107\n",
      "loss: 1.4473\n",
      "a: 0.0012\n",
      "step:  108\n",
      "loss: 1.5546\n",
      "a: 0.0013\n",
      "step:  109\n",
      "loss: 1.5561\n",
      "a: 0.0010\n",
      "step:  110\n",
      "loss: 1.5924\n",
      "a: 0.0002\n",
      "step:  111\n",
      "loss: 1.2335\n",
      "a: -0.0006\n",
      "step:  112\n",
      "loss: 1.6826\n",
      "a: -0.0015\n",
      "step:  113\n",
      "loss: 1.6981\n",
      "a: -0.0025\n",
      "step:  114\n",
      "loss: 1.3215\n",
      "a: -0.0034\n",
      "step:  115\n",
      "loss: 1.4294\n",
      "a: -0.0038\n",
      "step:  116\n",
      "loss: 1.6396\n",
      "a: -0.0041\n",
      "step:  117\n",
      "loss: 1.7817\n",
      "a: -0.0046\n",
      "step:  118\n",
      "loss: 1.7511\n",
      "a: -0.0051\n",
      "step:  119\n",
      "loss: 1.7832\n",
      "a: -0.0050\n",
      "step:  120\n",
      "loss: 1.6281\n",
      "a: -0.0046\n",
      "step:  121\n",
      "loss: 1.5801\n",
      "a: -0.0036\n",
      "step:  122\n",
      "loss: 1.4008\n",
      "a: -0.0029\n",
      "step:  123\n",
      "loss: 1.4286\n",
      "a: -0.0028\n",
      "step:  124\n",
      "loss: 1.6410\n",
      "a: -0.0028\n",
      "step:  125\n",
      "loss: 1.3077\n",
      "a: -0.0030\n",
      "step:  126\n",
      "loss: 1.6435\n",
      "a: -0.0031\n",
      "step:  127\n",
      "loss: 1.4600\n",
      "a: -0.0035\n",
      "step:  128\n",
      "loss: 1.7056\n",
      "a: -0.0044\n",
      "step:  129\n",
      "loss: 1.7571\n",
      "a: -0.0057\n",
      "step:  130\n",
      "loss: 1.4880\n",
      "a: -0.0061\n",
      "step:  131\n",
      "loss: 1.5708\n",
      "a: -0.0063\n",
      "step:  132\n",
      "loss: 1.5376\n",
      "a: -0.0054\n",
      "step:  133\n",
      "loss: 1.4449\n",
      "a: -0.0041\n",
      "step:  134\n",
      "loss: 1.6267\n",
      "a: -0.0026\n",
      "step:  135\n",
      "loss: 1.4549\n",
      "a: -0.0010\n",
      "step:  136\n",
      "loss: 1.5073\n",
      "a: 0.0003\n",
      "step:  137\n",
      "loss: 1.7053\n",
      "a: 0.0006\n",
      "step:  138\n",
      "loss: 1.4918\n",
      "a: 0.0003\n",
      "step:  139\n",
      "loss: 1.6660\n",
      "a: 0.0000\n",
      "step:  140\n",
      "loss: 1.4423\n",
      "a: 0.0002\n",
      "step:  141\n",
      "loss: 1.5049\n",
      "a: 0.0009\n",
      "step:  142\n",
      "loss: 1.5169\n",
      "a: 0.0013\n",
      "step:  143\n",
      "loss: 1.5914\n",
      "a: 0.0005\n",
      "step:  144\n",
      "loss: 1.5522\n",
      "a: -0.0002\n",
      "step:  145\n",
      "loss: 1.5344\n",
      "a: -0.0012\n",
      "step:  146\n",
      "loss: 1.4068\n",
      "a: -0.0027\n",
      "step:  147\n",
      "loss: 1.6969\n",
      "a: -0.0042\n",
      "step:  148\n",
      "loss: 1.6349\n",
      "a: -0.0054\n",
      "step:  149\n",
      "loss: 1.5064\n",
      "a: -0.0052\n",
      "step:  150\n",
      "loss: 1.4180\n",
      "a: -0.0037\n",
      "step:  151\n",
      "loss: 1.5857\n",
      "a: -0.0020\n",
      "step:  152\n",
      "loss: 1.4785\n",
      "a: -0.0012\n",
      "step:  153\n",
      "loss: 1.4036\n",
      "a: -0.0012\n",
      "step:  154\n",
      "loss: 1.4994\n",
      "a: -0.0025\n",
      "step:  155\n",
      "loss: 1.4950\n",
      "a: -0.0034\n",
      "step:  156\n",
      "loss: 1.4671\n",
      "a: -0.0043\n",
      "step:  157\n",
      "loss: 1.3805\n",
      "a: -0.0048\n",
      "step:  158\n",
      "loss: 1.6333\n",
      "a: -0.0048\n",
      "step:  159\n",
      "loss: 1.6569\n",
      "a: -0.0030\n",
      "step:  160\n",
      "loss: 1.4015\n",
      "a: -0.0014\n",
      "step:  161\n",
      "loss: 1.4531\n",
      "a: -0.0007\n",
      "step:  162\n",
      "loss: 1.5482\n",
      "a: -0.0009\n",
      "step:  163\n",
      "loss: 1.5637\n",
      "a: -0.0024\n",
      "step:  164\n",
      "loss: 1.5310\n",
      "a: -0.0034\n",
      "step:  165\n",
      "loss: 1.5554\n",
      "a: -0.0045\n",
      "step:  166\n",
      "loss: 1.5648\n",
      "a: -0.0047\n",
      "step:  167\n",
      "loss: 1.4782\n",
      "a: -0.0043\n",
      "step:  168\n",
      "loss: 1.4542\n",
      "a: -0.0036\n",
      "step:  169\n",
      "loss: 1.5651\n",
      "a: -0.0028\n",
      "step:  170\n",
      "loss: 1.6215\n",
      "a: -0.0008\n",
      "step:  171\n",
      "loss: 1.9141\n",
      "a: 0.0011\n",
      "step:  172\n",
      "loss: 1.5872\n",
      "a: -0.0013\n",
      "step:  173\n",
      "loss: 1.6786\n",
      "a: -0.0034\n",
      "step:  174\n",
      "loss: 1.4823\n",
      "a: -0.0051\n",
      "step:  175\n",
      "loss: 1.4915\n",
      "a: -0.0064\n",
      "step:  176\n",
      "loss: 1.7009\n",
      "a: -0.0071\n",
      "step:  177\n",
      "loss: 1.7038\n",
      "a: -0.0071\n",
      "step:  178\n",
      "loss: 1.5468\n",
      "a: -0.0060\n",
      "step:  179\n",
      "loss: 1.4559\n",
      "a: -0.0043\n",
      "step:  180\n",
      "loss: 1.4176\n",
      "a: -0.0026\n",
      "step:  181\n",
      "loss: 1.8261\n",
      "a: -0.0012\n",
      "step:  182\n",
      "loss: 1.7024\n",
      "a: -0.0000\n",
      "step:  183\n",
      "loss: 1.4918\n",
      "a: 0.0004\n",
      "step:  184\n",
      "loss: 1.4905\n",
      "a: 0.0003\n",
      "step:  185\n",
      "loss: 1.3890\n",
      "a: -0.0000\n",
      "step:  186\n",
      "loss: 1.4765\n",
      "a: -0.0009\n",
      "step:  187\n",
      "loss: 1.5421\n",
      "a: -0.0017\n",
      "step:  188\n",
      "loss: 1.2771\n",
      "a: -0.0026\n",
      "step:  189\n",
      "loss: 1.4485\n",
      "a: -0.0035\n",
      "step:  190\n",
      "loss: 1.4907\n",
      "a: -0.0041\n",
      "step:  191\n",
      "loss: 1.1742\n",
      "a: -0.0045\n",
      "step:  192\n",
      "loss: 1.6574\n",
      "a: -0.0045\n",
      "step:  193\n",
      "loss: 1.2833\n",
      "a: -0.0037\n",
      "step:  194\n",
      "loss: 1.5668\n",
      "a: -0.0030\n",
      "step:  195\n",
      "loss: 1.5464\n",
      "a: -0.0023\n",
      "step:  196\n",
      "loss: 1.5504\n",
      "a: -0.0014\n",
      "step:  197\n",
      "loss: 1.5030\n",
      "a: -0.0005\n",
      "step:  198\n",
      "loss: 1.6402\n",
      "a: -0.0003\n",
      "step:  199\n",
      "loss: 1.4054\n",
      "a: -0.0010\n",
      "step:  200\n",
      "loss: 1.3490\n",
      "a: -0.0025\n",
      "step:  201\n",
      "loss: 1.5555\n",
      "a: -0.0039\n",
      "step:  202\n",
      "loss: 1.2723\n",
      "a: -0.0054\n",
      "step:  203\n",
      "loss: 1.2722\n",
      "a: -0.0063\n",
      "step:  204\n",
      "loss: 1.5342\n",
      "a: -0.0073\n",
      "step:  205\n",
      "loss: 1.4231\n",
      "a: -0.0076\n",
      "step:  206\n",
      "loss: 1.4120\n",
      "a: -0.0072\n",
      "step:  207\n",
      "loss: 1.7237\n",
      "a: -0.0063\n",
      "step:  208\n",
      "loss: 1.7755\n",
      "a: -0.0051\n",
      "step:  209\n",
      "loss: 1.3786\n",
      "a: -0.0033\n",
      "step:  210\n",
      "loss: 1.5147\n",
      "a: -0.0017\n",
      "step:  211\n",
      "loss: 1.7378\n",
      "a: -0.0004\n",
      "step:  212\n",
      "loss: 1.5177\n",
      "a: 0.0004\n",
      "step:  213\n",
      "loss: 1.4615\n",
      "a: -0.0005\n",
      "step:  214\n",
      "loss: 1.4202\n",
      "a: -0.0017\n",
      "step:  215\n",
      "loss: 1.3972\n",
      "a: -0.0033\n",
      "step:  216\n",
      "loss: 1.3857\n",
      "a: -0.0048\n",
      "step:  217\n",
      "loss: 1.6005\n",
      "a: -0.0062\n",
      "step:  218\n",
      "loss: 1.5054\n",
      "a: -0.0073\n",
      "step:  219\n",
      "loss: 1.8596\n",
      "a: -0.0082\n",
      "step:  220\n",
      "loss: 1.1927\n",
      "a: -0.0086\n",
      "step:  221\n",
      "loss: 1.6698\n",
      "a: -0.0087\n",
      "step:  222\n",
      "loss: 1.6810\n",
      "a: -0.0080\n",
      "step:  223\n",
      "loss: 1.3340\n",
      "a: -0.0071\n",
      "step:  224\n",
      "loss: 1.5324\n",
      "a: -0.0058\n",
      "step:  225\n",
      "loss: 1.3293\n",
      "a: -0.0046\n",
      "step:  226\n",
      "loss: 1.4565\n",
      "a: -0.0033\n",
      "step:  227\n",
      "loss: 1.5513\n",
      "a: -0.0021\n",
      "step:  228\n",
      "loss: 1.6032\n",
      "a: -0.0014\n",
      "step:  229\n",
      "loss: 1.5490\n",
      "a: -0.0013\n",
      "step:  230\n",
      "loss: 1.4824\n",
      "a: -0.0018\n",
      "step:  231\n",
      "loss: 1.5488\n",
      "a: -0.0027\n",
      "step:  232\n",
      "loss: 1.3808\n",
      "a: -0.0035\n",
      "step:  233\n",
      "loss: 1.5430\n",
      "a: -0.0044\n",
      "step:  234\n",
      "loss: 1.4282\n",
      "a: -0.0052\n",
      "step:  235\n",
      "loss: 1.6667\n",
      "a: -0.0056\n",
      "step:  236\n",
      "loss: 1.5187\n",
      "a: -0.0058\n",
      "step:  237\n",
      "loss: 1.3366\n",
      "a: -0.0056\n",
      "step:  238\n",
      "loss: 1.5825\n",
      "a: -0.0054\n",
      "step:  239\n",
      "loss: 1.5864\n",
      "a: -0.0046\n",
      "step:  240\n",
      "loss: 1.6622\n",
      "a: -0.0037\n",
      "step:  241\n",
      "loss: 1.4211\n",
      "a: -0.0033\n",
      "step:  242\n",
      "loss: 1.5797\n",
      "a: -0.0030\n",
      "step:  243\n",
      "loss: 1.6200\n",
      "a: -0.0025\n",
      "step:  244\n",
      "loss: 1.1813\n",
      "a: -0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  245\n",
      "loss: 1.5209\n",
      "a: -0.0017\n",
      "step:  246\n",
      "loss: 1.5846\n",
      "a: -0.0019\n",
      "step:  247\n",
      "loss: 1.3325\n",
      "a: -0.0028\n",
      "step:  248\n",
      "loss: 1.3627\n",
      "a: -0.0039\n",
      "step:  249\n",
      "loss: 1.6347\n",
      "a: -0.0049\n",
      "step:  250\n",
      "loss: 1.4698\n",
      "a: -0.0052\n",
      "step:  251\n",
      "loss: 1.5196\n",
      "a: -0.0057\n",
      "step:  252\n",
      "loss: 1.4727\n",
      "a: -0.0055\n",
      "step:  253\n",
      "loss: 1.4236\n",
      "a: -0.0046\n",
      "step:  254\n",
      "loss: 1.3833\n",
      "a: -0.0031\n",
      "step:  255\n",
      "loss: 1.2631\n",
      "a: -0.0018\n",
      "step:  256\n",
      "loss: 1.5503\n",
      "a: -0.0013\n",
      "step:  257\n",
      "loss: 1.7177\n",
      "a: -0.0021\n",
      "step:  258\n",
      "loss: 1.5474\n",
      "a: -0.0038\n",
      "step:  259\n",
      "loss: 1.6490\n",
      "a: -0.0051\n",
      "step:  260\n",
      "loss: 1.3808\n",
      "a: -0.0059\n",
      "step:  261\n",
      "loss: 1.5072\n",
      "a: -0.0061\n",
      "step:  262\n",
      "loss: 1.4202\n",
      "a: -0.0058\n",
      "step:  263\n",
      "loss: 1.4471\n",
      "a: -0.0049\n",
      "step:  264\n",
      "loss: 1.4630\n",
      "a: -0.0037\n",
      "step:  265\n",
      "loss: 1.2740\n",
      "a: -0.0028\n",
      "step:  266\n",
      "loss: 1.6468\n",
      "a: -0.0021\n",
      "step:  267\n",
      "loss: 1.3712\n",
      "a: -0.0020\n",
      "step:  268\n",
      "loss: 1.4291\n",
      "a: -0.0029\n",
      "step:  269\n",
      "loss: 1.7333\n",
      "a: -0.0041\n",
      "step:  270\n",
      "loss: 1.5332\n",
      "a: -0.0057\n",
      "step:  271\n",
      "loss: 1.3080\n",
      "a: -0.0071\n",
      "step:  272\n",
      "loss: 1.6563\n",
      "a: -0.0082\n",
      "step:  273\n",
      "loss: 1.3753\n",
      "a: -0.0089\n",
      "step:  274\n",
      "loss: 1.5225\n",
      "a: -0.0095\n",
      "step:  275\n",
      "loss: 1.7651\n",
      "a: -0.0097\n",
      "step:  276\n",
      "loss: 1.4318\n",
      "a: -0.0091\n",
      "step:  277\n",
      "loss: 1.5283\n",
      "a: -0.0082\n",
      "step:  278\n",
      "loss: 1.3391\n",
      "a: -0.0069\n",
      "step:  279\n",
      "loss: 1.4175\n",
      "a: -0.0056\n",
      "step:  280\n",
      "loss: 1.2237\n",
      "a: -0.0042\n",
      "step:  281\n",
      "loss: 1.4786\n",
      "a: -0.0030\n",
      "step:  282\n",
      "loss: 1.6658\n",
      "a: -0.0023\n",
      "step:  283\n",
      "loss: 1.6725\n",
      "a: -0.0022\n",
      "step:  284\n",
      "loss: 1.4878\n",
      "a: -0.0030\n",
      "step:  285\n",
      "loss: 1.3781\n",
      "a: -0.0043\n",
      "step:  286\n",
      "loss: 1.3871\n",
      "a: -0.0055\n",
      "step:  287\n",
      "loss: 1.6144\n",
      "a: -0.0066\n",
      "step:  288\n",
      "loss: 1.4349\n",
      "a: -0.0078\n",
      "step:  289\n",
      "loss: 1.6048\n",
      "a: -0.0087\n",
      "step:  290\n",
      "loss: 1.4849\n",
      "a: -0.0093\n",
      "step:  291\n",
      "loss: 1.3999\n",
      "a: -0.0093\n",
      "step:  292\n",
      "loss: 1.5296\n",
      "a: -0.0088\n",
      "step:  293\n",
      "loss: 1.5687\n",
      "a: -0.0079\n",
      "step:  294\n",
      "loss: 1.3354\n",
      "a: -0.0065\n",
      "step:  295\n",
      "loss: 1.3680\n",
      "a: -0.0051\n",
      "step:  296\n",
      "loss: 1.2096\n",
      "a: -0.0038\n",
      "step:  297\n",
      "loss: 1.4351\n",
      "a: -0.0026\n",
      "step:  298\n",
      "loss: 1.5171\n",
      "a: -0.0015\n",
      "step:  299\n",
      "loss: 1.5058\n",
      "a: -0.0008\n",
      "step:  300\n",
      "loss: 1.6863\n",
      "a: -0.0019\n",
      "step:  301\n",
      "loss: 1.4714\n",
      "a: -0.0039\n",
      "step:  302\n",
      "loss: 1.3872\n",
      "a: -0.0059\n",
      "step:  303\n",
      "loss: 1.4142\n",
      "a: -0.0077\n",
      "step:  304\n",
      "loss: 1.5576\n",
      "a: -0.0091\n",
      "step:  305\n",
      "loss: 1.4400\n",
      "a: -0.0102\n",
      "step:  306\n",
      "loss: 1.7764\n",
      "a: -0.0110\n",
      "step:  307\n",
      "loss: 1.7344\n",
      "a: -0.0115\n",
      "step:  308\n",
      "loss: 1.6417\n",
      "a: -0.0113\n",
      "step:  309\n",
      "loss: 1.5149\n",
      "a: -0.0106\n",
      "step:  310\n",
      "loss: 1.4317\n",
      "a: -0.0094\n",
      "step:  311\n",
      "loss: 1.4205\n",
      "a: -0.0081\n",
      "step:  312\n",
      "loss: 1.4711\n",
      "a: -0.0067\n",
      "step:  313\n",
      "loss: 1.3685\n",
      "a: -0.0054\n",
      "step:  314\n",
      "loss: 1.3144\n",
      "a: -0.0041\n",
      "step:  315\n",
      "loss: 1.4795\n",
      "a: -0.0029\n",
      "step:  316\n",
      "loss: 1.4846\n",
      "a: -0.0020\n",
      "step:  317\n",
      "loss: 1.3948\n",
      "a: -0.0013\n",
      "step:  318\n",
      "loss: 1.5317\n",
      "a: -0.0010\n",
      "step:  319\n",
      "loss: 1.4584\n",
      "a: -0.0018\n",
      "step:  320\n",
      "loss: 1.3177\n",
      "a: -0.0030\n",
      "step:  321\n",
      "loss: 1.5132\n",
      "a: -0.0043\n",
      "step:  322\n",
      "loss: 1.4429\n",
      "a: -0.0054\n",
      "step:  323\n",
      "loss: 1.3838\n",
      "a: -0.0065\n",
      "step:  324\n",
      "loss: 1.3738\n",
      "a: -0.0073\n",
      "step:  325\n",
      "loss: 1.3955\n",
      "a: -0.0079\n",
      "step:  326\n",
      "loss: 1.5592\n",
      "a: -0.0084\n",
      "step:  327\n",
      "loss: 1.6626\n",
      "a: -0.0087\n",
      "step:  328\n",
      "loss: 1.6361\n",
      "a: -0.0089\n",
      "step:  329\n",
      "loss: 1.3335\n",
      "a: -0.0086\n",
      "step:  330\n",
      "loss: 1.4324\n",
      "a: -0.0081\n",
      "step:  331\n",
      "loss: 1.3349\n",
      "a: -0.0073\n",
      "step:  332\n",
      "loss: 1.3686\n",
      "a: -0.0063\n",
      "step:  333\n",
      "loss: 1.4735\n",
      "a: -0.0050\n",
      "step:  334\n",
      "loss: 1.2658\n",
      "a: -0.0036\n",
      "step:  335\n",
      "loss: 1.7588\n",
      "a: -0.0023\n",
      "step:  336\n",
      "loss: 1.3266\n",
      "a: -0.0017\n",
      "step:  337\n",
      "loss: 1.5655\n",
      "a: -0.0017\n",
      "step:  338\n",
      "loss: 1.3494\n",
      "a: -0.0024\n",
      "step:  339\n",
      "loss: 1.4804\n",
      "a: -0.0035\n",
      "step:  340\n",
      "loss: 1.5241\n",
      "a: -0.0045\n",
      "step:  341\n",
      "loss: 1.5804\n",
      "a: -0.0054\n",
      "step:  342\n",
      "loss: 1.3673\n",
      "a: -0.0061\n",
      "step:  343\n",
      "loss: 1.5484\n",
      "a: -0.0064\n",
      "step:  344\n",
      "loss: 1.5041\n",
      "a: -0.0066\n",
      "step:  345\n",
      "loss: 1.3817\n",
      "a: -0.0064\n",
      "step:  346\n",
      "loss: 1.3506\n",
      "a: -0.0059\n",
      "step:  347\n",
      "loss: 1.4246\n",
      "a: -0.0050\n",
      "step:  348\n",
      "loss: 1.3054\n",
      "a: -0.0038\n",
      "step:  349\n",
      "loss: 1.3737\n",
      "a: -0.0029\n",
      "step:  350\n",
      "loss: 1.5774\n",
      "a: -0.0025\n",
      "step:  351\n",
      "loss: 1.4159\n",
      "a: -0.0028\n",
      "step:  352\n",
      "loss: 1.4766\n",
      "a: -0.0029\n",
      "step:  353\n",
      "loss: 1.5666\n",
      "a: -0.0026\n",
      "step:  354\n",
      "loss: 1.5964\n",
      "a: -0.0029\n",
      "step:  355\n",
      "loss: 1.5003\n",
      "a: -0.0032\n",
      "step:  356\n",
      "loss: 1.2437\n",
      "a: -0.0035\n",
      "step:  357\n",
      "loss: 1.2803\n",
      "a: -0.0039\n",
      "step:  358\n",
      "loss: 1.5494\n",
      "a: -0.0038\n",
      "step:  359\n",
      "loss: 1.2628\n",
      "a: -0.0041\n",
      "step:  360\n",
      "loss: 1.4170\n",
      "a: -0.0046\n",
      "step:  361\n",
      "loss: 1.4638\n",
      "a: -0.0054\n",
      "step:  362\n",
      "loss: 1.6000\n",
      "a: -0.0060\n",
      "step:  363\n",
      "loss: 1.4271\n",
      "a: -0.0066\n",
      "step:  364\n",
      "loss: 1.3582\n",
      "a: -0.0062\n",
      "step:  365\n",
      "loss: 1.4264\n",
      "a: -0.0050\n",
      "step:  366\n",
      "loss: 1.5278\n",
      "a: -0.0033\n",
      "step:  367\n",
      "loss: 1.3411\n",
      "a: -0.0019\n",
      "step:  368\n",
      "loss: 1.7706\n",
      "a: -0.0013\n",
      "step:  369\n",
      "loss: 1.4430\n",
      "a: -0.0034\n",
      "step:  370\n",
      "loss: 1.4483\n",
      "a: -0.0055\n",
      "step:  371\n",
      "loss: 1.5394\n",
      "a: -0.0072\n",
      "step:  372\n",
      "loss: 1.6347\n",
      "a: -0.0081\n",
      "step:  373\n",
      "loss: 1.4695\n",
      "a: -0.0083\n",
      "step:  374\n",
      "loss: 1.5499\n",
      "a: -0.0080\n",
      "step:  375\n",
      "loss: 1.6720\n",
      "a: -0.0067\n",
      "step:  376\n",
      "loss: 1.1936\n",
      "a: -0.0049\n",
      "step:  377\n",
      "loss: 1.4614\n",
      "a: -0.0032\n",
      "step:  378\n",
      "loss: 1.3751\n",
      "a: -0.0015\n",
      "step:  379\n",
      "loss: 1.8183\n",
      "a: 0.0001\n",
      "step:  380\n",
      "loss: 1.1914\n",
      "a: -0.0024\n",
      "step:  381\n",
      "loss: 1.3997\n",
      "a: -0.0045\n",
      "step:  382\n",
      "loss: 1.3573\n",
      "a: -0.0063\n",
      "step:  383\n",
      "loss: 1.7872\n",
      "a: -0.0079\n",
      "step:  384\n",
      "loss: 1.4352\n",
      "a: -0.0093\n",
      "step:  385\n",
      "loss: 1.5285\n",
      "a: -0.0105\n",
      "step:  386\n",
      "loss: 1.7676\n",
      "a: -0.0115\n",
      "step:  387\n",
      "loss: 1.4668\n",
      "a: -0.0122\n",
      "step:  388\n",
      "loss: 1.8264\n",
      "a: -0.0127\n",
      "step:  389\n",
      "loss: 1.7786\n",
      "a: -0.0129\n",
      "step:  390\n",
      "loss: 1.8431\n",
      "a: -0.0128\n",
      "step:  391\n",
      "loss: 1.8258\n",
      "a: -0.0124\n",
      "step:  392\n",
      "loss: 1.7676\n",
      "a: -0.0116\n",
      "step:  393\n",
      "loss: 1.2490\n",
      "a: -0.0104\n",
      "step:  394\n",
      "loss: 1.5124\n",
      "a: -0.0092\n",
      "step:  395\n",
      "loss: 1.4598\n",
      "a: -0.0079\n",
      "step:  396\n",
      "loss: 1.4803\n",
      "a: -0.0067\n",
      "step:  397\n",
      "loss: 1.5367\n",
      "a: -0.0055\n",
      "step:  398\n",
      "loss: 1.7900\n",
      "a: -0.0044\n",
      "step:  399\n",
      "loss: 1.4096\n",
      "a: -0.0035\n",
      "step:  400\n",
      "loss: 1.3900\n",
      "a: -0.0027\n",
      "step:  401\n",
      "loss: 1.7690\n",
      "a: -0.0022\n",
      "step:  402\n",
      "loss: 1.6105\n",
      "a: -0.0020\n",
      "step:  403\n",
      "loss: 1.6909\n",
      "a: -0.0021\n",
      "step:  404\n",
      "loss: 1.6312\n",
      "a: -0.0027\n",
      "step:  405\n",
      "loss: 1.7770\n",
      "a: -0.0035\n",
      "step:  406\n",
      "loss: 1.5975\n",
      "a: -0.0045\n",
      "step:  407\n",
      "loss: 1.5194\n",
      "a: -0.0056\n",
      "step:  408\n",
      "loss: 1.6813\n",
      "a: -0.0068\n",
      "step:  409\n",
      "loss: 1.4421\n",
      "a: -0.0079\n",
      "step:  410\n",
      "loss: 1.5364\n",
      "a: -0.0090\n",
      "step:  411\n",
      "loss: 1.6065\n",
      "a: -0.0099\n",
      "step:  412\n",
      "loss: 1.4862\n",
      "a: -0.0107\n",
      "step:  413\n",
      "loss: 1.4724\n",
      "a: -0.0114\n",
      "step:  414\n",
      "loss: 1.6989\n",
      "a: -0.0118\n",
      "step:  415\n",
      "loss: 1.8300\n",
      "a: -0.0118\n",
      "step:  416\n",
      "loss: 1.5716\n",
      "a: -0.0115\n",
      "step:  417\n",
      "loss: 1.3857\n",
      "a: -0.0110\n",
      "step:  418\n",
      "loss: 1.6650\n",
      "a: -0.0102\n",
      "step:  419\n",
      "loss: 1.6840\n",
      "a: -0.0092\n",
      "step:  420\n",
      "loss: 1.5038\n",
      "a: -0.0081\n",
      "step:  421\n",
      "loss: 1.6525\n",
      "a: -0.0069\n",
      "step:  422\n",
      "loss: 1.4078\n",
      "a: -0.0056\n",
      "step:  423\n",
      "loss: 1.5590\n",
      "a: -0.0043\n",
      "step:  424\n",
      "loss: 1.3243\n",
      "a: -0.0031\n",
      "step:  425\n",
      "loss: 1.3945\n",
      "a: -0.0018\n",
      "step:  426\n",
      "loss: 1.6448\n",
      "a: -0.0008\n",
      "step:  427\n",
      "loss: 1.9456\n",
      "a: 0.0001\n",
      "step:  428\n",
      "loss: 1.5604\n",
      "a: 0.0003\n",
      "step:  429\n",
      "loss: 1.3839\n",
      "a: -0.0024\n",
      "step:  430\n",
      "loss: 1.5211\n",
      "a: -0.0045\n",
      "step:  431\n",
      "loss: 1.7385\n",
      "a: -0.0064\n",
      "step:  432\n",
      "loss: 1.3746\n",
      "a: -0.0080\n",
      "step:  433\n",
      "loss: 1.5746\n",
      "a: -0.0095\n",
      "step:  434\n",
      "loss: 1.8084\n",
      "a: -0.0109\n",
      "step:  435\n",
      "loss: 1.8677\n",
      "a: -0.0121\n",
      "step:  436\n",
      "loss: 1.7727\n",
      "a: -0.0132\n",
      "step:  437\n",
      "loss: 1.9030\n",
      "a: -0.0141\n",
      "step:  438\n",
      "loss: 2.2071\n",
      "a: -0.0150\n",
      "step:  439\n",
      "loss: 1.9243\n",
      "a: -0.0157\n",
      "step:  440\n",
      "loss: 2.1869\n",
      "a: -0.0162\n",
      "step:  441\n",
      "loss: 2.0041\n",
      "a: -0.0165\n",
      "step:  442\n",
      "loss: 2.3599\n",
      "a: -0.0167\n",
      "step:  443\n",
      "loss: 1.9475\n",
      "a: -0.0167\n",
      "step:  444\n",
      "loss: 1.9053\n",
      "a: -0.0165\n",
      "step:  445\n",
      "loss: 1.7523\n",
      "a: -0.0161\n",
      "step:  446\n",
      "loss: 2.0476\n",
      "a: -0.0155\n",
      "step:  447\n",
      "loss: 2.0135\n",
      "a: -0.0147\n",
      "step:  448\n",
      "loss: 1.8224\n",
      "a: -0.0137\n",
      "step:  449\n",
      "loss: 1.9502\n",
      "a: -0.0126\n",
      "step:  450\n",
      "loss: 1.6249\n",
      "a: -0.0115\n",
      "step:  451\n",
      "loss: 1.4393\n",
      "a: -0.0104\n",
      "step:  452\n",
      "loss: 1.7461\n",
      "a: -0.0092\n",
      "step:  453\n",
      "loss: 1.6359\n",
      "a: -0.0081\n",
      "step:  454\n",
      "loss: 1.6725\n",
      "a: -0.0069\n",
      "step:  455\n",
      "loss: 1.4999\n",
      "a: -0.0058\n",
      "step:  456\n",
      "loss: 1.6318\n",
      "a: -0.0047\n",
      "step:  457\n",
      "loss: 1.3595\n",
      "a: -0.0036\n",
      "step:  458\n",
      "loss: 1.4765\n",
      "a: -0.0026\n",
      "step:  459\n",
      "loss: 1.8408\n",
      "a: -0.0016\n",
      "step:  460\n",
      "loss: 1.6302\n",
      "a: -0.0006\n",
      "step:  461\n",
      "loss: 1.6242\n",
      "a: 0.0002\n",
      "step:  462\n",
      "loss: 1.9497\n",
      "a: 0.0004\n",
      "step:  463\n",
      "loss: 1.5056\n",
      "a: -0.0012\n",
      "step:  464\n",
      "loss: 1.4580\n",
      "a: -0.0025\n",
      "step:  465\n",
      "loss: 1.6800\n",
      "a: -0.0039\n",
      "step:  466\n",
      "loss: 1.5558\n",
      "a: -0.0052\n",
      "step:  467\n",
      "loss: 1.5933\n",
      "a: -0.0065\n",
      "step:  468\n",
      "loss: 1.5844\n",
      "a: -0.0077\n",
      "step:  469\n",
      "loss: 1.5516\n",
      "a: -0.0089\n",
      "step:  470\n",
      "loss: 1.4709\n",
      "a: -0.0100\n",
      "step:  471\n",
      "loss: 1.5399\n",
      "a: -0.0111\n",
      "step:  472\n",
      "loss: 1.5777\n",
      "a: -0.0121\n",
      "step:  473\n",
      "loss: 1.8503\n",
      "a: -0.0130\n",
      "step:  474\n",
      "loss: 1.8718\n",
      "a: -0.0139\n",
      "step:  475\n",
      "loss: 1.9781\n",
      "a: -0.0147\n",
      "step:  476\n",
      "loss: 1.9116\n",
      "a: -0.0153\n",
      "step:  477\n",
      "loss: 2.1566\n",
      "a: -0.0158\n",
      "step:  478\n",
      "loss: 2.1755\n",
      "a: -0.0161\n",
      "step:  479\n",
      "loss: 1.7344\n",
      "a: -0.0162\n",
      "step:  480\n",
      "loss: 2.3249\n",
      "a: -0.0162\n",
      "step:  481\n",
      "loss: 2.1301\n",
      "a: -0.0160\n",
      "step:  482\n",
      "loss: 1.9009\n",
      "a: -0.0156\n",
      "step:  483\n",
      "loss: 2.2466\n",
      "a: -0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  484\n",
      "loss: 1.8223\n",
      "a: -0.0142\n",
      "step:  485\n",
      "loss: 1.6479\n",
      "a: -0.0133\n",
      "step:  486\n",
      "loss: 1.8781\n",
      "a: -0.0122\n",
      "step:  487\n",
      "loss: 1.7053\n",
      "a: -0.0110\n",
      "step:  488\n",
      "loss: 1.6862\n",
      "a: -0.0098\n",
      "step:  489\n",
      "loss: 1.5665\n",
      "a: -0.0085\n",
      "step:  490\n",
      "loss: 1.5441\n",
      "a: -0.0072\n",
      "step:  491\n",
      "loss: 1.5239\n",
      "a: -0.0060\n",
      "step:  492\n",
      "loss: 1.6709\n",
      "a: -0.0048\n",
      "step:  493\n",
      "loss: 1.4837\n",
      "a: -0.0037\n",
      "step:  494\n",
      "loss: 1.5473\n",
      "a: -0.0026\n",
      "step:  495\n",
      "loss: 1.5519\n",
      "a: -0.0015\n",
      "step:  496\n",
      "loss: 1.8529\n",
      "a: -0.0005\n",
      "step:  497\n",
      "loss: 1.7404\n",
      "a: 0.0002\n",
      "step:  498\n",
      "loss: 1.6038\n",
      "a: -0.0001\n",
      "step:  499\n",
      "loss: 1.6841\n",
      "a: -0.0006\n",
      "step:  500\n",
      "loss: 1.5672\n",
      "a: -0.0014\n",
      "step:  501\n",
      "loss: 1.6632\n",
      "a: -0.0022\n",
      "step:  502\n",
      "loss: 1.6321\n",
      "a: -0.0032\n",
      "step:  503\n",
      "loss: 1.4383\n",
      "a: -0.0041\n",
      "step:  504\n",
      "loss: 1.6570\n",
      "a: -0.0051\n",
      "step:  505\n",
      "loss: 1.4947\n",
      "a: -0.0061\n",
      "step:  506\n",
      "loss: 1.6434\n",
      "a: -0.0070\n",
      "step:  507\n",
      "loss: 1.5852\n",
      "a: -0.0079\n",
      "step:  508\n",
      "loss: 1.6863\n",
      "a: -0.0088\n",
      "step:  509\n",
      "loss: 1.5260\n",
      "a: -0.0097\n",
      "step:  510\n",
      "loss: 1.6556\n",
      "a: -0.0104\n",
      "step:  511\n",
      "loss: 1.5281\n",
      "a: -0.0112\n",
      "step:  512\n",
      "loss: 1.5763\n",
      "a: -0.0118\n",
      "step:  513\n",
      "loss: 1.9250\n",
      "a: -0.0124\n",
      "step:  514\n",
      "loss: 1.7211\n",
      "a: -0.0128\n",
      "step:  515\n",
      "loss: 1.5164\n",
      "a: -0.0132\n",
      "step:  516\n",
      "loss: 1.9687\n",
      "a: -0.0133\n",
      "step:  517\n",
      "loss: 1.7745\n",
      "a: -0.0133\n",
      "step:  518\n",
      "loss: 1.6445\n",
      "a: -0.0129\n",
      "step:  519\n",
      "loss: 1.7095\n",
      "a: -0.0125\n",
      "step:  520\n",
      "loss: 1.7216\n",
      "a: -0.0118\n",
      "step:  521\n",
      "loss: 1.7386\n",
      "a: -0.0111\n",
      "step:  522\n",
      "loss: 1.8622\n",
      "a: -0.0101\n",
      "step:  523\n",
      "loss: 1.6694\n",
      "a: -0.0091\n",
      "step:  524\n",
      "loss: 1.6177\n",
      "a: -0.0080\n",
      "step:  525\n",
      "loss: 1.5338\n",
      "a: -0.0069\n",
      "step:  526\n",
      "loss: 1.5866\n",
      "a: -0.0057\n",
      "step:  527\n",
      "loss: 1.6463\n",
      "a: -0.0044\n",
      "step:  528\n",
      "loss: 1.5078\n",
      "a: -0.0032\n",
      "step:  529\n",
      "loss: 1.8282\n",
      "a: -0.0021\n",
      "step:  530\n",
      "loss: 1.5575\n",
      "a: -0.0011\n",
      "step:  531\n",
      "loss: 1.6924\n",
      "a: -0.0002\n",
      "step:  532\n",
      "loss: 2.4011\n",
      "a: 0.0006\n",
      "step:  533\n",
      "loss: 1.6422\n",
      "a: -0.0020\n",
      "step:  534\n",
      "loss: 1.4662\n",
      "a: -0.0041\n",
      "step:  535\n",
      "loss: 1.2930\n",
      "a: -0.0060\n",
      "step:  536\n",
      "loss: 1.5241\n",
      "a: -0.0076\n",
      "step:  537\n",
      "loss: 1.6975\n",
      "a: -0.0091\n",
      "step:  538\n",
      "loss: 1.4229\n",
      "a: -0.0104\n",
      "step:  539\n",
      "loss: 1.7580\n",
      "a: -0.0117\n",
      "step:  540\n",
      "loss: 1.7183\n",
      "a: -0.0129\n",
      "step:  541\n",
      "loss: 2.0004\n",
      "a: -0.0141\n",
      "step:  542\n",
      "loss: 2.1851\n",
      "a: -0.0152\n",
      "step:  543\n",
      "loss: 2.1807\n",
      "a: -0.0162\n",
      "step:  544\n",
      "loss: 2.2652\n",
      "a: -0.0171\n",
      "step:  545\n",
      "loss: 2.1195\n",
      "a: -0.0179\n",
      "step:  546\n",
      "loss: 2.0966\n",
      "a: -0.0187\n",
      "step:  547\n",
      "loss: 2.4934\n",
      "a: -0.0193\n",
      "step:  548\n",
      "loss: 2.6251\n",
      "a: -0.0199\n",
      "step:  549\n",
      "loss: 2.4641\n",
      "a: -0.0203\n",
      "step:  550\n",
      "loss: 2.4123\n",
      "a: -0.0205\n",
      "step:  551\n",
      "loss: 2.4635\n",
      "a: -0.0207\n",
      "step:  552\n",
      "loss: 2.4450\n",
      "a: -0.0207\n",
      "step:  553\n",
      "loss: 2.2607\n",
      "a: -0.0205\n",
      "step:  554\n",
      "loss: 2.2623\n",
      "a: -0.0201\n",
      "step:  555\n",
      "loss: 2.1236\n",
      "a: -0.0196\n",
      "step:  556\n",
      "loss: 1.9308\n",
      "a: -0.0189\n",
      "step:  557\n",
      "loss: 2.1605\n",
      "a: -0.0180\n",
      "step:  558\n",
      "loss: 2.3417\n",
      "a: -0.0170\n",
      "step:  559\n",
      "loss: 1.7416\n",
      "a: -0.0158\n",
      "step:  560\n",
      "loss: 1.9004\n",
      "a: -0.0146\n",
      "step:  561\n",
      "loss: 1.7424\n",
      "a: -0.0133\n",
      "step:  562\n",
      "loss: 1.6182\n",
      "a: -0.0120\n",
      "step:  563\n",
      "loss: 1.4844\n",
      "a: -0.0108\n",
      "step:  564\n",
      "loss: 1.7367\n",
      "a: -0.0095\n",
      "step:  565\n",
      "loss: 1.6736\n",
      "a: -0.0083\n",
      "step:  566\n",
      "loss: 1.8476\n",
      "a: -0.0071\n",
      "step:  567\n",
      "loss: 1.8394\n",
      "a: -0.0060\n",
      "step:  568\n",
      "loss: 1.6672\n",
      "a: -0.0048\n",
      "step:  569\n",
      "loss: 1.8599\n",
      "a: -0.0038\n",
      "step:  570\n",
      "loss: 2.3641\n",
      "a: -0.0028\n",
      "step:  571\n",
      "loss: 1.8664\n",
      "a: -0.0019\n",
      "step:  572\n",
      "loss: 1.6074\n",
      "a: -0.0010\n",
      "step:  573\n",
      "loss: 1.3696\n",
      "a: -0.0002\n",
      "step:  574\n",
      "loss: 2.2987\n",
      "a: 0.0005\n",
      "step:  575\n",
      "loss: 2.0533\n",
      "a: 0.0003\n",
      "step:  576\n",
      "loss: 1.4549\n",
      "a: -0.0008\n",
      "step:  577\n",
      "loss: 1.7564\n",
      "a: -0.0020\n",
      "step:  578\n",
      "loss: 1.6598\n",
      "a: -0.0032\n",
      "step:  579\n",
      "loss: 1.5558\n",
      "a: -0.0044\n",
      "step:  580\n",
      "loss: 1.6511\n",
      "a: -0.0056\n",
      "step:  581\n",
      "loss: 1.5704\n",
      "a: -0.0068\n",
      "step:  582\n",
      "loss: 1.4829\n",
      "a: -0.0080\n",
      "step:  583\n",
      "loss: 1.5268\n",
      "a: -0.0090\n",
      "step:  584\n",
      "loss: 1.5883\n",
      "a: -0.0101\n",
      "step:  585\n",
      "loss: 1.7334\n",
      "a: -0.0111\n",
      "step:  586\n",
      "loss: 2.0150\n",
      "a: -0.0121\n",
      "step:  587\n",
      "loss: 1.7514\n",
      "a: -0.0130\n",
      "step:  588\n",
      "loss: 2.1706\n",
      "a: -0.0139\n",
      "step:  589\n",
      "loss: 1.9335\n",
      "a: -0.0147\n",
      "step:  590\n",
      "loss: 2.0530\n",
      "a: -0.0154\n",
      "step:  591\n",
      "loss: 2.1806\n",
      "a: -0.0160\n",
      "step:  592\n",
      "loss: 2.2779\n",
      "a: -0.0165\n",
      "step:  593\n",
      "loss: 2.2316\n",
      "a: -0.0169\n",
      "step:  594\n",
      "loss: 2.4358\n",
      "a: -0.0172\n",
      "step:  595\n",
      "loss: 2.4354\n",
      "a: -0.0173\n",
      "step:  596\n",
      "loss: 2.0021\n",
      "a: -0.0173\n",
      "step:  597\n",
      "loss: 2.2460\n",
      "a: -0.0171\n",
      "step:  598\n",
      "loss: 1.9036\n",
      "a: -0.0167\n",
      "step:  599\n",
      "loss: 2.0858\n",
      "a: -0.0162\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code snippets to build the convolutional network:\n",
    "\n",
    "```python\n",
    "    from torch . nn . functional import conv2d , max_pool2d\n",
    "    convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "    subsampleing_layer = max_pool_2d ( convolutional_layer , (2 , 2) ) # reduces window 2x2 to 1 pixel\n",
    "    out_layer = dropout ( subsample_layer , p_drop_input )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  80\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "lr = 2e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "# modify for more speed\n",
    "f1, f2, f3 = 20, 40, 80\n",
    "pic_in1, pic_in2, pic_in3 = 1, 20, 40\n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        X = conv_layer(X, w_conv3, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, w_conv3, p_drop_input)\n",
    "    return Y.size()[1]\n",
    "\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, w_conv3, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([-0.1], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 1., 1.)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(train_loader, test_loader, num_epochs, log_interval):\n",
    "    # run training\n",
    "    t1 = time.time()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, epoch, log_interval)\n",
    "        test(test_loader)       \n",
    "    t2 = time.time()\n",
    "    print('Training took {:.1f} seconds (or {:.1f} minutes)'.format(t2 - t1, (t2 - t1) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 30.0028\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 9.7253\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 6.6157\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 3.2101\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.7628\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.2692\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 1456/10000 (14%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.6366\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.2841\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.2028\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.3096\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.2625\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.3449\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 1068/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.2836\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.2142\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.3127\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.2287\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.2821\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 2.2145\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 1059/10000 (10%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.2853\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 2.2563\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 2.1615\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 2.1911\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 2.0870\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 2.1436\n",
      "\n",
      "Test set: Average loss: 0.0410, Accuracy: 2872/10000 (28%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.1091\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.0247\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 2.1613\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.0517\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.8969\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 2.0623\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 3686/10000 (36%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.1061\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.9809\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.9810\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.8462\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.8995\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 2.0821\n",
      "\n",
      "Test set: Average loss: 0.0334, Accuracy: 3892/10000 (38%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.8623\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.8157\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.8083\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.8013\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.9421\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.9191\n",
      "\n",
      "Test set: Average loss: 0.0311, Accuracy: 4547/10000 (45%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.8805\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.7535\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.7669\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.7033\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.7209\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.9088\n",
      "\n",
      "Test set: Average loss: 0.0295, Accuracy: 5088/10000 (50%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.4914\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.6508\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.6114\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.7634\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.6147\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 2.2214\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 5135/10000 (51%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.4977\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.9266\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.8256\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.9486\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.7400\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.4921\n",
      "\n",
      "Test set: Average loss: 0.0274, Accuracy: 5307/10000 (53%)\n",
      "\n",
      "Training took 468.5 seconds (or 7.8 minutes)\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 10\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-03 *\n",
      "       [ 6.1090])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing a to the screen we see that it is indeed modified during training (initial value: a = -0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues:\n",
    " - find nice hyperparameters (learning rate, dropout, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4.2 Application of Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we sketch the network architecure in order to determine the number of neurons in the last layer. For this purpose we have to take into account the following:<br>\n",
    "<ul>\n",
    "<li>Convolving an (n x n) image with a (5 x 5) filter without padding produces an output of dimension (n-4 x n-4) since we lose 2 pixels on each side of the image.</li>\n",
    "<li>Applying a (2 x 2) max pooling layer on  a (n x n) image gives an (n/2 x n/2) output.</li>\n",
    "<li>Convolving an (n x n) image with a (2 x 2) filter witout padding produces an output of dimension (n-1 x n-1).</li>\n",
    "<li>In the last step, we apply a (2 x 2) pooling operation on a (3 x 3) image where the dimensions of filter and image do not really match. Nevertheless we obtain a (1 x 1) output, but maybe, it would be more convenient to apply a (3 x 3) filter in the previous layer such that we could apply the (2 x 2) pooling to a (2 x 2) image?</li>\n",
    "</ul>\n",
    "Taking all this into account, we obtain the architecture displayed below and can read off that the last convolutional layer contains (128 x 1 x 1) = 128 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'architecture.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-dc8d215d1769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marchitecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'architecture.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_dedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'architecture.png'"
     ]
    }
   ],
   "source": [
    "architecture = plt.imread('architecture.png')\n",
    "fig, ax = plt.subplots(figsize=(40,12))\n",
    "ax.imshow(architecture)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize one image of the test set, its convolution with 3 filters of the first convolutional layer and the corresponding filter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9fa0827a20>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOqvdZo/UF0J0EAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9faaa8bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 11\n",
    "selected_image = testset[index]\n",
    "plt.imshow(selected_image[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 3, 24, 24])\n",
      "torch.Size([20, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "filters_of_interest = torch.tensor([1,2,3])\n",
    "print(selected_image[0].size())\n",
    "feature_maps = conv2d(selected_image[0].unsqueeze(0), w_conv1[filters_of_interest,:,:,:])\n",
    "print(feature_maps.size())\n",
    "feature_maps = feature_maps.detach()  # detach from comp. graph\n",
    "filters = w_conv1.detach()\n",
    "print(filters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAJCCAYAAAA2rjfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACA4SkwAAAAgOH9H3Czn1qy5N2GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9fa09ebeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(2,3, figsize=(20,10))\n",
    "for i in range(3):\n",
    "    ax1[0,i].imshow(feature_maps[0,i,:,:])\n",
    "    ax1[1,i].imshow(filters[filters_of_interest[i]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upper row we show the feature maps which have been obtained by convolving the input image with the three filters shown in the lower row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For comparison: feature maps and filters with no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAJCCAYAAAA2rjfmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncmNo7VnxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9fa08db080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "filters_of_interest = torch.tensor([1,2,3])\n",
    "feature_maps = conv2d(selected_image[0].unsqueeze(0), w_conv1[filters_of_interest,:,:,:])\n",
    "feature_maps = feature_maps.detach()  # detach from comp. graph\n",
    "filters = w_conv1.detach()\n",
    "\n",
    "fig1, ax1 = plt.subplots(2,3, figsize=(20,10))\n",
    "for i in range(3):\n",
    "    ax1[0,i].imshow(feature_maps[0,i,:,:])\n",
    "    ax1[1,i].imshow(filters[filters_of_interest[i]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different network architecture\n",
    "\n",
    "### 1. add additional convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  64\n"
     ]
    }
   ],
   "source": [
    "lr = 2e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "# modify for more speed\n",
    "f1, f2, f3, f4 = 32, 32, 32, 64\n",
    "pic_in1, pic_in2, pic_in3, pic_in4 = 1, 32, 32, 32\n",
    "k_x1, k_x2, k_x3, k_x4 = 3, 3, 2, 2\n",
    "k_y1, k_y2, k_y3, k_y4 = 3, 3, 2, 2\n",
    "\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "w_conv4 = init_weights((f4, pic_in4, k_x4, k_y4))\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, w_conv3, w_conv4, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, w_conv3, w_conv4, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        X = conv_layer(X, w_conv3, p_drop_input)\n",
    "        # do not apply max_pooling after last layer (input dimension too low)\n",
    "        X = rectify(conv2d (X, w_conv4))\n",
    "        X = dropout(X, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, w_conv3, w_conv4, p_drop_input)\n",
    "    return Y.size()[1]\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, w_conv3, w_conv4, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([-0.1], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_conv4, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_conv4, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = rectify(conv2d (X, w_conv4))\n",
    "    X = dropout(X, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.9090\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 3.0006\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.4719\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.3394\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.3524\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.2874\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 944/10000 (9%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.3524\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.3074\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.3076\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.2982\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.2837\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.3021\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 980/10000 (9%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.3055\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.3026\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.3026\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.3026\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.3026\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 2.3026\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 980/10000 (9%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.3026\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 2.3026\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 2.3026\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 2.3026\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 2.3026\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 2.3026\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 980/10000 (9%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.3026\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.3026\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 2.3026\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.3026\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 2.3026\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 2.3026\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 10\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> We do not any observe significant imporovement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Anistotropic filters\n",
    "Then we try to use anisotropic filters with $k_x \\neq k_y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

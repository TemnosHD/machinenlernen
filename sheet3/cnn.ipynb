{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 50 # mini-batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                 (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in trainset and testset. The trainset consists of 60000 images, the testset of 10000 imgs.\n",
    "\n",
    "trainset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "testset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classnames = [str(i) for i in range(10)]\n",
    "\n",
    "def imshow(img, title=\"\", cmap = \"Greys_r\"): #convert tensor to image\n",
    "    plt.title(title)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    numpyimg = img.numpy()[0]\n",
    "    plt.imshow(numpyimg, cmap = cmap)\n",
    "    \n",
    "\n",
    "def display_10_images_from_dataset(dataset, class_names):\n",
    "    \"\"\"\n",
    "    plots 10 randomly chosen images from a given dataset\n",
    "    \"\"\"\n",
    "    display = [] #holds tuples of image and respective label\n",
    "    for _ in range(10):\n",
    "        index = np.random.randint(0,len(dataset)+1)\n",
    "        display.append(dataset[index])\n",
    "    \n",
    "    nr = 1\n",
    "    fig, axes = plt.subplots(2,5,sharex='col',sharey='row', figsize = (14,9))\n",
    "    for image, label in display:\n",
    "        axes[(nr-1)//5][(nr-1)%5] = plt.subplot(2,5,nr)\n",
    "        plt.title(class_names[label], fontsize = 16)\n",
    "        imshow(image, title = class_names[label])\n",
    "        nr+=1\n",
    "    fig.subplots_adjust(hspace=-0.4)\n",
    "    plt.setp([a.get_xticklabels() for a in fig.axes[0:5]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in fig.axes[1:5]+fig.axes[6:]], visible=False)\n",
    "    plt.show()\n",
    "    plt.savefig(\"previewMNIST.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAFtCAYAAADccl8mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzv8HlHZ3/xRGAFAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8620f66208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f85bccf5470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_10_images_from_dataset(testset, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4616\n",
      "Loss: 2.2623\n",
      "Loss: 2.2687\n",
      "Loss: 2.3062\n",
      "Loss: 1.9459\n",
      "Loss: 1.8585\n",
      "Loss: 1.7999\n",
      "Loss: 1.8672\n",
      "Loss: 1.5682\n",
      "Loss: 1.6160\n",
      "Loss: 1.5095\n",
      "Loss: 1.4167\n",
      "Loss: 1.3357\n",
      "Loss: 1.2428\n",
      "Loss: 1.2929\n",
      "Loss: 1.2513\n",
      "Loss: 1.0737\n",
      "Loss: 1.0431\n",
      "Loss: 0.9244\n",
      "Loss: 0.8819\n",
      "Loss: 0.8752\n",
      "Loss: 0.9034\n",
      "Loss: 0.9561\n",
      "Loss: 0.9420\n",
      "Loss: 0.6420\n",
      "Loss: 0.8193\n",
      "Loss: 0.9044\n",
      "Loss: 0.8192\n",
      "Loss: 0.6635\n",
      "Loss: 0.9786\n",
      "Loss: 0.8285\n",
      "Loss: 0.8810\n",
      "Loss: 0.7102\n",
      "Loss: 0.8722\n",
      "Loss: 0.7622\n",
      "Loss: 0.9330\n",
      "Loss: 0.8628\n",
      "Loss: 0.5893\n",
      "Loss: 0.8791\n",
      "Loss: 0.8498\n",
      "Loss: 0.6238\n",
      "Loss: 0.8118\n",
      "Loss: 0.6374\n",
      "Loss: 0.6403\n",
      "Loss: 0.5777\n",
      "Loss: 0.6154\n",
      "Loss: 0.8124\n",
      "Loss: 0.7551\n",
      "Loss: 0.4743\n",
      "Loss: 0.9171\n",
      "Loss: 0.6680\n",
      "Loss: 0.4804\n",
      "Loss: 0.4202\n",
      "Loss: 0.7785\n",
      "Loss: 0.7798\n",
      "Loss: 0.6439\n",
      "Loss: 0.7925\n",
      "Loss: 0.5243\n",
      "Loss: 0.8232\n",
      "Loss: 1.1416\n",
      "Loss: 0.9142\n",
      "Loss: 0.5445\n",
      "Loss: 0.5664\n",
      "Loss: 0.5694\n",
      "Loss: 0.7296\n",
      "Loss: 0.7483\n",
      "Loss: 0.6519\n",
      "Loss: 0.6084\n",
      "Loss: 0.5755\n",
      "Loss: 0.6159\n",
      "Loss: 0.7813\n",
      "Loss: 0.8101\n",
      "Loss: 0.8068\n",
      "Loss: 1.1041\n",
      "Loss: 0.6524\n",
      "Loss: 0.7702\n",
      "Loss: 0.5993\n",
      "Loss: 0.5065\n",
      "Loss: 0.6113\n",
      "Loss: 0.6083\n",
      "Loss: 0.7676\n",
      "Loss: 0.5566\n",
      "Loss: 0.8223\n",
      "Loss: 0.8087\n",
      "Loss: 0.4615\n",
      "Loss: 0.7688\n",
      "Loss: 0.8530\n",
      "Loss: 0.6136\n",
      "Loss: 0.3724\n",
      "Loss: 0.5497\n",
      "Loss: 0.6407\n",
      "Loss: 0.4824\n",
      "Loss: 0.5681\n",
      "Loss: 0.4612\n",
      "Loss: 0.4049\n",
      "Loss: 0.5048\n",
      "Loss: 0.5059\n",
      "Loss: 0.3631\n",
      "Loss: 0.4935\n",
      "Loss: 0.4644\n",
      "Loss: 0.4627\n",
      "Loss: 0.5506\n",
      "Loss: 0.3552\n",
      "Loss: 0.3457\n",
      "Loss: 0.4223\n",
      "Loss: 0.3638\n",
      "Loss: 0.4555\n",
      "Loss: 0.5683\n",
      "Loss: 0.5084\n",
      "Loss: 0.6139\n",
      "Loss: 0.3561\n",
      "Loss: 0.3619\n",
      "Loss: 0.5608\n",
      "Loss: 0.4616\n",
      "Loss: 0.3960\n",
      "Loss: 0.5241\n",
      "Loss: 0.6519\n",
      "Loss: 0.4764\n",
      "Loss: 0.4469\n",
      "Loss: 0.3596\n",
      "Loss: 0.5653\n",
      "Loss: 0.5156\n",
      "Loss: 0.7987\n",
      "Loss: 0.6775\n",
      "Loss: 0.8012\n",
      "Loss: 0.4932\n",
      "Loss: 0.5695\n",
      "Loss: 0.5412\n",
      "Loss: 0.3161\n",
      "Loss: 0.4107\n",
      "Loss: 0.3143\n",
      "Loss: 0.6117\n",
      "Loss: 0.5519\n",
      "Loss: 0.4310\n",
      "Loss: 0.4528\n",
      "Loss: 0.6208\n",
      "Loss: 0.6337\n",
      "Loss: 0.5806\n",
      "Loss: 0.6459\n",
      "Loss: 0.5501\n",
      "Loss: 1.0889\n",
      "Loss: 0.6188\n",
      "Loss: 0.3678\n",
      "Loss: 0.6154\n",
      "Loss: 0.5195\n",
      "Loss: 0.6008\n",
      "Loss: 0.5994\n",
      "Loss: 0.4818\n",
      "Loss: 0.3397\n",
      "Loss: 0.5009\n",
      "Loss: 0.5956\n",
      "Loss: 0.6631\n",
      "Loss: 0.3033\n",
      "Loss: 0.3443\n",
      "Loss: 0.3661\n",
      "Loss: 0.5735\n",
      "Loss: 0.8253\n",
      "Loss: 0.5399\n",
      "Loss: 0.2460\n",
      "Loss: 0.6103\n",
      "Loss: 0.5278\n",
      "Loss: 0.3016\n",
      "Loss: 0.7616\n",
      "Loss: 0.3581\n",
      "Loss: 0.6241\n",
      "Loss: 0.6071\n",
      "Loss: 0.2660\n",
      "Loss: 0.5084\n",
      "Loss: 0.6929\n",
      "Loss: 0.6686\n",
      "Loss: 0.5319\n",
      "Loss: 0.5198\n",
      "Loss: 0.4971\n",
      "Loss: 0.3130\n",
      "Loss: 0.3810\n",
      "Loss: 0.4776\n",
      "Loss: 0.3751\n",
      "Loss: 0.5262\n",
      "Loss: 0.5847\n",
      "Loss: 0.4147\n",
      "Loss: 1.0081\n",
      "Loss: 0.6562\n",
      "Loss: 0.7714\n",
      "Loss: 0.4018\n",
      "Loss: 0.3387\n",
      "Loss: 0.5060\n",
      "Loss: 0.6864\n",
      "Loss: 0.3463\n",
      "Loss: 0.9258\n",
      "Loss: 0.5689\n",
      "Loss: 0.5029\n",
      "Loss: 0.2791\n",
      "Loss: 0.5064\n",
      "Loss: 0.7162\n",
      "Loss: 0.9503\n",
      "Loss: 0.7706\n",
      "Loss: 0.5334\n",
      "Loss: 0.7142\n",
      "Loss: 0.5468\n",
      "Loss: 0.4680\n",
      "Loss: 0.5346\n",
      "Loss: 0.3577\n",
      "Loss: 0.4539\n",
      "Loss: 0.5988\n",
      "Loss: 0.8486\n",
      "Loss: 0.5251\n",
      "Loss: 0.3513\n",
      "Loss: 0.8071\n",
      "Loss: 0.3385\n",
      "Loss: 0.3458\n",
      "Loss: 0.7672\n",
      "Loss: 0.5986\n",
      "Loss: 0.3748\n",
      "Loss: 0.5346\n",
      "Loss: 0.6651\n",
      "Loss: 0.7060\n",
      "Loss: 0.3089\n",
      "Loss: 0.3123\n",
      "Loss: 0.4823\n",
      "Loss: 0.5656\n",
      "Loss: 0.5826\n",
      "Loss: 0.6617\n",
      "Loss: 0.7189\n",
      "Loss: 0.5287\n",
      "Loss: 1.0457\n",
      "Loss: 0.5443\n",
      "Loss: 0.5592\n",
      "Loss: 0.4268\n",
      "Loss: 0.5496\n",
      "Loss: 0.8281\n",
      "Loss: 0.2701\n",
      "Loss: 0.4742\n",
      "Loss: 0.7734\n",
      "Loss: 0.5355\n",
      "Loss: 0.5746\n",
      "Loss: 0.5162\n",
      "Loss: 0.5794\n",
      "Loss: 0.4154\n",
      "Loss: 0.5395\n",
      "Loss: 0.4341\n",
      "Loss: 0.3951\n",
      "Loss: 0.7457\n",
      "Loss: 0.5920\n",
      "Loss: 0.5922\n",
      "Loss: 0.5333\n",
      "Loss: 0.3329\n",
      "Loss: 0.9013\n",
      "Loss: 0.3802\n",
      "Loss: 0.5442\n",
      "Loss: 0.4862\n",
      "Loss: 0.4633\n",
      "Loss: 0.5156\n",
      "Loss: 0.7939\n",
      "Loss: 0.7227\n",
      "Loss: 0.4730\n",
      "Loss: 0.3706\n",
      "Loss: 0.6891\n",
      "Loss: 0.4992\n",
      "Loss: 0.5920\n",
      "Loss: 0.4109\n",
      "Loss: 0.5846\n",
      "Loss: 0.4111\n",
      "Loss: 0.4905\n",
      "Loss: 0.4662\n",
      "Loss: 0.4490\n",
      "Loss: 0.3806\n",
      "Loss: 0.3043\n",
      "Loss: 0.2153\n",
      "Loss: 0.4967\n",
      "Loss: 0.6346\n",
      "Loss: 0.4570\n",
      "Loss: 0.6542\n",
      "Loss: 0.4453\n",
      "Loss: 0.4115\n",
      "Loss: 0.3210\n",
      "Loss: 0.3969\n",
      "Loss: 0.4671\n",
      "Loss: 0.4463\n",
      "Loss: 0.2722\n",
      "Loss: 0.6704\n",
      "Loss: 0.5603\n",
      "Loss: 0.4002\n",
      "Loss: 0.6814\n",
      "Loss: 0.3246\n",
      "Loss: 0.3658\n",
      "Loss: 0.4499\n",
      "Loss: 0.6866\n",
      "Loss: 0.5176\n",
      "Loss: 0.5008\n",
      "Loss: 0.3147\n",
      "Loss: 0.5580\n",
      "Loss: 0.5748\n",
      "Loss: 0.3781\n",
      "Loss: 0.2691\n",
      "Loss: 0.5608\n",
      "Loss: 0.4585\n",
      "Loss: 0.6194\n",
      "Loss: 0.3689\n",
      "Loss: 0.4253\n",
      "Loss: 0.3593\n",
      "Loss: 0.6162\n",
      "Loss: 0.3170\n",
      "Loss: 0.5060\n",
      "Loss: 0.4654\n",
      "Loss: 0.4376\n",
      "Loss: 0.8769\n",
      "Loss: 0.3793\n",
      "Loss: 0.3859\n",
      "Loss: 0.5717\n",
      "Loss: 0.3544\n",
      "Loss: 0.6459\n",
      "Loss: 0.4938\n",
      "Loss: 0.2960\n",
      "Loss: 0.4165\n",
      "Loss: 0.3877\n",
      "Loss: 0.3235\n",
      "Loss: 0.5462\n",
      "Loss: 0.5003\n",
      "Loss: 0.3738\n",
      "Loss: 0.4763\n",
      "Loss: 0.3834\n",
      "Loss: 0.5366\n",
      "Loss: 0.4890\n",
      "Loss: 0.5048\n",
      "Loss: 0.3665\n",
      "Loss: 0.5615\n",
      "Loss: 0.4951\n",
      "Loss: 0.3332\n",
      "Loss: 0.5571\n",
      "Loss: 0.3276\n",
      "Loss: 0.4268\n",
      "Loss: 0.5432\n",
      "Loss: 0.3775\n",
      "Loss: 0.4099\n",
      "Loss: 0.5498\n",
      "Loss: 0.5800\n",
      "Loss: 0.5582\n",
      "Loss: 0.3626\n",
      "Loss: 0.5827\n",
      "Loss: 0.3552\n",
      "Loss: 0.7910\n",
      "Loss: 0.4607\n",
      "Loss: 0.4372\n",
      "Loss: 0.2572\n",
      "Loss: 0.6701\n",
      "Loss: 0.5336\n",
      "Loss: 0.6299\n",
      "Loss: 0.2954\n",
      "Loss: 0.4850\n",
      "Loss: 0.4365\n",
      "Loss: 0.3882\n",
      "Loss: 0.4287\n",
      "Loss: 0.5315\n",
      "Loss: 0.6359\n",
      "Loss: 0.4263\n",
      "Loss: 0.4065\n",
      "Loss: 0.3427\n",
      "Loss: 0.5349\n",
      "Loss: 0.3398\n",
      "Loss: 0.4635\n",
      "Loss: 0.5358\n",
      "Loss: 0.4481\n",
      "Loss: 0.3800\n",
      "Loss: 0.5976\n",
      "Loss: 0.4899\n",
      "Loss: 0.4744\n",
      "Loss: 0.3063\n",
      "Loss: 0.5163\n",
      "Loss: 0.4555\n",
      "Loss: 0.5230\n",
      "Loss: 0.3794\n",
      "Loss: 0.7661\n",
      "Loss: 0.4034\n",
      "Loss: 0.4500\n",
      "Loss: 0.4440\n",
      "Loss: 0.4394\n",
      "Loss: 0.4078\n",
      "Loss: 0.5002\n",
      "Loss: 0.3511\n",
      "Loss: 0.4337\n",
      "Loss: 0.2991\n",
      "Loss: 0.7771\n",
      "Loss: 0.3169\n",
      "Loss: 0.2788\n",
      "Loss: 0.7220\n",
      "Loss: 0.2496\n",
      "Loss: 0.7759\n",
      "Loss: 0.4751\n",
      "Loss: 0.4411\n",
      "Loss: 0.4566\n",
      "Loss: 0.4143\n",
      "Loss: 0.4321\n",
      "Loss: 0.4727\n",
      "Loss: 0.5029\n",
      "Loss: 0.5534\n",
      "Loss: 0.3229\n",
      "Loss: 0.5087\n",
      "Loss: 0.4032\n",
      "Loss: 0.3273\n",
      "Loss: 0.3290\n",
      "Loss: 0.3601\n",
      "Loss: 0.4091\n",
      "Loss: 0.3301\n",
      "Loss: 0.5504\n",
      "Loss: 0.3214\n",
      "Loss: 0.4117\n",
      "Loss: 0.4683\n",
      "Loss: 0.5451\n",
      "Loss: 0.5662\n",
      "Loss: 0.4219\n",
      "Loss: 0.4620\n",
      "Loss: 0.4889\n",
      "Loss: 0.4099\n",
      "Loss: 0.3905\n",
      "Loss: 0.2928\n",
      "Loss: 0.2827\n",
      "Loss: 0.4515\n",
      "Loss: 0.3770\n",
      "Loss: 0.7293\n",
      "Loss: 0.6723\n",
      "Loss: 0.5357\n",
      "Loss: 0.4687\n",
      "Loss: 0.7209\n",
      "Loss: 0.4211\n",
      "Loss: 0.5916\n",
      "Loss: 0.3764\n",
      "Loss: 0.6307\n",
      "Loss: 0.4605\n",
      "Loss: 0.5263\n",
      "Loss: 0.3403\n",
      "Loss: 0.5405\n",
      "Loss: 0.2212\n",
      "Loss: 0.2955\n",
      "Loss: 0.5630\n",
      "Loss: 0.5846\n",
      "Loss: 0.4816\n",
      "Loss: 0.8059\n",
      "Loss: 0.4175\n",
      "Loss: 0.4718\n",
      "Loss: 0.4672\n",
      "Loss: 0.3800\n",
      "Loss: 0.5999\n",
      "Loss: 0.5009\n",
      "Loss: 0.4884\n",
      "Loss: 0.4238\n",
      "Loss: 0.3324\n",
      "Loss: 0.2835\n",
      "Loss: 0.2854\n",
      "Loss: 0.3697\n",
      "Loss: 0.2945\n",
      "Loss: 0.4416\n",
      "Loss: 0.3408\n",
      "Loss: 0.2571\n",
      "Loss: 0.3184\n",
      "Loss: 0.5421\n",
      "Loss: 0.3616\n",
      "Loss: 0.4156\n",
      "Loss: 0.3264\n",
      "Loss: 0.5469\n",
      "Loss: 0.3640\n",
      "Loss: 0.1283\n",
      "Loss: 0.2844\n",
      "Loss: 0.3389\n",
      "Loss: 0.6840\n",
      "Loss: 0.3857\n",
      "Loss: 0.3060\n",
      "Loss: 0.4610\n",
      "Loss: 0.4309\n",
      "Loss: 0.3358\n",
      "Loss: 0.3594\n",
      "Loss: 0.2930\n",
      "Loss: 0.2050\n",
      "Loss: 0.6327\n",
      "Loss: 0.3841\n",
      "Loss: 0.3991\n",
      "Loss: 0.6249\n",
      "Loss: 0.7920\n",
      "Loss: 0.8226\n",
      "Loss: 0.2881\n",
      "Loss: 0.4108\n",
      "Loss: 0.2964\n",
      "Loss: 0.4324\n",
      "Loss: 0.7763\n",
      "Loss: 0.5533\n",
      "Loss: 0.5627\n",
      "Loss: 0.2879\n",
      "Loss: 0.3243\n",
      "Loss: 0.3827\n",
      "Loss: 0.2166\n",
      "Loss: 0.2598\n",
      "Loss: 0.3612\n",
      "Loss: 0.4857\n",
      "Loss: 0.6551\n",
      "Loss: 0.3499\n",
      "Loss: 0.3770\n",
      "Loss: 0.4011\n",
      "Loss: 0.3225\n",
      "Loss: 0.6388\n",
      "Loss: 0.2935\n",
      "Loss: 0.2533\n",
      "Loss: 0.5639\n",
      "Loss: 0.6246\n",
      "Loss: 0.5892\n",
      "Loss: 0.4082\n",
      "Loss: 0.7108\n",
      "Loss: 0.4996\n",
      "Loss: 0.4141\n",
      "Loss: 0.3437\n",
      "Loss: 0.3003\n",
      "Loss: 0.2630\n",
      "Loss: 0.2989\n",
      "Loss: 0.4879\n",
      "Loss: 0.4748\n",
      "Loss: 0.2661\n",
      "Loss: 0.2179\n",
      "Loss: 0.5489\n",
      "Loss: 0.6702\n",
      "Loss: 0.4912\n",
      "Loss: 0.5852\n",
      "Loss: 0.5248\n",
      "Loss: 0.4798\n",
      "Loss: 0.3958\n",
      "Loss: 0.4397\n",
      "Loss: 0.2556\n",
      "Loss: 0.6624\n",
      "Loss: 0.3231\n",
      "Loss: 0.3485\n",
      "Loss: 0.4423\n",
      "Loss: 0.3493\n",
      "Loss: 0.3847\n",
      "Loss: 0.5707\n",
      "Loss: 0.4374\n",
      "Loss: 0.6499\n",
      "Loss: 0.4231\n",
      "Loss: 0.3916\n",
      "Loss: 0.2718\n",
      "Loss: 0.3710\n",
      "Loss: 0.3987\n",
      "Loss: 0.6048\n",
      "Loss: 0.2581\n",
      "Loss: 0.3456\n",
      "Loss: 0.3289\n",
      "Loss: 0.3561\n",
      "Loss: 0.3654\n",
      "Loss: 0.5201\n",
      "Loss: 0.2678\n",
      "Loss: 0.3805\n",
      "Loss: 0.1776\n",
      "Loss: 0.5063\n",
      "Loss: 0.3214\n",
      "Loss: 0.3035\n",
      "Loss: 0.4356\n",
      "Loss: 0.6392\n",
      "Loss: 0.5581\n",
      "Loss: 0.4488\n",
      "Loss: 0.3308\n",
      "Loss: 0.5436\n",
      "Loss: 0.4916\n",
      "Loss: 0.8371\n",
      "Loss: 0.4399\n",
      "Loss: 0.7166\n",
      "Loss: 0.3481\n",
      "Loss: 0.5592\n",
      "Loss: 0.1957\n",
      "Loss: 0.6562\n",
      "Loss: 0.2271\n",
      "Loss: 0.3201\n",
      "Loss: 0.4854\n",
      "Loss: 0.5390\n",
      "Loss: 0.3106\n",
      "Loss: 0.6640\n",
      "Loss: 0.4056\n",
      "Loss: 0.4467\n",
      "Loss: 0.4425\n",
      "Loss: 0.3170\n",
      "Loss: 0.4059\n",
      "Loss: 0.2320\n",
      "Loss: 0.3860\n",
      "Loss: 0.4788\n",
      "Loss: 0.4850\n",
      "Loss: 0.3389\n",
      "Loss: 0.4528\n",
      "Loss: 0.4506\n",
      "Loss: 0.4980\n",
      "Loss: 0.3779\n",
      "Loss: 0.7441\n",
      "Loss: 0.6545\n",
      "Loss: 0.3650\n",
      "Loss: 0.2083\n",
      "Loss: 0.5279\n",
      "Loss: 0.5429\n",
      "Loss: 0.2747\n",
      "Loss: 0.5620\n",
      "Loss: 0.4168\n",
      "Loss: 0.4494\n",
      "Loss: 0.4217\n",
      "Loss: 0.2789\n",
      "Loss: 0.3679\n",
      "Loss: 0.4033\n",
      "Loss: 0.3569\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print('Loss: %.4f' % cost)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout1(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.tensor(np.random.binomial(1, p_drop, X.size())).float()\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()\n",
    "\n",
    "def dropout(X, p_drop=1.):\n",
    "    if 0 < p_drop < 1:\n",
    "        phi = torch.bernoulli(torch.full(X.shape, p_drop))\n",
    "        X = phi*X/p_drop\n",
    "        return X.float()\n",
    "    else:\n",
    "        return X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.235685110092163\n",
      "Loss: 2.6122305393218994\n",
      "Loss: 2.633882761001587\n",
      "Loss: 2.463536262512207\n",
      "Loss: 2.253995418548584\n",
      "Loss: 2.227691173553467\n",
      "Loss: 2.2037668228149414\n",
      "Loss: 2.2376036643981934\n",
      "Loss: 2.2737526893615723\n",
      "Loss: 2.1525075435638428\n",
      "Loss: 2.1420462131500244\n",
      "Loss: 2.104735851287842\n",
      "Loss: 2.2754225730895996\n",
      "Loss: 2.1633718013763428\n",
      "Loss: 2.124685525894165\n",
      "Loss: 2.085428237915039\n",
      "Loss: 2.080772876739502\n",
      "Loss: 1.9635969400405884\n",
      "Loss: 1.9688693284988403\n",
      "Loss: 2.071608543395996\n",
      "Loss: 1.9973366260528564\n",
      "Loss: 1.8948277235031128\n",
      "Loss: 1.8804954290390015\n",
      "Loss: 1.899318814277649\n",
      "Loss: 1.8611576557159424\n",
      "Loss: 1.9064974784851074\n",
      "Loss: 1.904478907585144\n",
      "Loss: 1.9299267530441284\n",
      "Loss: 1.7353285551071167\n",
      "Loss: 1.746910572052002\n",
      "Loss: 1.9866044521331787\n",
      "Loss: 1.872605323791504\n",
      "Loss: 1.9907395839691162\n",
      "Loss: 1.8297711610794067\n",
      "Loss: 1.760430932044983\n",
      "Loss: 1.9318454265594482\n",
      "Loss: 1.8175140619277954\n",
      "Loss: 1.8376332521438599\n",
      "Loss: 1.6989854574203491\n",
      "Loss: 1.582572340965271\n",
      "Loss: 1.8445755243301392\n",
      "Loss: 1.7279452085494995\n",
      "Loss: 1.8538498878479004\n",
      "Loss: 1.6030325889587402\n",
      "Loss: 1.6972392797470093\n",
      "Loss: 1.833166480064392\n",
      "Loss: 1.6762057542800903\n",
      "Loss: 1.7540795803070068\n",
      "Loss: 1.7157599925994873\n",
      "Loss: 1.5768641233444214\n",
      "Loss: 1.4103089570999146\n",
      "Loss: 1.5442253351211548\n",
      "Loss: 1.577795386314392\n",
      "Loss: 1.6203781366348267\n",
      "Loss: 1.6437221765518188\n",
      "Loss: 1.670914888381958\n",
      "Loss: 1.6315300464630127\n",
      "Loss: 1.7126332521438599\n",
      "Loss: 1.8169559240341187\n",
      "Loss: 1.4232901334762573\n",
      "Loss: 1.5889582633972168\n",
      "Loss: 1.4107666015625\n",
      "Loss: 1.5896573066711426\n",
      "Loss: 1.4864424467086792\n",
      "Loss: 1.431215524673462\n",
      "Loss: 1.6170637607574463\n",
      "Loss: 1.5263373851776123\n",
      "Loss: 1.5201152563095093\n",
      "Loss: 1.66840398311615\n",
      "Loss: 1.4270063638687134\n",
      "Loss: 1.4221171140670776\n",
      "Loss: 1.619748830795288\n",
      "Loss: 1.5797163248062134\n",
      "Loss: 1.518632411956787\n",
      "Loss: 1.5847445726394653\n",
      "Loss: 1.669315218925476\n",
      "Loss: 1.4403001070022583\n",
      "Loss: 1.5390825271606445\n",
      "Loss: 1.648163914680481\n",
      "Loss: 1.5826245546340942\n",
      "Loss: 1.3668900728225708\n",
      "Loss: 1.497296929359436\n",
      "Loss: 1.6096783876419067\n",
      "Loss: 1.7891534566879272\n",
      "Loss: 1.590016484260559\n",
      "Loss: 1.3957737684249878\n",
      "Loss: 1.5452258586883545\n",
      "Loss: 1.5920830965042114\n",
      "Loss: 1.62259840965271\n",
      "Loss: 1.413475751876831\n",
      "Loss: 1.4523564577102661\n",
      "Loss: 1.3758866786956787\n",
      "Loss: 1.5771470069885254\n",
      "Loss: 1.4463021755218506\n",
      "Loss: 1.3915131092071533\n",
      "Loss: 1.313422679901123\n",
      "Loss: 1.5220309495925903\n",
      "Loss: 1.5391067266464233\n",
      "Loss: 1.5877798795700073\n",
      "Loss: 1.497165322303772\n",
      "Loss: 1.3383429050445557\n",
      "Loss: 1.379634976387024\n",
      "Loss: 1.3977731466293335\n",
      "Loss: 1.4110832214355469\n",
      "Loss: 1.4959138631820679\n",
      "Loss: 1.4388912916183472\n",
      "Loss: 1.1972342729568481\n",
      "Loss: 1.4660602807998657\n",
      "Loss: 1.4974255561828613\n",
      "Loss: 1.2635573148727417\n",
      "Loss: 1.6518816947937012\n",
      "Loss: 1.2052518129348755\n",
      "Loss: 1.3269137144088745\n",
      "Loss: 1.4603450298309326\n",
      "Loss: 1.7873073816299438\n",
      "Loss: 1.3309401273727417\n",
      "Loss: 1.2543457746505737\n",
      "Loss: 1.335268497467041\n",
      "Loss: 1.3518387079238892\n",
      "Loss: 1.397591233253479\n",
      "Loss: 1.6119184494018555\n",
      "Loss: 1.6606438159942627\n",
      "Loss: 1.3041276931762695\n",
      "Loss: 1.3731377124786377\n",
      "Loss: 1.409706711769104\n",
      "Loss: 1.4399464130401611\n",
      "Loss: 1.3342857360839844\n",
      "Loss: 1.457720160484314\n",
      "Loss: 1.2296181917190552\n",
      "Loss: 1.4223198890686035\n",
      "Loss: 1.383384108543396\n",
      "Loss: 1.2598140239715576\n",
      "Loss: 1.2187786102294922\n",
      "Loss: 1.2607779502868652\n",
      "Loss: 1.0820035934448242\n",
      "Loss: 1.6282519102096558\n",
      "Loss: 1.3300622701644897\n",
      "Loss: 1.098014235496521\n",
      "Loss: 1.4603869915008545\n",
      "Loss: 1.431526780128479\n",
      "Loss: 1.1109938621520996\n",
      "Loss: 1.2632489204406738\n",
      "Loss: 1.4098953008651733\n",
      "Loss: 1.1122719049453735\n",
      "Loss: 1.2248601913452148\n",
      "Loss: 1.6237895488739014\n",
      "Loss: 1.763085126876831\n",
      "Loss: 1.632572889328003\n",
      "Loss: 1.3014280796051025\n",
      "Loss: 1.3012524843215942\n",
      "Loss: 1.250795841217041\n",
      "Loss: 1.4027079343795776\n",
      "Loss: 1.3468444347381592\n",
      "Loss: 1.5457476377487183\n",
      "Loss: 1.277178168296814\n",
      "Loss: 1.196873664855957\n",
      "Loss: 1.2334718704223633\n",
      "Loss: 1.0899207592010498\n",
      "Loss: 1.5114836692810059\n",
      "Loss: 1.340662956237793\n",
      "Loss: 1.253896951675415\n",
      "Loss: 1.2866239547729492\n",
      "Loss: 1.342124581336975\n",
      "Loss: 1.324912428855896\n",
      "Loss: 1.2508654594421387\n",
      "Loss: 1.2259007692337036\n",
      "Loss: 1.2639505863189697\n",
      "Loss: 1.392605185508728\n",
      "Loss: 1.2682194709777832\n",
      "Loss: 1.2202367782592773\n",
      "Loss: 1.1189669370651245\n",
      "Loss: 1.4634824991226196\n",
      "Loss: 1.1182035207748413\n",
      "Loss: 1.617924451828003\n",
      "Loss: 1.312849998474121\n",
      "Loss: 1.7567520141601562\n",
      "Loss: 1.2642279863357544\n",
      "Loss: 1.0016878843307495\n",
      "Loss: 1.278499722480774\n",
      "Loss: 1.3395122289657593\n",
      "Loss: 1.0473235845565796\n",
      "Loss: 1.303757667541504\n",
      "Loss: 1.0364866256713867\n",
      "Loss: 1.3158003091812134\n",
      "Loss: 1.0334320068359375\n",
      "Loss: 1.3454898595809937\n",
      "Loss: 1.610535740852356\n",
      "Loss: 1.117316484451294\n",
      "Loss: 1.3290739059448242\n",
      "Loss: 1.1589869260787964\n",
      "Loss: 1.2751556634902954\n",
      "Loss: 1.118372917175293\n",
      "Loss: 1.0789942741394043\n",
      "Loss: 1.2052178382873535\n",
      "Loss: 1.25667142868042\n",
      "Loss: 1.087365746498108\n",
      "Loss: 1.26752507686615\n",
      "Loss: 1.3001240491867065\n",
      "Loss: 1.2131319046020508\n",
      "Loss: 1.383064866065979\n",
      "Loss: 1.0269824266433716\n",
      "Loss: 1.267236351966858\n",
      "Loss: 1.2051447629928589\n",
      "Loss: 1.1581027507781982\n",
      "Loss: 1.2143549919128418\n",
      "Loss: 1.4053155183792114\n",
      "Loss: 1.2736320495605469\n",
      "Loss: 1.4560455083847046\n",
      "Loss: 1.3637226819992065\n",
      "Loss: 1.280386209487915\n",
      "Loss: 0.9875839352607727\n",
      "Loss: 1.3149687051773071\n",
      "Loss: 1.1509945392608643\n",
      "Loss: 1.4508002996444702\n",
      "Loss: 1.1669307947158813\n",
      "Loss: 1.1835060119628906\n",
      "Loss: 1.4548437595367432\n",
      "Loss: 1.2214583158493042\n",
      "Loss: 1.1926418542861938\n",
      "Loss: 1.3775122165679932\n",
      "Loss: 1.3205671310424805\n",
      "Loss: 1.2841286659240723\n",
      "Loss: 1.219007968902588\n",
      "Loss: 1.1335276365280151\n",
      "Loss: 1.2119208574295044\n",
      "Loss: 1.2257529497146606\n",
      "Loss: 1.2654953002929688\n",
      "Loss: 1.1596870422363281\n",
      "Loss: 0.9888041615486145\n",
      "Loss: 1.4068117141723633\n",
      "Loss: 1.2808972597122192\n",
      "Loss: 1.1286488771438599\n",
      "Loss: 1.3028886318206787\n",
      "Loss: 1.3703080415725708\n",
      "Loss: 1.261032223701477\n",
      "Loss: 1.4445573091506958\n",
      "Loss: 1.1982226371765137\n",
      "Loss: 1.2251136302947998\n",
      "Loss: 1.4952645301818848\n",
      "Loss: 0.9675335884094238\n",
      "Loss: 1.4960349798202515\n",
      "Loss: 1.3032640218734741\n",
      "Loss: 1.3615292310714722\n",
      "Loss: 1.1314189434051514\n",
      "Loss: 1.0800020694732666\n",
      "Loss: 1.1606308221817017\n",
      "Loss: 1.2875251770019531\n",
      "Loss: 0.9796814918518066\n",
      "Loss: 1.0468345880508423\n",
      "Loss: 1.3220371007919312\n",
      "Loss: 1.2171556949615479\n",
      "Loss: 1.4108604192733765\n",
      "Loss: 1.0667273998260498\n",
      "Loss: 1.2724387645721436\n",
      "Loss: 0.9733000993728638\n",
      "Loss: 1.1657841205596924\n",
      "Loss: 1.2085964679718018\n",
      "Loss: 1.4145327806472778\n",
      "Loss: 1.0316940546035767\n",
      "Loss: 1.2020612955093384\n",
      "Loss: 1.4044190645217896\n",
      "Loss: 1.2166486978530884\n",
      "Loss: 1.3140478134155273\n",
      "Loss: 1.1699028015136719\n",
      "Loss: 1.0640208721160889\n",
      "Loss: 1.1109665632247925\n",
      "Loss: 1.282559871673584\n",
      "Loss: 1.13416588306427\n",
      "Loss: 1.378733515739441\n",
      "Loss: 1.3009631633758545\n",
      "Loss: 1.1336040496826172\n",
      "Loss: 1.117401361465454\n",
      "Loss: 0.9894036054611206\n",
      "Loss: 1.2349382638931274\n",
      "Loss: 1.293837547302246\n",
      "Loss: 1.0066263675689697\n",
      "Loss: 1.144315481185913\n",
      "Loss: 1.2892123460769653\n",
      "Loss: 1.3642756938934326\n",
      "Loss: 1.1503819227218628\n",
      "Loss: 1.238848090171814\n",
      "Loss: 1.2947592735290527\n",
      "Loss: 1.2793198823928833\n",
      "Loss: 1.3867994546890259\n",
      "Loss: 1.1846293210983276\n",
      "Loss: 1.3020211458206177\n",
      "Loss: 1.2613575458526611\n",
      "Loss: 1.4629641771316528\n",
      "Loss: 1.1246036291122437\n",
      "Loss: 1.4205130338668823\n",
      "Loss: 1.4473727941513062\n",
      "Loss: 1.1376562118530273\n",
      "Loss: 1.229251503944397\n",
      "Loss: 1.0496493577957153\n",
      "Loss: 1.1700372695922852\n",
      "Loss: 1.2896748781204224\n",
      "Loss: 1.1218352317810059\n",
      "Loss: 1.2222869396209717\n",
      "Loss: 1.0251193046569824\n",
      "Loss: 1.4456483125686646\n",
      "Loss: 1.1176317930221558\n",
      "Loss: 1.35329270362854\n",
      "Loss: 1.3475489616394043\n",
      "Loss: 1.1959071159362793\n",
      "Loss: 1.2581703662872314\n",
      "Loss: 1.0993905067443848\n",
      "Loss: 0.9449941515922546\n",
      "Loss: 1.2014377117156982\n",
      "Loss: 1.462572455406189\n",
      "Loss: 1.3420929908752441\n",
      "Loss: 1.0339393615722656\n",
      "Loss: 1.534349799156189\n",
      "Loss: 1.1335688829421997\n",
      "Loss: 1.3005404472351074\n",
      "Loss: 1.113824486732483\n",
      "Loss: 1.1736772060394287\n",
      "Loss: 1.0841693878173828\n",
      "Loss: 1.008832573890686\n",
      "Loss: 1.2442015409469604\n",
      "Loss: 1.378468632698059\n",
      "Loss: 1.2424261569976807\n",
      "Loss: 1.1010956764221191\n",
      "Loss: 1.2528785467147827\n",
      "Loss: 1.4006870985031128\n",
      "Loss: 1.3451794385910034\n",
      "Loss: 1.1506346464157104\n",
      "Loss: 1.104378581047058\n",
      "Loss: 1.1331069469451904\n",
      "Loss: 1.5056166648864746\n",
      "Loss: 1.4190624952316284\n",
      "Loss: 1.3183910846710205\n",
      "Loss: 1.2393198013305664\n",
      "Loss: 1.3059238195419312\n",
      "Loss: 1.1074869632720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2563494443893433\n",
      "Loss: 1.403342843055725\n",
      "Loss: 1.1448407173156738\n",
      "Loss: 1.2700246572494507\n",
      "Loss: 1.197383999824524\n",
      "Loss: 1.1524964570999146\n",
      "Loss: 1.1833837032318115\n",
      "Loss: 1.2829405069351196\n",
      "Loss: 1.2575676441192627\n",
      "Loss: 1.3691112995147705\n",
      "Loss: 1.1953160762786865\n",
      "Loss: 0.9843840003013611\n",
      "Loss: 1.049376130104065\n",
      "Loss: 1.2238192558288574\n",
      "Loss: 1.0818769931793213\n",
      "Loss: 1.221265435218811\n",
      "Loss: 1.296981692314148\n",
      "Loss: 1.2105437517166138\n",
      "Loss: 1.3517882823944092\n",
      "Loss: 1.4353647232055664\n",
      "Loss: 1.157196044921875\n",
      "Loss: 1.3252780437469482\n",
      "Loss: 1.3122432231903076\n",
      "Loss: 1.1853066682815552\n",
      "Loss: 1.2519562244415283\n",
      "Loss: 1.3399854898452759\n",
      "Loss: 1.3301687240600586\n",
      "Loss: 1.2446739673614502\n",
      "Loss: 0.9673069715499878\n",
      "Loss: 1.179593563079834\n",
      "Loss: 1.198248028755188\n",
      "Loss: 1.2492250204086304\n",
      "Loss: 1.1838258504867554\n",
      "Loss: 1.1154186725616455\n",
      "Loss: 1.2435503005981445\n",
      "Loss: 1.1733181476593018\n",
      "Loss: 1.2816259860992432\n",
      "Loss: 1.4226664304733276\n",
      "Loss: 1.1587389707565308\n",
      "Loss: 1.1353973150253296\n",
      "Loss: 1.1618744134902954\n",
      "Loss: 1.3599145412445068\n",
      "Loss: 1.1938884258270264\n",
      "Loss: 1.4208449125289917\n",
      "Loss: 1.2440860271453857\n",
      "Loss: 1.2013260126113892\n",
      "Loss: 1.167707085609436\n",
      "Loss: 1.190779685974121\n",
      "Loss: 1.1260404586791992\n",
      "Loss: 1.0278937816619873\n",
      "Loss: 1.303391456604004\n",
      "Loss: 1.3113423585891724\n",
      "Loss: 1.2229841947555542\n",
      "Loss: 1.0783264636993408\n",
      "Loss: 1.2692532539367676\n",
      "Loss: 1.2664270401000977\n",
      "Loss: 1.1705702543258667\n",
      "Loss: 1.1029691696166992\n",
      "Loss: 1.2340068817138672\n",
      "Loss: 1.0087275505065918\n",
      "Loss: 1.2008581161499023\n",
      "Loss: 1.1323069334030151\n",
      "Loss: 1.0741431713104248\n",
      "Loss: 1.0341225862503052\n",
      "Loss: 1.0548293590545654\n",
      "Loss: 1.1281077861785889\n",
      "Loss: 1.0293493270874023\n",
      "Loss: 1.1280714273452759\n",
      "Loss: 1.2192203998565674\n",
      "Loss: 1.3690423965454102\n",
      "Loss: 1.1918632984161377\n",
      "Loss: 1.2687883377075195\n",
      "Loss: 1.3266428709030151\n",
      "Loss: 1.1690545082092285\n",
      "Loss: 1.0636903047561646\n",
      "Loss: 1.2885087728500366\n",
      "Loss: 1.456192970275879\n",
      "Loss: 1.339510440826416\n",
      "Loss: 1.3222934007644653\n",
      "Loss: 1.2157245874404907\n",
      "Loss: 1.2957513332366943\n",
      "Loss: 1.1628409624099731\n",
      "Loss: 0.9602407217025757\n",
      "Loss: 1.3418344259262085\n",
      "Loss: 1.2332618236541748\n",
      "Loss: 1.0941405296325684\n",
      "Loss: 1.273746132850647\n",
      "Loss: 1.234391450881958\n",
      "Loss: 1.2861526012420654\n",
      "Loss: 1.3526252508163452\n",
      "Loss: 1.0530059337615967\n",
      "Loss: 1.195702075958252\n",
      "Loss: 1.2838160991668701\n",
      "Loss: 1.321865200996399\n",
      "Loss: 1.2850500345230103\n",
      "Loss: 1.1863939762115479\n",
      "Loss: 1.2922964096069336\n",
      "Loss: 0.9848041534423828\n",
      "Loss: 1.1725280284881592\n",
      "Loss: 1.1318854093551636\n",
      "Loss: 1.270263433456421\n",
      "Loss: 1.2737932205200195\n",
      "Loss: 1.2582436800003052\n",
      "Loss: 1.2190213203430176\n",
      "Loss: 1.478855848312378\n",
      "Loss: 1.263303518295288\n",
      "Loss: 1.0482492446899414\n",
      "Loss: 1.1727783679962158\n",
      "Loss: 1.2216111421585083\n",
      "Loss: 0.8183339834213257\n",
      "Loss: 1.4877914190292358\n",
      "Loss: 1.1857587099075317\n",
      "Loss: 0.9796085357666016\n",
      "Loss: 1.1918102502822876\n",
      "Loss: 1.1566601991653442\n",
      "Loss: 1.1214625835418701\n",
      "Loss: 1.2897753715515137\n",
      "Loss: 1.393506646156311\n",
      "Loss: 1.1876639127731323\n",
      "Loss: 1.3913214206695557\n",
      "Loss: 1.1665016412734985\n",
      "Loss: 1.1178789138793945\n",
      "Loss: 1.231469988822937\n",
      "Loss: 1.1592686176300049\n",
      "Loss: 1.1318601369857788\n",
      "Loss: 1.157965898513794\n",
      "Loss: 1.1896733045578003\n",
      "Loss: 1.0742554664611816\n",
      "Loss: 1.121546983718872\n",
      "Loss: 1.0540724992752075\n",
      "Loss: 1.296531081199646\n",
      "Loss: 1.1609470844268799\n",
      "Loss: 1.66414213180542\n",
      "Loss: 1.3626017570495605\n",
      "Loss: 1.2246931791305542\n",
      "Loss: 1.3278210163116455\n",
      "Loss: 0.998862087726593\n",
      "Loss: 1.0043548345565796\n",
      "Loss: 1.2588485479354858\n",
      "Loss: 1.3776724338531494\n",
      "Loss: 1.0444905757904053\n",
      "Loss: 1.3926353454589844\n",
      "Loss: 1.283640742301941\n",
      "Loss: 1.3935601711273193\n",
      "Loss: 1.2700749635696411\n",
      "Loss: 1.4010580778121948\n",
      "Loss: 1.2938357591629028\n",
      "Loss: 1.2393853664398193\n",
      "Loss: 1.1966496706008911\n",
      "Loss: 1.2760347127914429\n",
      "Loss: 1.4307186603546143\n",
      "Loss: 1.2587332725524902\n",
      "Loss: 1.5049831867218018\n",
      "Loss: 1.1174768209457397\n",
      "Loss: 1.3139582872390747\n",
      "Loss: 1.2776484489440918\n",
      "Loss: 1.1688232421875\n",
      "Loss: 1.2090564966201782\n",
      "Loss: 1.3832415342330933\n",
      "Loss: 1.4675743579864502\n",
      "Loss: 1.2586028575897217\n",
      "Loss: 1.2986814975738525\n",
      "Loss: 1.0805147886276245\n",
      "Loss: 1.240971326828003\n",
      "Loss: 1.2335131168365479\n",
      "Loss: 1.1076582670211792\n",
      "Loss: 1.2298142910003662\n",
      "Loss: 1.3393030166625977\n",
      "Loss: 1.3650034666061401\n",
      "Loss: 1.1393321752548218\n",
      "Loss: 1.3616055250167847\n",
      "Loss: 1.1057307720184326\n",
      "Loss: 1.307349443435669\n",
      "Loss: 1.2807817459106445\n",
      "Loss: 1.2801460027694702\n",
      "Loss: 1.2532916069030762\n",
      "Loss: 1.2900947332382202\n",
      "Loss: 1.0855380296707153\n",
      "Loss: 1.379518747329712\n",
      "Loss: 1.1467593908309937\n",
      "Loss: 0.9852449893951416\n",
      "Loss: 1.4831522703170776\n",
      "Loss: 1.1293200254440308\n",
      "Loss: 1.4231446981430054\n",
      "Loss: 1.0936334133148193\n",
      "Loss: 1.4027410745620728\n",
      "Loss: 1.1791912317276\n",
      "Loss: 1.1081740856170654\n",
      "Loss: 1.1920348405838013\n",
      "Loss: 1.1149975061416626\n",
      "Loss: 1.2038017511367798\n",
      "Loss: 1.0106333494186401\n",
      "Loss: 1.4339001178741455\n",
      "Loss: 1.1490801572799683\n",
      "Loss: 1.4813343286514282\n",
      "Loss: 1.8436601161956787\n",
      "Loss: 0.992838442325592\n",
      "Loss: 1.2004950046539307\n",
      "Loss: 1.2700743675231934\n",
      "Loss: 1.0762827396392822\n",
      "Loss: 1.3768911361694336\n",
      "Loss: 1.4488389492034912\n",
      "Loss: 1.3022938966751099\n",
      "Loss: 1.3164743185043335\n",
      "Loss: 1.0845494270324707\n",
      "Loss: 1.231898307800293\n",
      "Loss: 1.4370489120483398\n",
      "Loss: 1.406612515449524\n",
      "Loss: 1.1785396337509155\n",
      "Loss: 1.197891354560852\n",
      "Loss: 1.1143451929092407\n",
      "Loss: 1.0594983100891113\n",
      "Loss: 1.1966118812561035\n",
      "Loss: 1.439035177230835\n",
      "Loss: 0.9877829551696777\n",
      "Loss: 1.245133638381958\n",
      "Loss: 1.376039981842041\n",
      "Loss: 1.07768976688385\n",
      "Loss: 1.2206538915634155\n",
      "Loss: 1.2591651678085327\n",
      "Loss: 1.369421124458313\n",
      "Loss: 1.1184288263320923\n",
      "Loss: 1.0475637912750244\n",
      "Loss: 0.930880069732666\n",
      "Loss: 1.2883648872375488\n",
      "Loss: 1.217452883720398\n",
      "Loss: 1.0205053091049194\n",
      "Loss: 1.0486727952957153\n",
      "Loss: 1.3300988674163818\n",
      "Loss: 1.0711673498153687\n",
      "Loss: 1.2951669692993164\n",
      "Loss: 1.0698494911193848\n",
      "Loss: 1.7003238201141357\n",
      "Loss: 1.296053409576416\n",
      "Loss: 1.2219146490097046\n",
      "Loss: 1.202470302581787\n",
      "Loss: 1.3263145685195923\n",
      "Loss: 1.121506929397583\n",
      "Loss: 1.2151283025741577\n",
      "Loss: 1.3277678489685059\n",
      "Loss: 1.4927781820297241\n",
      "Loss: 1.1611453294754028\n",
      "Loss: 1.3179692029953003\n",
      "Loss: 1.4605231285095215\n",
      "Loss: 1.1721575260162354\n",
      "Loss: 1.158571481704712\n",
      "Loss: 1.1376765966415405\n",
      "Loss: 1.0068988800048828\n",
      "Loss: 1.4054572582244873\n",
      "Loss: 1.2447326183319092\n",
      "Loss: 1.4498543739318848\n",
      "Loss: 1.3311924934387207\n",
      "Loss: 1.0564143657684326\n",
      "Loss: 1.1576443910598755\n",
      "Loss: 1.2988070249557495\n",
      "Loss: 1.1543501615524292\n",
      "Loss: 1.1974117755889893\n",
      "Loss: 1.5257419347763062\n",
      "Loss: 1.471794605255127\n",
      "Loss: 1.1776920557022095\n",
      "Loss: 1.3780800104141235\n",
      "Loss: 1.1459885835647583\n",
      "Loss: 1.1982214450836182\n",
      "Loss: 1.676869511604309\n",
      "Loss: 1.2468976974487305\n",
      "Loss: 1.1324021816253662\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Explanation here!\n",
    "probably because random dropouts draw the NN away from overfitting/minima and allow for a well trained network to fine-adjust to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "        return torch.where(X > 0, X, a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_h = init_weights((784, 50))\n",
    "w_h2 = init_weights((50, 50))\n",
    "w_o = init_weights((50, 10))\n",
    "a = torch.tensor([-0.1], requires_grad = True)\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "loss: 3.4085\n",
      "a: -0.1000\n",
      "step:  1\n",
      "loss: 2.5221\n",
      "a: -0.0968\n",
      "step:  2\n",
      "loss: 2.5012\n",
      "a: -0.0941\n",
      "step:  3\n",
      "loss: 2.6047\n",
      "a: -0.0919\n",
      "step:  4\n",
      "loss: 2.3506\n",
      "a: -0.0891\n",
      "step:  5\n",
      "loss: 2.4400\n",
      "a: -0.0866\n",
      "step:  6\n",
      "loss: 2.3091\n",
      "a: -0.0840\n",
      "step:  7\n",
      "loss: 2.3205\n",
      "a: -0.0816\n",
      "step:  8\n",
      "loss: 2.3555\n",
      "a: -0.0792\n",
      "step:  9\n",
      "loss: 2.3413\n",
      "a: -0.0770\n",
      "step:  10\n",
      "loss: 2.3310\n",
      "a: -0.0750\n",
      "step:  11\n",
      "loss: 2.4262\n",
      "a: -0.0729\n",
      "step:  12\n",
      "loss: 2.2930\n",
      "a: -0.0709\n",
      "step:  13\n",
      "loss: 2.3100\n",
      "a: -0.0690\n",
      "step:  14\n",
      "loss: 2.2143\n",
      "a: -0.0672\n",
      "step:  15\n",
      "loss: 2.2310\n",
      "a: -0.0655\n",
      "step:  16\n",
      "loss: 2.2795\n",
      "a: -0.0639\n",
      "step:  17\n",
      "loss: 2.3911\n",
      "a: -0.0623\n",
      "step:  18\n",
      "loss: 2.2091\n",
      "a: -0.0607\n",
      "step:  19\n",
      "loss: 2.1138\n",
      "a: -0.0591\n",
      "step:  20\n",
      "loss: 2.1172\n",
      "a: -0.0576\n",
      "step:  21\n",
      "loss: 2.2578\n",
      "a: -0.0561\n",
      "step:  22\n",
      "loss: 2.2086\n",
      "a: -0.0546\n",
      "step:  23\n",
      "loss: 2.1716\n",
      "a: -0.0532\n",
      "step:  24\n",
      "loss: 2.0940\n",
      "a: -0.0518\n",
      "step:  25\n",
      "loss: 2.1407\n",
      "a: -0.0503\n",
      "step:  26\n",
      "loss: 2.1106\n",
      "a: -0.0489\n",
      "step:  27\n",
      "loss: 2.0607\n",
      "a: -0.0474\n",
      "step:  28\n",
      "loss: 2.1161\n",
      "a: -0.0460\n",
      "step:  29\n",
      "loss: 1.9676\n",
      "a: -0.0447\n",
      "step:  30\n",
      "loss: 2.1163\n",
      "a: -0.0434\n",
      "step:  31\n",
      "loss: 1.9599\n",
      "a: -0.0420\n",
      "step:  32\n",
      "loss: 2.1428\n",
      "a: -0.0407\n",
      "step:  33\n",
      "loss: 2.0046\n",
      "a: -0.0394\n",
      "step:  34\n",
      "loss: 1.9258\n",
      "a: -0.0381\n",
      "step:  35\n",
      "loss: 1.9724\n",
      "a: -0.0368\n",
      "step:  36\n",
      "loss: 1.8967\n",
      "a: -0.0355\n",
      "step:  37\n",
      "loss: 1.9229\n",
      "a: -0.0342\n",
      "step:  38\n",
      "loss: 1.8829\n",
      "a: -0.0329\n",
      "step:  39\n",
      "loss: 1.8738\n",
      "a: -0.0316\n",
      "step:  40\n",
      "loss: 1.9295\n",
      "a: -0.0304\n",
      "step:  41\n",
      "loss: 1.7957\n",
      "a: -0.0292\n",
      "step:  42\n",
      "loss: 1.8951\n",
      "a: -0.0280\n",
      "step:  43\n",
      "loss: 1.7749\n",
      "a: -0.0268\n",
      "step:  44\n",
      "loss: 1.9031\n",
      "a: -0.0256\n",
      "step:  45\n",
      "loss: 1.7559\n",
      "a: -0.0244\n",
      "step:  46\n",
      "loss: 1.8441\n",
      "a: -0.0233\n",
      "step:  47\n",
      "loss: 1.7691\n",
      "a: -0.0222\n",
      "step:  48\n",
      "loss: 1.6500\n",
      "a: -0.0210\n",
      "step:  49\n",
      "loss: 1.8786\n",
      "a: -0.0198\n",
      "step:  50\n",
      "loss: 1.6281\n",
      "a: -0.0186\n",
      "step:  51\n",
      "loss: 1.9056\n",
      "a: -0.0174\n",
      "step:  52\n",
      "loss: 1.9222\n",
      "a: -0.0162\n",
      "step:  53\n",
      "loss: 1.5522\n",
      "a: -0.0150\n",
      "step:  54\n",
      "loss: 1.7388\n",
      "a: -0.0138\n",
      "step:  55\n",
      "loss: 1.7017\n",
      "a: -0.0127\n",
      "step:  56\n",
      "loss: 1.8135\n",
      "a: -0.0116\n",
      "step:  57\n",
      "loss: 1.7568\n",
      "a: -0.0105\n",
      "step:  58\n",
      "loss: 1.5650\n",
      "a: -0.0095\n",
      "step:  59\n",
      "loss: 1.6894\n",
      "a: -0.0084\n",
      "step:  60\n",
      "loss: 1.7822\n",
      "a: -0.0074\n",
      "step:  61\n",
      "loss: 1.8146\n",
      "a: -0.0064\n",
      "step:  62\n",
      "loss: 1.7770\n",
      "a: -0.0055\n",
      "step:  63\n",
      "loss: 1.7493\n",
      "a: -0.0045\n",
      "step:  64\n",
      "loss: 1.6809\n",
      "a: -0.0035\n",
      "step:  65\n",
      "loss: 1.5123\n",
      "a: -0.0026\n",
      "step:  66\n",
      "loss: 1.4301\n",
      "a: -0.0017\n",
      "step:  67\n",
      "loss: 1.6585\n",
      "a: -0.0008\n",
      "step:  68\n",
      "loss: 1.6485\n",
      "a: -0.0001\n",
      "step:  69\n",
      "loss: 1.8119\n",
      "a: 0.0007\n",
      "step:  70\n",
      "loss: 1.3545\n",
      "a: 0.0014\n",
      "step:  71\n",
      "loss: 1.7212\n",
      "a: 0.0021\n",
      "step:  72\n",
      "loss: 1.7668\n",
      "a: 0.0025\n",
      "step:  73\n",
      "loss: 1.5602\n",
      "a: 0.0028\n",
      "step:  74\n",
      "loss: 1.8980\n",
      "a: 0.0030\n",
      "step:  75\n",
      "loss: 1.6417\n",
      "a: 0.0031\n",
      "step:  76\n",
      "loss: 1.6261\n",
      "a: 0.0029\n",
      "step:  77\n",
      "loss: 1.8146\n",
      "a: 0.0026\n",
      "step:  78\n",
      "loss: 1.9130\n",
      "a: 0.0020\n",
      "step:  79\n",
      "loss: 1.6634\n",
      "a: 0.0011\n",
      "step:  80\n",
      "loss: 1.7140\n",
      "a: 0.0001\n",
      "step:  81\n",
      "loss: 1.6001\n",
      "a: -0.0009\n",
      "step:  82\n",
      "loss: 1.5459\n",
      "a: -0.0019\n",
      "step:  83\n",
      "loss: 1.7669\n",
      "a: -0.0030\n",
      "step:  84\n",
      "loss: 1.6140\n",
      "a: -0.0040\n",
      "step:  85\n",
      "loss: 1.5271\n",
      "a: -0.0051\n",
      "step:  86\n",
      "loss: 1.4989\n",
      "a: -0.0062\n",
      "step:  87\n",
      "loss: 1.5702\n",
      "a: -0.0072\n",
      "step:  88\n",
      "loss: 1.6632\n",
      "a: -0.0082\n",
      "step:  89\n",
      "loss: 1.7257\n",
      "a: -0.0090\n",
      "step:  90\n",
      "loss: 1.8331\n",
      "a: -0.0096\n",
      "step:  91\n",
      "loss: 1.7898\n",
      "a: -0.0101\n",
      "step:  92\n",
      "loss: 1.5673\n",
      "a: -0.0104\n",
      "step:  93\n",
      "loss: 1.6740\n",
      "a: -0.0103\n",
      "step:  94\n",
      "loss: 1.6874\n",
      "a: -0.0100\n",
      "step:  95\n",
      "loss: 1.5068\n",
      "a: -0.0093\n",
      "step:  96\n",
      "loss: 1.7248\n",
      "a: -0.0085\n",
      "step:  97\n",
      "loss: 1.6170\n",
      "a: -0.0075\n",
      "step:  98\n",
      "loss: 1.5882\n",
      "a: -0.0063\n",
      "step:  99\n",
      "loss: 1.5802\n",
      "a: -0.0052\n",
      "step:  100\n",
      "loss: 1.6569\n",
      "a: -0.0041\n",
      "step:  101\n",
      "loss: 1.7392\n",
      "a: -0.0029\n",
      "step:  102\n",
      "loss: 1.5889\n",
      "a: -0.0020\n",
      "step:  103\n",
      "loss: 1.6225\n",
      "a: -0.0010\n",
      "step:  104\n",
      "loss: 1.5066\n",
      "a: -0.0003\n",
      "step:  105\n",
      "loss: 1.8229\n",
      "a: 0.0004\n",
      "step:  106\n",
      "loss: 1.6268\n",
      "a: 0.0010\n",
      "step:  107\n",
      "loss: 1.4473\n",
      "a: 0.0012\n",
      "step:  108\n",
      "loss: 1.5546\n",
      "a: 0.0013\n",
      "step:  109\n",
      "loss: 1.5561\n",
      "a: 0.0010\n",
      "step:  110\n",
      "loss: 1.5924\n",
      "a: 0.0002\n",
      "step:  111\n",
      "loss: 1.2335\n",
      "a: -0.0006\n",
      "step:  112\n",
      "loss: 1.6826\n",
      "a: -0.0015\n",
      "step:  113\n",
      "loss: 1.6981\n",
      "a: -0.0025\n",
      "step:  114\n",
      "loss: 1.3215\n",
      "a: -0.0034\n",
      "step:  115\n",
      "loss: 1.4294\n",
      "a: -0.0038\n",
      "step:  116\n",
      "loss: 1.6396\n",
      "a: -0.0041\n",
      "step:  117\n",
      "loss: 1.7817\n",
      "a: -0.0046\n",
      "step:  118\n",
      "loss: 1.7511\n",
      "a: -0.0051\n",
      "step:  119\n",
      "loss: 1.7832\n",
      "a: -0.0050\n",
      "step:  120\n",
      "loss: 1.6281\n",
      "a: -0.0046\n",
      "step:  121\n",
      "loss: 1.5801\n",
      "a: -0.0036\n",
      "step:  122\n",
      "loss: 1.4008\n",
      "a: -0.0029\n",
      "step:  123\n",
      "loss: 1.4286\n",
      "a: -0.0028\n",
      "step:  124\n",
      "loss: 1.6410\n",
      "a: -0.0028\n",
      "step:  125\n",
      "loss: 1.3077\n",
      "a: -0.0030\n",
      "step:  126\n",
      "loss: 1.6435\n",
      "a: -0.0031\n",
      "step:  127\n",
      "loss: 1.4600\n",
      "a: -0.0035\n",
      "step:  128\n",
      "loss: 1.7056\n",
      "a: -0.0044\n",
      "step:  129\n",
      "loss: 1.7571\n",
      "a: -0.0057\n",
      "step:  130\n",
      "loss: 1.4880\n",
      "a: -0.0061\n",
      "step:  131\n",
      "loss: 1.5708\n",
      "a: -0.0063\n",
      "step:  132\n",
      "loss: 1.5376\n",
      "a: -0.0054\n",
      "step:  133\n",
      "loss: 1.4449\n",
      "a: -0.0041\n",
      "step:  134\n",
      "loss: 1.6267\n",
      "a: -0.0026\n",
      "step:  135\n",
      "loss: 1.4549\n",
      "a: -0.0010\n",
      "step:  136\n",
      "loss: 1.5073\n",
      "a: 0.0003\n",
      "step:  137\n",
      "loss: 1.7053\n",
      "a: 0.0006\n",
      "step:  138\n",
      "loss: 1.4918\n",
      "a: 0.0003\n",
      "step:  139\n",
      "loss: 1.6660\n",
      "a: 0.0000\n",
      "step:  140\n",
      "loss: 1.4423\n",
      "a: 0.0002\n",
      "step:  141\n",
      "loss: 1.5049\n",
      "a: 0.0009\n",
      "step:  142\n",
      "loss: 1.5169\n",
      "a: 0.0013\n",
      "step:  143\n",
      "loss: 1.5914\n",
      "a: 0.0005\n",
      "step:  144\n",
      "loss: 1.5522\n",
      "a: -0.0002\n",
      "step:  145\n",
      "loss: 1.5344\n",
      "a: -0.0012\n",
      "step:  146\n",
      "loss: 1.4068\n",
      "a: -0.0027\n",
      "step:  147\n",
      "loss: 1.6969\n",
      "a: -0.0042\n",
      "step:  148\n",
      "loss: 1.6349\n",
      "a: -0.0054\n",
      "step:  149\n",
      "loss: 1.5064\n",
      "a: -0.0052\n",
      "step:  150\n",
      "loss: 1.4180\n",
      "a: -0.0037\n",
      "step:  151\n",
      "loss: 1.5857\n",
      "a: -0.0020\n",
      "step:  152\n",
      "loss: 1.4785\n",
      "a: -0.0012\n",
      "step:  153\n",
      "loss: 1.4036\n",
      "a: -0.0012\n",
      "step:  154\n",
      "loss: 1.4994\n",
      "a: -0.0025\n",
      "step:  155\n",
      "loss: 1.4950\n",
      "a: -0.0034\n",
      "step:  156\n",
      "loss: 1.4671\n",
      "a: -0.0043\n",
      "step:  157\n",
      "loss: 1.3805\n",
      "a: -0.0048\n",
      "step:  158\n",
      "loss: 1.6333\n",
      "a: -0.0048\n",
      "step:  159\n",
      "loss: 1.6569\n",
      "a: -0.0030\n",
      "step:  160\n",
      "loss: 1.4015\n",
      "a: -0.0014\n",
      "step:  161\n",
      "loss: 1.4531\n",
      "a: -0.0007\n",
      "step:  162\n",
      "loss: 1.5482\n",
      "a: -0.0009\n",
      "step:  163\n",
      "loss: 1.5637\n",
      "a: -0.0024\n",
      "step:  164\n",
      "loss: 1.5310\n",
      "a: -0.0034\n",
      "step:  165\n",
      "loss: 1.5554\n",
      "a: -0.0045\n",
      "step:  166\n",
      "loss: 1.5648\n",
      "a: -0.0047\n",
      "step:  167\n",
      "loss: 1.4782\n",
      "a: -0.0043\n",
      "step:  168\n",
      "loss: 1.4542\n",
      "a: -0.0036\n",
      "step:  169\n",
      "loss: 1.5651\n",
      "a: -0.0028\n",
      "step:  170\n",
      "loss: 1.6215\n",
      "a: -0.0008\n",
      "step:  171\n",
      "loss: 1.9141\n",
      "a: 0.0011\n",
      "step:  172\n",
      "loss: 1.5872\n",
      "a: -0.0013\n",
      "step:  173\n",
      "loss: 1.6786\n",
      "a: -0.0034\n",
      "step:  174\n",
      "loss: 1.4823\n",
      "a: -0.0051\n",
      "step:  175\n",
      "loss: 1.4915\n",
      "a: -0.0064\n",
      "step:  176\n",
      "loss: 1.7009\n",
      "a: -0.0071\n",
      "step:  177\n",
      "loss: 1.7038\n",
      "a: -0.0071\n",
      "step:  178\n",
      "loss: 1.5468\n",
      "a: -0.0060\n",
      "step:  179\n",
      "loss: 1.4559\n",
      "a: -0.0043\n",
      "step:  180\n",
      "loss: 1.4176\n",
      "a: -0.0026\n",
      "step:  181\n",
      "loss: 1.8261\n",
      "a: -0.0012\n",
      "step:  182\n",
      "loss: 1.7024\n",
      "a: -0.0000\n",
      "step:  183\n",
      "loss: 1.4918\n",
      "a: 0.0004\n",
      "step:  184\n",
      "loss: 1.4905\n",
      "a: 0.0003\n",
      "step:  185\n",
      "loss: 1.3890\n",
      "a: -0.0000\n",
      "step:  186\n",
      "loss: 1.4765\n",
      "a: -0.0009\n",
      "step:  187\n",
      "loss: 1.5421\n",
      "a: -0.0017\n",
      "step:  188\n",
      "loss: 1.2771\n",
      "a: -0.0026\n",
      "step:  189\n",
      "loss: 1.4485\n",
      "a: -0.0035\n",
      "step:  190\n",
      "loss: 1.4907\n",
      "a: -0.0041\n",
      "step:  191\n",
      "loss: 1.1742\n",
      "a: -0.0045\n",
      "step:  192\n",
      "loss: 1.6574\n",
      "a: -0.0045\n",
      "step:  193\n",
      "loss: 1.2833\n",
      "a: -0.0037\n",
      "step:  194\n",
      "loss: 1.5668\n",
      "a: -0.0030\n",
      "step:  195\n",
      "loss: 1.5464\n",
      "a: -0.0023\n",
      "step:  196\n",
      "loss: 1.5504\n",
      "a: -0.0014\n",
      "step:  197\n",
      "loss: 1.5030\n",
      "a: -0.0005\n",
      "step:  198\n",
      "loss: 1.6402\n",
      "a: -0.0003\n",
      "step:  199\n",
      "loss: 1.4054\n",
      "a: -0.0010\n",
      "step:  200\n",
      "loss: 1.3490\n",
      "a: -0.0025\n",
      "step:  201\n",
      "loss: 1.5555\n",
      "a: -0.0039\n",
      "step:  202\n",
      "loss: 1.2723\n",
      "a: -0.0054\n",
      "step:  203\n",
      "loss: 1.2722\n",
      "a: -0.0063\n",
      "step:  204\n",
      "loss: 1.5342\n",
      "a: -0.0073\n",
      "step:  205\n",
      "loss: 1.4231\n",
      "a: -0.0076\n",
      "step:  206\n",
      "loss: 1.4120\n",
      "a: -0.0072\n",
      "step:  207\n",
      "loss: 1.7237\n",
      "a: -0.0063\n",
      "step:  208\n",
      "loss: 1.7755\n",
      "a: -0.0051\n",
      "step:  209\n",
      "loss: 1.3786\n",
      "a: -0.0033\n",
      "step:  210\n",
      "loss: 1.5147\n",
      "a: -0.0017\n",
      "step:  211\n",
      "loss: 1.7378\n",
      "a: -0.0004\n",
      "step:  212\n",
      "loss: 1.5177\n",
      "a: 0.0004\n",
      "step:  213\n",
      "loss: 1.4615\n",
      "a: -0.0005\n",
      "step:  214\n",
      "loss: 1.4202\n",
      "a: -0.0017\n",
      "step:  215\n",
      "loss: 1.3972\n",
      "a: -0.0033\n",
      "step:  216\n",
      "loss: 1.3857\n",
      "a: -0.0048\n",
      "step:  217\n",
      "loss: 1.6005\n",
      "a: -0.0062\n",
      "step:  218\n",
      "loss: 1.5054\n",
      "a: -0.0073\n",
      "step:  219\n",
      "loss: 1.8596\n",
      "a: -0.0082\n",
      "step:  220\n",
      "loss: 1.1927\n",
      "a: -0.0086\n",
      "step:  221\n",
      "loss: 1.6698\n",
      "a: -0.0087\n",
      "step:  222\n",
      "loss: 1.6810\n",
      "a: -0.0080\n",
      "step:  223\n",
      "loss: 1.3340\n",
      "a: -0.0071\n",
      "step:  224\n",
      "loss: 1.5324\n",
      "a: -0.0058\n",
      "step:  225\n",
      "loss: 1.3293\n",
      "a: -0.0046\n",
      "step:  226\n",
      "loss: 1.4565\n",
      "a: -0.0033\n",
      "step:  227\n",
      "loss: 1.5513\n",
      "a: -0.0021\n",
      "step:  228\n",
      "loss: 1.6032\n",
      "a: -0.0014\n",
      "step:  229\n",
      "loss: 1.5490\n",
      "a: -0.0013\n",
      "step:  230\n",
      "loss: 1.4824\n",
      "a: -0.0018\n",
      "step:  231\n",
      "loss: 1.5488\n",
      "a: -0.0027\n",
      "step:  232\n",
      "loss: 1.3808\n",
      "a: -0.0035\n",
      "step:  233\n",
      "loss: 1.5430\n",
      "a: -0.0044\n",
      "step:  234\n",
      "loss: 1.4282\n",
      "a: -0.0052\n",
      "step:  235\n",
      "loss: 1.6667\n",
      "a: -0.0056\n",
      "step:  236\n",
      "loss: 1.5187\n",
      "a: -0.0058\n",
      "step:  237\n",
      "loss: 1.3366\n",
      "a: -0.0056\n",
      "step:  238\n",
      "loss: 1.5825\n",
      "a: -0.0054\n",
      "step:  239\n",
      "loss: 1.5864\n",
      "a: -0.0046\n",
      "step:  240\n",
      "loss: 1.6622\n",
      "a: -0.0037\n",
      "step:  241\n",
      "loss: 1.4211\n",
      "a: -0.0033\n",
      "step:  242\n",
      "loss: 1.5797\n",
      "a: -0.0030\n",
      "step:  243\n",
      "loss: 1.6200\n",
      "a: -0.0025\n",
      "step:  244\n",
      "loss: 1.1813\n",
      "a: -0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  245\n",
      "loss: 1.5209\n",
      "a: -0.0017\n",
      "step:  246\n",
      "loss: 1.5846\n",
      "a: -0.0019\n",
      "step:  247\n",
      "loss: 1.3325\n",
      "a: -0.0028\n",
      "step:  248\n",
      "loss: 1.3627\n",
      "a: -0.0039\n",
      "step:  249\n",
      "loss: 1.6347\n",
      "a: -0.0049\n",
      "step:  250\n",
      "loss: 1.4698\n",
      "a: -0.0052\n",
      "step:  251\n",
      "loss: 1.5196\n",
      "a: -0.0057\n",
      "step:  252\n",
      "loss: 1.4727\n",
      "a: -0.0055\n",
      "step:  253\n",
      "loss: 1.4236\n",
      "a: -0.0046\n",
      "step:  254\n",
      "loss: 1.3833\n",
      "a: -0.0031\n",
      "step:  255\n",
      "loss: 1.2631\n",
      "a: -0.0018\n",
      "step:  256\n",
      "loss: 1.5503\n",
      "a: -0.0013\n",
      "step:  257\n",
      "loss: 1.7177\n",
      "a: -0.0021\n",
      "step:  258\n",
      "loss: 1.5474\n",
      "a: -0.0038\n",
      "step:  259\n",
      "loss: 1.6490\n",
      "a: -0.0051\n",
      "step:  260\n",
      "loss: 1.3808\n",
      "a: -0.0059\n",
      "step:  261\n",
      "loss: 1.5072\n",
      "a: -0.0061\n",
      "step:  262\n",
      "loss: 1.4202\n",
      "a: -0.0058\n",
      "step:  263\n",
      "loss: 1.4471\n",
      "a: -0.0049\n",
      "step:  264\n",
      "loss: 1.4630\n",
      "a: -0.0037\n",
      "step:  265\n",
      "loss: 1.2740\n",
      "a: -0.0028\n",
      "step:  266\n",
      "loss: 1.6468\n",
      "a: -0.0021\n",
      "step:  267\n",
      "loss: 1.3712\n",
      "a: -0.0020\n",
      "step:  268\n",
      "loss: 1.4291\n",
      "a: -0.0029\n",
      "step:  269\n",
      "loss: 1.7333\n",
      "a: -0.0041\n",
      "step:  270\n",
      "loss: 1.5332\n",
      "a: -0.0057\n",
      "step:  271\n",
      "loss: 1.3080\n",
      "a: -0.0071\n",
      "step:  272\n",
      "loss: 1.6563\n",
      "a: -0.0082\n",
      "step:  273\n",
      "loss: 1.3753\n",
      "a: -0.0089\n",
      "step:  274\n",
      "loss: 1.5225\n",
      "a: -0.0095\n",
      "step:  275\n",
      "loss: 1.7651\n",
      "a: -0.0097\n",
      "step:  276\n",
      "loss: 1.4318\n",
      "a: -0.0091\n",
      "step:  277\n",
      "loss: 1.5283\n",
      "a: -0.0082\n",
      "step:  278\n",
      "loss: 1.3391\n",
      "a: -0.0069\n",
      "step:  279\n",
      "loss: 1.4175\n",
      "a: -0.0056\n",
      "step:  280\n",
      "loss: 1.2237\n",
      "a: -0.0042\n",
      "step:  281\n",
      "loss: 1.4786\n",
      "a: -0.0030\n",
      "step:  282\n",
      "loss: 1.6658\n",
      "a: -0.0023\n",
      "step:  283\n",
      "loss: 1.6725\n",
      "a: -0.0022\n",
      "step:  284\n",
      "loss: 1.4878\n",
      "a: -0.0030\n",
      "step:  285\n",
      "loss: 1.3781\n",
      "a: -0.0043\n",
      "step:  286\n",
      "loss: 1.3871\n",
      "a: -0.0055\n",
      "step:  287\n",
      "loss: 1.6144\n",
      "a: -0.0066\n",
      "step:  288\n",
      "loss: 1.4349\n",
      "a: -0.0078\n",
      "step:  289\n",
      "loss: 1.6048\n",
      "a: -0.0087\n",
      "step:  290\n",
      "loss: 1.4849\n",
      "a: -0.0093\n",
      "step:  291\n",
      "loss: 1.3999\n",
      "a: -0.0093\n",
      "step:  292\n",
      "loss: 1.5296\n",
      "a: -0.0088\n",
      "step:  293\n",
      "loss: 1.5687\n",
      "a: -0.0079\n",
      "step:  294\n",
      "loss: 1.3354\n",
      "a: -0.0065\n",
      "step:  295\n",
      "loss: 1.3680\n",
      "a: -0.0051\n",
      "step:  296\n",
      "loss: 1.2096\n",
      "a: -0.0038\n",
      "step:  297\n",
      "loss: 1.4351\n",
      "a: -0.0026\n",
      "step:  298\n",
      "loss: 1.5171\n",
      "a: -0.0015\n",
      "step:  299\n",
      "loss: 1.5058\n",
      "a: -0.0008\n",
      "step:  300\n",
      "loss: 1.6863\n",
      "a: -0.0019\n",
      "step:  301\n",
      "loss: 1.4714\n",
      "a: -0.0039\n",
      "step:  302\n",
      "loss: 1.3872\n",
      "a: -0.0059\n",
      "step:  303\n",
      "loss: 1.4142\n",
      "a: -0.0077\n",
      "step:  304\n",
      "loss: 1.5576\n",
      "a: -0.0091\n",
      "step:  305\n",
      "loss: 1.4400\n",
      "a: -0.0102\n",
      "step:  306\n",
      "loss: 1.7764\n",
      "a: -0.0110\n",
      "step:  307\n",
      "loss: 1.7344\n",
      "a: -0.0115\n",
      "step:  308\n",
      "loss: 1.6417\n",
      "a: -0.0113\n",
      "step:  309\n",
      "loss: 1.5149\n",
      "a: -0.0106\n",
      "step:  310\n",
      "loss: 1.4317\n",
      "a: -0.0094\n",
      "step:  311\n",
      "loss: 1.4205\n",
      "a: -0.0081\n",
      "step:  312\n",
      "loss: 1.4711\n",
      "a: -0.0067\n",
      "step:  313\n",
      "loss: 1.3685\n",
      "a: -0.0054\n",
      "step:  314\n",
      "loss: 1.3144\n",
      "a: -0.0041\n",
      "step:  315\n",
      "loss: 1.4795\n",
      "a: -0.0029\n",
      "step:  316\n",
      "loss: 1.4846\n",
      "a: -0.0020\n",
      "step:  317\n",
      "loss: 1.3948\n",
      "a: -0.0013\n",
      "step:  318\n",
      "loss: 1.5317\n",
      "a: -0.0010\n",
      "step:  319\n",
      "loss: 1.4584\n",
      "a: -0.0018\n",
      "step:  320\n",
      "loss: 1.3177\n",
      "a: -0.0030\n",
      "step:  321\n",
      "loss: 1.5132\n",
      "a: -0.0043\n",
      "step:  322\n",
      "loss: 1.4429\n",
      "a: -0.0054\n",
      "step:  323\n",
      "loss: 1.3838\n",
      "a: -0.0065\n",
      "step:  324\n",
      "loss: 1.3738\n",
      "a: -0.0073\n",
      "step:  325\n",
      "loss: 1.3955\n",
      "a: -0.0079\n",
      "step:  326\n",
      "loss: 1.5592\n",
      "a: -0.0084\n",
      "step:  327\n",
      "loss: 1.6626\n",
      "a: -0.0087\n",
      "step:  328\n",
      "loss: 1.6361\n",
      "a: -0.0089\n",
      "step:  329\n",
      "loss: 1.3335\n",
      "a: -0.0086\n",
      "step:  330\n",
      "loss: 1.4324\n",
      "a: -0.0081\n",
      "step:  331\n",
      "loss: 1.3349\n",
      "a: -0.0073\n",
      "step:  332\n",
      "loss: 1.3686\n",
      "a: -0.0063\n",
      "step:  333\n",
      "loss: 1.4735\n",
      "a: -0.0050\n",
      "step:  334\n",
      "loss: 1.2658\n",
      "a: -0.0036\n",
      "step:  335\n",
      "loss: 1.7588\n",
      "a: -0.0023\n",
      "step:  336\n",
      "loss: 1.3266\n",
      "a: -0.0017\n",
      "step:  337\n",
      "loss: 1.5655\n",
      "a: -0.0017\n",
      "step:  338\n",
      "loss: 1.3494\n",
      "a: -0.0024\n",
      "step:  339\n",
      "loss: 1.4804\n",
      "a: -0.0035\n",
      "step:  340\n",
      "loss: 1.5241\n",
      "a: -0.0045\n",
      "step:  341\n",
      "loss: 1.5804\n",
      "a: -0.0054\n",
      "step:  342\n",
      "loss: 1.3673\n",
      "a: -0.0061\n",
      "step:  343\n",
      "loss: 1.5484\n",
      "a: -0.0064\n",
      "step:  344\n",
      "loss: 1.5041\n",
      "a: -0.0066\n",
      "step:  345\n",
      "loss: 1.3817\n",
      "a: -0.0064\n",
      "step:  346\n",
      "loss: 1.3506\n",
      "a: -0.0059\n",
      "step:  347\n",
      "loss: 1.4246\n",
      "a: -0.0050\n",
      "step:  348\n",
      "loss: 1.3054\n",
      "a: -0.0038\n",
      "step:  349\n",
      "loss: 1.3737\n",
      "a: -0.0029\n",
      "step:  350\n",
      "loss: 1.5774\n",
      "a: -0.0025\n",
      "step:  351\n",
      "loss: 1.4159\n",
      "a: -0.0028\n",
      "step:  352\n",
      "loss: 1.4766\n",
      "a: -0.0029\n",
      "step:  353\n",
      "loss: 1.5666\n",
      "a: -0.0026\n",
      "step:  354\n",
      "loss: 1.5964\n",
      "a: -0.0029\n",
      "step:  355\n",
      "loss: 1.5003\n",
      "a: -0.0032\n",
      "step:  356\n",
      "loss: 1.2437\n",
      "a: -0.0035\n",
      "step:  357\n",
      "loss: 1.2803\n",
      "a: -0.0039\n",
      "step:  358\n",
      "loss: 1.5494\n",
      "a: -0.0038\n",
      "step:  359\n",
      "loss: 1.2628\n",
      "a: -0.0041\n",
      "step:  360\n",
      "loss: 1.4170\n",
      "a: -0.0046\n",
      "step:  361\n",
      "loss: 1.4638\n",
      "a: -0.0054\n",
      "step:  362\n",
      "loss: 1.6000\n",
      "a: -0.0060\n",
      "step:  363\n",
      "loss: 1.4271\n",
      "a: -0.0066\n",
      "step:  364\n",
      "loss: 1.3582\n",
      "a: -0.0062\n",
      "step:  365\n",
      "loss: 1.4264\n",
      "a: -0.0050\n",
      "step:  366\n",
      "loss: 1.5278\n",
      "a: -0.0033\n",
      "step:  367\n",
      "loss: 1.3411\n",
      "a: -0.0019\n",
      "step:  368\n",
      "loss: 1.7706\n",
      "a: -0.0013\n",
      "step:  369\n",
      "loss: 1.4430\n",
      "a: -0.0034\n",
      "step:  370\n",
      "loss: 1.4483\n",
      "a: -0.0055\n",
      "step:  371\n",
      "loss: 1.5394\n",
      "a: -0.0072\n",
      "step:  372\n",
      "loss: 1.6347\n",
      "a: -0.0081\n",
      "step:  373\n",
      "loss: 1.4695\n",
      "a: -0.0083\n",
      "step:  374\n",
      "loss: 1.5499\n",
      "a: -0.0080\n",
      "step:  375\n",
      "loss: 1.6720\n",
      "a: -0.0067\n",
      "step:  376\n",
      "loss: 1.1936\n",
      "a: -0.0049\n",
      "step:  377\n",
      "loss: 1.4614\n",
      "a: -0.0032\n",
      "step:  378\n",
      "loss: 1.3751\n",
      "a: -0.0015\n",
      "step:  379\n",
      "loss: 1.8183\n",
      "a: 0.0001\n",
      "step:  380\n",
      "loss: 1.1914\n",
      "a: -0.0024\n",
      "step:  381\n",
      "loss: 1.3997\n",
      "a: -0.0045\n",
      "step:  382\n",
      "loss: 1.3573\n",
      "a: -0.0063\n",
      "step:  383\n",
      "loss: 1.7872\n",
      "a: -0.0079\n",
      "step:  384\n",
      "loss: 1.4352\n",
      "a: -0.0093\n",
      "step:  385\n",
      "loss: 1.5285\n",
      "a: -0.0105\n",
      "step:  386\n",
      "loss: 1.7676\n",
      "a: -0.0115\n",
      "step:  387\n",
      "loss: 1.4668\n",
      "a: -0.0122\n",
      "step:  388\n",
      "loss: 1.8264\n",
      "a: -0.0127\n",
      "step:  389\n",
      "loss: 1.7786\n",
      "a: -0.0129\n",
      "step:  390\n",
      "loss: 1.8431\n",
      "a: -0.0128\n",
      "step:  391\n",
      "loss: 1.8258\n",
      "a: -0.0124\n",
      "step:  392\n",
      "loss: 1.7676\n",
      "a: -0.0116\n",
      "step:  393\n",
      "loss: 1.2490\n",
      "a: -0.0104\n",
      "step:  394\n",
      "loss: 1.5124\n",
      "a: -0.0092\n",
      "step:  395\n",
      "loss: 1.4598\n",
      "a: -0.0079\n",
      "step:  396\n",
      "loss: 1.4803\n",
      "a: -0.0067\n",
      "step:  397\n",
      "loss: 1.5367\n",
      "a: -0.0055\n",
      "step:  398\n",
      "loss: 1.7900\n",
      "a: -0.0044\n",
      "step:  399\n",
      "loss: 1.4096\n",
      "a: -0.0035\n",
      "step:  400\n",
      "loss: 1.3900\n",
      "a: -0.0027\n",
      "step:  401\n",
      "loss: 1.7690\n",
      "a: -0.0022\n",
      "step:  402\n",
      "loss: 1.6105\n",
      "a: -0.0020\n",
      "step:  403\n",
      "loss: 1.6909\n",
      "a: -0.0021\n",
      "step:  404\n",
      "loss: 1.6312\n",
      "a: -0.0027\n",
      "step:  405\n",
      "loss: 1.7770\n",
      "a: -0.0035\n",
      "step:  406\n",
      "loss: 1.5975\n",
      "a: -0.0045\n",
      "step:  407\n",
      "loss: 1.5194\n",
      "a: -0.0056\n",
      "step:  408\n",
      "loss: 1.6813\n",
      "a: -0.0068\n",
      "step:  409\n",
      "loss: 1.4421\n",
      "a: -0.0079\n",
      "step:  410\n",
      "loss: 1.5364\n",
      "a: -0.0090\n",
      "step:  411\n",
      "loss: 1.6065\n",
      "a: -0.0099\n",
      "step:  412\n",
      "loss: 1.4862\n",
      "a: -0.0107\n",
      "step:  413\n",
      "loss: 1.4724\n",
      "a: -0.0114\n",
      "step:  414\n",
      "loss: 1.6989\n",
      "a: -0.0118\n",
      "step:  415\n",
      "loss: 1.8300\n",
      "a: -0.0118\n",
      "step:  416\n",
      "loss: 1.5716\n",
      "a: -0.0115\n",
      "step:  417\n",
      "loss: 1.3857\n",
      "a: -0.0110\n",
      "step:  418\n",
      "loss: 1.6650\n",
      "a: -0.0102\n",
      "step:  419\n",
      "loss: 1.6840\n",
      "a: -0.0092\n",
      "step:  420\n",
      "loss: 1.5038\n",
      "a: -0.0081\n",
      "step:  421\n",
      "loss: 1.6525\n",
      "a: -0.0069\n",
      "step:  422\n",
      "loss: 1.4078\n",
      "a: -0.0056\n",
      "step:  423\n",
      "loss: 1.5590\n",
      "a: -0.0043\n",
      "step:  424\n",
      "loss: 1.3243\n",
      "a: -0.0031\n",
      "step:  425\n",
      "loss: 1.3945\n",
      "a: -0.0018\n",
      "step:  426\n",
      "loss: 1.6448\n",
      "a: -0.0008\n",
      "step:  427\n",
      "loss: 1.9456\n",
      "a: 0.0001\n",
      "step:  428\n",
      "loss: 1.5604\n",
      "a: 0.0003\n",
      "step:  429\n",
      "loss: 1.3839\n",
      "a: -0.0024\n",
      "step:  430\n",
      "loss: 1.5211\n",
      "a: -0.0045\n",
      "step:  431\n",
      "loss: 1.7385\n",
      "a: -0.0064\n",
      "step:  432\n",
      "loss: 1.3746\n",
      "a: -0.0080\n",
      "step:  433\n",
      "loss: 1.5746\n",
      "a: -0.0095\n",
      "step:  434\n",
      "loss: 1.8084\n",
      "a: -0.0109\n",
      "step:  435\n",
      "loss: 1.8677\n",
      "a: -0.0121\n",
      "step:  436\n",
      "loss: 1.7727\n",
      "a: -0.0132\n",
      "step:  437\n",
      "loss: 1.9030\n",
      "a: -0.0141\n",
      "step:  438\n",
      "loss: 2.2071\n",
      "a: -0.0150\n",
      "step:  439\n",
      "loss: 1.9243\n",
      "a: -0.0157\n",
      "step:  440\n",
      "loss: 2.1869\n",
      "a: -0.0162\n",
      "step:  441\n",
      "loss: 2.0041\n",
      "a: -0.0165\n",
      "step:  442\n",
      "loss: 2.3599\n",
      "a: -0.0167\n",
      "step:  443\n",
      "loss: 1.9475\n",
      "a: -0.0167\n",
      "step:  444\n",
      "loss: 1.9053\n",
      "a: -0.0165\n",
      "step:  445\n",
      "loss: 1.7523\n",
      "a: -0.0161\n",
      "step:  446\n",
      "loss: 2.0476\n",
      "a: -0.0155\n",
      "step:  447\n",
      "loss: 2.0135\n",
      "a: -0.0147\n",
      "step:  448\n",
      "loss: 1.8224\n",
      "a: -0.0137\n",
      "step:  449\n",
      "loss: 1.9502\n",
      "a: -0.0126\n",
      "step:  450\n",
      "loss: 1.6249\n",
      "a: -0.0115\n",
      "step:  451\n",
      "loss: 1.4393\n",
      "a: -0.0104\n",
      "step:  452\n",
      "loss: 1.7461\n",
      "a: -0.0092\n",
      "step:  453\n",
      "loss: 1.6359\n",
      "a: -0.0081\n",
      "step:  454\n",
      "loss: 1.6725\n",
      "a: -0.0069\n",
      "step:  455\n",
      "loss: 1.4999\n",
      "a: -0.0058\n",
      "step:  456\n",
      "loss: 1.6318\n",
      "a: -0.0047\n",
      "step:  457\n",
      "loss: 1.3595\n",
      "a: -0.0036\n",
      "step:  458\n",
      "loss: 1.4765\n",
      "a: -0.0026\n",
      "step:  459\n",
      "loss: 1.8408\n",
      "a: -0.0016\n",
      "step:  460\n",
      "loss: 1.6302\n",
      "a: -0.0006\n",
      "step:  461\n",
      "loss: 1.6242\n",
      "a: 0.0002\n",
      "step:  462\n",
      "loss: 1.9497\n",
      "a: 0.0004\n",
      "step:  463\n",
      "loss: 1.5056\n",
      "a: -0.0012\n",
      "step:  464\n",
      "loss: 1.4580\n",
      "a: -0.0025\n",
      "step:  465\n",
      "loss: 1.6800\n",
      "a: -0.0039\n",
      "step:  466\n",
      "loss: 1.5558\n",
      "a: -0.0052\n",
      "step:  467\n",
      "loss: 1.5933\n",
      "a: -0.0065\n",
      "step:  468\n",
      "loss: 1.5844\n",
      "a: -0.0077\n",
      "step:  469\n",
      "loss: 1.5516\n",
      "a: -0.0089\n",
      "step:  470\n",
      "loss: 1.4709\n",
      "a: -0.0100\n",
      "step:  471\n",
      "loss: 1.5399\n",
      "a: -0.0111\n",
      "step:  472\n",
      "loss: 1.5777\n",
      "a: -0.0121\n",
      "step:  473\n",
      "loss: 1.8503\n",
      "a: -0.0130\n",
      "step:  474\n",
      "loss: 1.8718\n",
      "a: -0.0139\n",
      "step:  475\n",
      "loss: 1.9781\n",
      "a: -0.0147\n",
      "step:  476\n",
      "loss: 1.9116\n",
      "a: -0.0153\n",
      "step:  477\n",
      "loss: 2.1566\n",
      "a: -0.0158\n",
      "step:  478\n",
      "loss: 2.1755\n",
      "a: -0.0161\n",
      "step:  479\n",
      "loss: 1.7344\n",
      "a: -0.0162\n",
      "step:  480\n",
      "loss: 2.3249\n",
      "a: -0.0162\n",
      "step:  481\n",
      "loss: 2.1301\n",
      "a: -0.0160\n",
      "step:  482\n",
      "loss: 1.9009\n",
      "a: -0.0156\n",
      "step:  483\n",
      "loss: 2.2466\n",
      "a: -0.0150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  484\n",
      "loss: 1.8223\n",
      "a: -0.0142\n",
      "step:  485\n",
      "loss: 1.6479\n",
      "a: -0.0133\n",
      "step:  486\n",
      "loss: 1.8781\n",
      "a: -0.0122\n",
      "step:  487\n",
      "loss: 1.7053\n",
      "a: -0.0110\n",
      "step:  488\n",
      "loss: 1.6862\n",
      "a: -0.0098\n",
      "step:  489\n",
      "loss: 1.5665\n",
      "a: -0.0085\n",
      "step:  490\n",
      "loss: 1.5441\n",
      "a: -0.0072\n",
      "step:  491\n",
      "loss: 1.5239\n",
      "a: -0.0060\n",
      "step:  492\n",
      "loss: 1.6709\n",
      "a: -0.0048\n",
      "step:  493\n",
      "loss: 1.4837\n",
      "a: -0.0037\n",
      "step:  494\n",
      "loss: 1.5473\n",
      "a: -0.0026\n",
      "step:  495\n",
      "loss: 1.5519\n",
      "a: -0.0015\n",
      "step:  496\n",
      "loss: 1.8529\n",
      "a: -0.0005\n",
      "step:  497\n",
      "loss: 1.7404\n",
      "a: 0.0002\n",
      "step:  498\n",
      "loss: 1.6038\n",
      "a: -0.0001\n",
      "step:  499\n",
      "loss: 1.6841\n",
      "a: -0.0006\n",
      "step:  500\n",
      "loss: 1.5672\n",
      "a: -0.0014\n",
      "step:  501\n",
      "loss: 1.6632\n",
      "a: -0.0022\n",
      "step:  502\n",
      "loss: 1.6321\n",
      "a: -0.0032\n",
      "step:  503\n",
      "loss: 1.4383\n",
      "a: -0.0041\n",
      "step:  504\n",
      "loss: 1.6570\n",
      "a: -0.0051\n",
      "step:  505\n",
      "loss: 1.4947\n",
      "a: -0.0061\n",
      "step:  506\n",
      "loss: 1.6434\n",
      "a: -0.0070\n",
      "step:  507\n",
      "loss: 1.5852\n",
      "a: -0.0079\n",
      "step:  508\n",
      "loss: 1.6863\n",
      "a: -0.0088\n",
      "step:  509\n",
      "loss: 1.5260\n",
      "a: -0.0097\n",
      "step:  510\n",
      "loss: 1.6556\n",
      "a: -0.0104\n",
      "step:  511\n",
      "loss: 1.5281\n",
      "a: -0.0112\n",
      "step:  512\n",
      "loss: 1.5763\n",
      "a: -0.0118\n",
      "step:  513\n",
      "loss: 1.9250\n",
      "a: -0.0124\n",
      "step:  514\n",
      "loss: 1.7211\n",
      "a: -0.0128\n",
      "step:  515\n",
      "loss: 1.5164\n",
      "a: -0.0132\n",
      "step:  516\n",
      "loss: 1.9687\n",
      "a: -0.0133\n",
      "step:  517\n",
      "loss: 1.7745\n",
      "a: -0.0133\n",
      "step:  518\n",
      "loss: 1.6445\n",
      "a: -0.0129\n",
      "step:  519\n",
      "loss: 1.7095\n",
      "a: -0.0125\n",
      "step:  520\n",
      "loss: 1.7216\n",
      "a: -0.0118\n",
      "step:  521\n",
      "loss: 1.7386\n",
      "a: -0.0111\n",
      "step:  522\n",
      "loss: 1.8622\n",
      "a: -0.0101\n",
      "step:  523\n",
      "loss: 1.6694\n",
      "a: -0.0091\n",
      "step:  524\n",
      "loss: 1.6177\n",
      "a: -0.0080\n",
      "step:  525\n",
      "loss: 1.5338\n",
      "a: -0.0069\n",
      "step:  526\n",
      "loss: 1.5866\n",
      "a: -0.0057\n",
      "step:  527\n",
      "loss: 1.6463\n",
      "a: -0.0044\n",
      "step:  528\n",
      "loss: 1.5078\n",
      "a: -0.0032\n",
      "step:  529\n",
      "loss: 1.8282\n",
      "a: -0.0021\n",
      "step:  530\n",
      "loss: 1.5575\n",
      "a: -0.0011\n",
      "step:  531\n",
      "loss: 1.6924\n",
      "a: -0.0002\n",
      "step:  532\n",
      "loss: 2.4011\n",
      "a: 0.0006\n",
      "step:  533\n",
      "loss: 1.6422\n",
      "a: -0.0020\n",
      "step:  534\n",
      "loss: 1.4662\n",
      "a: -0.0041\n",
      "step:  535\n",
      "loss: 1.2930\n",
      "a: -0.0060\n",
      "step:  536\n",
      "loss: 1.5241\n",
      "a: -0.0076\n",
      "step:  537\n",
      "loss: 1.6975\n",
      "a: -0.0091\n",
      "step:  538\n",
      "loss: 1.4229\n",
      "a: -0.0104\n",
      "step:  539\n",
      "loss: 1.7580\n",
      "a: -0.0117\n",
      "step:  540\n",
      "loss: 1.7183\n",
      "a: -0.0129\n",
      "step:  541\n",
      "loss: 2.0004\n",
      "a: -0.0141\n",
      "step:  542\n",
      "loss: 2.1851\n",
      "a: -0.0152\n",
      "step:  543\n",
      "loss: 2.1807\n",
      "a: -0.0162\n",
      "step:  544\n",
      "loss: 2.2652\n",
      "a: -0.0171\n",
      "step:  545\n",
      "loss: 2.1195\n",
      "a: -0.0179\n",
      "step:  546\n",
      "loss: 2.0966\n",
      "a: -0.0187\n",
      "step:  547\n",
      "loss: 2.4934\n",
      "a: -0.0193\n",
      "step:  548\n",
      "loss: 2.6251\n",
      "a: -0.0199\n",
      "step:  549\n",
      "loss: 2.4641\n",
      "a: -0.0203\n",
      "step:  550\n",
      "loss: 2.4123\n",
      "a: -0.0205\n",
      "step:  551\n",
      "loss: 2.4635\n",
      "a: -0.0207\n",
      "step:  552\n",
      "loss: 2.4450\n",
      "a: -0.0207\n",
      "step:  553\n",
      "loss: 2.2607\n",
      "a: -0.0205\n",
      "step:  554\n",
      "loss: 2.2623\n",
      "a: -0.0201\n",
      "step:  555\n",
      "loss: 2.1236\n",
      "a: -0.0196\n",
      "step:  556\n",
      "loss: 1.9308\n",
      "a: -0.0189\n",
      "step:  557\n",
      "loss: 2.1605\n",
      "a: -0.0180\n",
      "step:  558\n",
      "loss: 2.3417\n",
      "a: -0.0170\n",
      "step:  559\n",
      "loss: 1.7416\n",
      "a: -0.0158\n",
      "step:  560\n",
      "loss: 1.9004\n",
      "a: -0.0146\n",
      "step:  561\n",
      "loss: 1.7424\n",
      "a: -0.0133\n",
      "step:  562\n",
      "loss: 1.6182\n",
      "a: -0.0120\n",
      "step:  563\n",
      "loss: 1.4844\n",
      "a: -0.0108\n",
      "step:  564\n",
      "loss: 1.7367\n",
      "a: -0.0095\n",
      "step:  565\n",
      "loss: 1.6736\n",
      "a: -0.0083\n",
      "step:  566\n",
      "loss: 1.8476\n",
      "a: -0.0071\n",
      "step:  567\n",
      "loss: 1.8394\n",
      "a: -0.0060\n",
      "step:  568\n",
      "loss: 1.6672\n",
      "a: -0.0048\n",
      "step:  569\n",
      "loss: 1.8599\n",
      "a: -0.0038\n",
      "step:  570\n",
      "loss: 2.3641\n",
      "a: -0.0028\n",
      "step:  571\n",
      "loss: 1.8664\n",
      "a: -0.0019\n",
      "step:  572\n",
      "loss: 1.6074\n",
      "a: -0.0010\n",
      "step:  573\n",
      "loss: 1.3696\n",
      "a: -0.0002\n",
      "step:  574\n",
      "loss: 2.2987\n",
      "a: 0.0005\n",
      "step:  575\n",
      "loss: 2.0533\n",
      "a: 0.0003\n",
      "step:  576\n",
      "loss: 1.4549\n",
      "a: -0.0008\n",
      "step:  577\n",
      "loss: 1.7564\n",
      "a: -0.0020\n",
      "step:  578\n",
      "loss: 1.6598\n",
      "a: -0.0032\n",
      "step:  579\n",
      "loss: 1.5558\n",
      "a: -0.0044\n",
      "step:  580\n",
      "loss: 1.6511\n",
      "a: -0.0056\n",
      "step:  581\n",
      "loss: 1.5704\n",
      "a: -0.0068\n",
      "step:  582\n",
      "loss: 1.4829\n",
      "a: -0.0080\n",
      "step:  583\n",
      "loss: 1.5268\n",
      "a: -0.0090\n",
      "step:  584\n",
      "loss: 1.5883\n",
      "a: -0.0101\n",
      "step:  585\n",
      "loss: 1.7334\n",
      "a: -0.0111\n",
      "step:  586\n",
      "loss: 2.0150\n",
      "a: -0.0121\n",
      "step:  587\n",
      "loss: 1.7514\n",
      "a: -0.0130\n",
      "step:  588\n",
      "loss: 2.1706\n",
      "a: -0.0139\n",
      "step:  589\n",
      "loss: 1.9335\n",
      "a: -0.0147\n",
      "step:  590\n",
      "loss: 2.0530\n",
      "a: -0.0154\n",
      "step:  591\n",
      "loss: 2.1806\n",
      "a: -0.0160\n",
      "step:  592\n",
      "loss: 2.2779\n",
      "a: -0.0165\n",
      "step:  593\n",
      "loss: 2.2316\n",
      "a: -0.0169\n",
      "step:  594\n",
      "loss: 2.4358\n",
      "a: -0.0172\n",
      "step:  595\n",
      "loss: 2.4354\n",
      "a: -0.0173\n",
      "step:  596\n",
      "loss: 2.0021\n",
      "a: -0.0173\n",
      "step:  597\n",
      "loss: 2.2460\n",
      "a: -0.0171\n",
      "step:  598\n",
      "loss: 1.9036\n",
      "a: -0.0167\n",
      "step:  599\n",
      "loss: 2.0858\n",
      "a: -0.0162\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {:3f}\".format(cost))\n",
    "    print('step: ', _)\n",
    "    print('loss: %.4f' % cost)\n",
    "    print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As one can see, the PRelu is adaptedin each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code snippets to build the convolutional network:\n",
    "\n",
    "```python\n",
    "    from torch . nn . functional import conv2d , max_pool2d\n",
    "    convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "    subsampleing_layer = max_pool_2d ( convolutional_layer , (2 , 2) ) # reduces window 2x2 to 1 pixel\n",
    "    out_layer = dropout ( subsample_layer , p_drop_input )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output pixels =  64\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "lr = 3e-5\n",
    "\n",
    "# given on exercise sheet\n",
    "f1, f2, f3 = 32, 64, 128\n",
    "pic_in1, pic_in2, pic_in3 = 1, 32, 64 \n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "# modify for more speed\n",
    "f1, f2, f3 = 16, 32, 64\n",
    "pic_in1, pic_in2, pic_in3 = 1, 16, 32\n",
    "k_x1, k_x2, k_x3 = 5, 5, 2\n",
    "k_y1, k_y2, k_y3 = 5, 5, 2\n",
    "\n",
    "\n",
    "activation = 'prelu'\n",
    "\n",
    "\n",
    "w_conv1 = init_weights((f1, pic_in1, k_x1, k_y1))\n",
    "w_conv2 = init_weights((f2, pic_in2, k_x2, k_y2))\n",
    "w_conv3 = init_weights((f3, pic_in3, k_x3, k_y3))\n",
    "\n",
    "def conv_layer(X, weightvector, p_drop):\n",
    "    X = rectify(conv2d (X, weightvector))\n",
    "    X = max_pool2d(X, (2 , 2)) # reduces window 2x2 to 1 pixel\n",
    "    return dropout(X, p_drop)\n",
    "\n",
    "def get_num_output_pix(w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "    def cnn_pre(X, w_conv1, w_conv2, w_conv3, p_drop_input):\n",
    "        X = conv_layer(X, w_conv1, p_drop_input)\n",
    "        X = conv_layer(X, w_conv2, p_drop_input)\n",
    "        X = conv_layer(X, w_conv3, p_drop_input)\n",
    "        return X\n",
    "    Y = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "    # get output size\n",
    "    Y = cnn_pre(Y, w_conv1, w_conv2, w_conv3, p_drop_input)\n",
    "    return Y.size()[1]\n",
    "\n",
    "number_of_output_pixels = get_num_output_pix(w_conv1, w_conv2, w_conv3, 0.5)\n",
    "print('number of output pixels = ', number_of_output_pixels)\n",
    "\n",
    "# given on exercise sheet\n",
    "w_h2 = init_weights((number_of_output_pixels, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "# modify for more speed\n",
    "w_h2 = init_weights((number_of_output_pixels, 250))\n",
    "w_o = init_weights((250, 10))\n",
    "\n",
    "# in case pReLU is needed:\n",
    "if activation == 'prelu':\n",
    "    a = torch.tensor([-0.1], requires_grad = True)\n",
    "elif activation == 'relu':\n",
    "    a = torch.tensor([0.], requires_grad = False)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')\n",
    "\n",
    "if activation == 'prelu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o, a], lr = lr)\n",
    "elif activation == 'relu':\n",
    "    optimizer = RMSprop([w_conv1, w_conv2, w_conv3, w_h2, w_o], lr = lr)\n",
    "else:\n",
    "    print('Please enter valid activation function (either relu or prelu)')    \n",
    "\n",
    "# add a here if running with pReLU\n",
    "def cnn(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    X = X.reshape(mb_size, number_of_output_pixels)\n",
    "    h2 = PRelu(X @ w_h2, a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train(train_loader, epoch, log_interval):\n",
    "    # print to screen every log_interval\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        pre_softmax = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        #output = softmax(pre_softmax)\n",
    "        # note: torch.nn.functional.cross_entropy applies log_softmax\n",
    "        loss = torch.nn.functional.cross_entropy(pre_softmax, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #print('pre_soft size: ', pre_softmax.size())\n",
    "            #print('target size: ', target.size())\n",
    "            #print('loss size: ', loss.size())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "# define test loop\n",
    "def test(test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = cnn(data.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target) # returns average over minibatch\n",
    "        test_loss += loss # maybe loss.data[0] ?  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum() # sum up pair-wise equalities (marked with 1, others 0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(train_loader, test_loader, num_epochs, log_interval):\n",
    "    # run training\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, epoch, log_interval)\n",
    "        test(test_loader)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 18.8760\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 5.9904\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.7458\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.6818\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.3205\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.2437\n",
      "\n",
      "Test set: Average loss: 0.0456, Accuracy: 1513/10000 (15%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.2908\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.2282\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.2839\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.2467\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.2303\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.2722\n",
      "\n",
      "Test set: Average loss: 0.0446, Accuracy: 1707/10000 (17%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.2839\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.1052\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.1819\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.0948\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.9906\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.9471\n",
      "\n",
      "Test set: Average loss: 0.0394, Accuracy: 2709/10000 (27%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.9523\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.7335\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.7523\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.7650\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.8191\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.7967\n",
      "\n",
      "Test set: Average loss: 0.0338, Accuracy: 3849/10000 (38%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.4762\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.7282\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.7589\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.4197\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.6448\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.5572\n",
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 4608/10000 (46%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.3089\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.3960\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.8333\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.3564\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.4032\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.0568\n",
      "\n",
      "Test set: Average loss: 0.0264, Accuracy: 5258/10000 (52%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.3036\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.5390\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.3823\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.3688\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.0806\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.2500\n",
      "\n",
      "Test set: Average loss: 0.0250, Accuracy: 5709/10000 (57%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.2750\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.1471\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.9855\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.2948\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.4041\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.4840\n",
      "\n",
      "Test set: Average loss: 0.0237, Accuracy: 5982/10000 (59%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.4202\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.0942\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.2607\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.0916\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.0523\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.0923\n",
      "\n",
      "Test set: Average loss: 0.0227, Accuracy: 6236/10000 (62%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.3110\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.8844\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.2503\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.8446\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.1471\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.1141\n",
      "\n",
      "Test set: Average loss: 0.0220, Accuracy: 6269/10000 (62%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.2329\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 1.0910\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 1.3691\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 1.2059\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 1.1740\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 1.1848\n",
      "\n",
      "Test set: Average loss: 0.0219, Accuracy: 6415/10000 (64%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.0647\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 1.1554\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 1.3672\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 0.9906\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.8950\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 1.0821\n",
      "\n",
      "Test set: Average loss: 0.0216, Accuracy: 6413/10000 (64%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.0796\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.9845\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 1.2722\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 0.9882\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 1.0012\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 1.7228\n",
      "\n",
      "Test set: Average loss: 0.0220, Accuracy: 6366/10000 (63%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.0260\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.9002\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.8687\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 1.1002\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 1.3226\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.8662\n",
      "\n",
      "Test set: Average loss: 0.0223, Accuracy: 6320/10000 (63%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.0175\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 1.2404\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 1.2634\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 1.0489\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 1.0461\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 1.0402\n",
      "\n",
      "Test set: Average loss: 0.0212, Accuracy: 6674/10000 (66%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.7999\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 1.6520\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 1.0653\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 1.0884\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 1.1390\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 1.5211\n",
      "\n",
      "Test set: Average loss: 0.0221, Accuracy: 6589/10000 (65%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.9514\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 1.1560\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.6480\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 1.5160\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 1.0073\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 0.7370\n",
      "\n",
      "Test set: Average loss: 0.0215, Accuracy: 6548/10000 (65%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.8836\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 0.8049\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 1.0270\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 1.6601\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.7894\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 1.0234\n",
      "\n",
      "Test set: Average loss: 0.0221, Accuracy: 6598/10000 (65%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.1965\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 1.0523\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.9082\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 0.9084\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.8066\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 1.0576\n",
      "\n",
      "Test set: Average loss: 0.0217, Accuracy: 6669/10000 (66%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.0751\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 1.1317\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 0.9371\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 0.8365\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 1.0753\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 20\n",
    "log_interval = 200\n",
    "\n",
    "run_model(trainloader, testloader, N_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-03 *\n",
      "       [ 4.2836])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## issues:\n",
    " - find nice hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8749\n",
      "loss: 1.8950\n",
      "loss: 1.8771\n",
      "loss: 1.8343\n",
      "loss: 1.9477\n",
      "loss: 1.9044\n",
      "loss: 1.8572\n",
      "loss: 1.9090\n",
      "loss: 1.8267\n",
      "loss: 1.8636\n",
      "loss: 1.9367\n",
      "loss: 1.9012\n",
      "loss: 1.9369\n",
      "loss: 1.8933\n",
      "loss: 1.8630\n",
      "loss: 1.8968\n",
      "loss: 1.9009\n",
      "loss: 1.8796\n",
      "loss: 1.8150\n",
      "loss: 1.9533\n",
      "loss: 1.8822\n",
      "loss: 1.9460\n",
      "loss: 1.8386\n",
      "loss: 1.9072\n",
      "loss: 1.8292\n",
      "loss: 1.9125\n",
      "loss: 1.8710\n",
      "loss: 1.8571\n",
      "loss: 1.9005\n",
      "loss: 1.8392\n",
      "loss: 1.8970\n",
      "loss: 1.8447\n",
      "loss: 1.8780\n",
      "loss: 1.8806\n",
      "loss: 1.8873\n",
      "loss: 1.8552\n",
      "loss: 1.8339\n",
      "loss: 1.8392\n",
      "loss: 1.8212\n",
      "loss: 1.8735\n",
      "loss: 1.8874\n",
      "loss: 1.8615\n",
      "loss: 1.9124\n",
      "loss: 1.8411\n",
      "loss: 1.8206\n",
      "loss: 1.8935\n",
      "loss: 1.8432\n",
      "loss: 1.9238\n",
      "loss: 1.9352\n",
      "loss: 1.8857\n",
      "loss: 1.8065\n",
      "loss: 1.8790\n",
      "loss: 1.8132\n",
      "loss: 1.8741\n",
      "loss: 1.9009\n",
      "loss: 1.8336\n",
      "loss: 1.8713\n",
      "loss: 1.8421\n",
      "loss: 1.7888\n",
      "loss: 1.7799\n",
      "loss: 1.7837\n",
      "loss: 1.8092\n",
      "loss: 1.8242\n",
      "loss: 1.8140\n",
      "loss: 1.8656\n",
      "loss: 1.8403\n",
      "loss: 1.8153\n",
      "loss: 1.8496\n",
      "loss: 1.8439\n",
      "loss: 1.8918\n",
      "loss: 1.8485\n",
      "loss: 1.8378\n",
      "loss: 1.8581\n",
      "loss: 1.7738\n",
      "loss: 1.8080\n",
      "loss: 1.8133\n",
      "loss: 1.8092\n",
      "loss: 1.8143\n",
      "loss: 1.8490\n",
      "loss: 1.7667\n",
      "loss: 1.7212\n",
      "loss: 1.8062\n",
      "loss: 1.7978\n",
      "loss: 1.7868\n",
      "loss: 1.7989\n",
      "loss: 1.8277\n",
      "loss: 1.7998\n",
      "loss: 1.8413\n",
      "loss: 1.8210\n",
      "loss: 1.8264\n",
      "loss: 1.8087\n",
      "loss: 1.7470\n",
      "loss: 1.7644\n",
      "loss: 1.8610\n",
      "loss: 1.8446\n",
      "loss: 1.7186\n",
      "loss: 1.8349\n",
      "loss: 1.8434\n",
      "loss: 1.8812\n",
      "loss: 1.8365\n",
      "loss: 1.7569\n",
      "loss: 1.8365\n",
      "loss: 1.8170\n",
      "loss: 1.8175\n",
      "loss: 1.8149\n",
      "loss: 1.8459\n",
      "loss: 1.8367\n",
      "loss: 1.8607\n",
      "loss: 1.7893\n",
      "loss: 1.7572\n",
      "loss: 1.8453\n",
      "loss: 1.7716\n",
      "loss: 1.8107\n",
      "loss: 1.7477\n",
      "loss: 1.7281\n",
      "loss: 1.8440\n",
      "loss: 1.8014\n",
      "loss: 1.7704\n",
      "loss: 1.7524\n",
      "loss: 1.7771\n",
      "loss: 1.8088\n",
      "loss: 1.8187\n",
      "loss: 1.7149\n",
      "loss: 1.7950\n",
      "loss: 1.7776\n",
      "loss: 1.7011\n",
      "loss: 1.7546\n",
      "loss: 1.8485\n",
      "loss: 1.7178\n",
      "loss: 1.8066\n",
      "loss: 1.8186\n",
      "loss: 1.8279\n",
      "loss: 1.7982\n",
      "loss: 1.8055\n",
      "loss: 1.8498\n",
      "loss: 1.8045\n",
      "loss: 1.7782\n",
      "loss: 1.8023\n",
      "loss: 1.7961\n",
      "loss: 1.8064\n",
      "loss: 1.7273\n",
      "loss: 1.8281\n",
      "loss: 1.7170\n",
      "loss: 1.7340\n",
      "loss: 1.8307\n",
      "loss: 1.7604\n",
      "loss: 1.8114\n",
      "loss: 1.8612\n",
      "loss: 1.8577\n",
      "loss: 1.7611\n",
      "loss: 1.7591\n",
      "loss: 1.7715\n",
      "loss: 1.7855\n",
      "loss: 1.8118\n",
      "loss: 1.8048\n",
      "loss: 1.8655\n",
      "loss: 1.7945\n",
      "loss: 1.7455\n",
      "loss: 1.8127\n",
      "loss: 1.8343\n",
      "loss: 1.7884\n",
      "loss: 1.7463\n",
      "loss: 1.8200\n",
      "loss: 1.7785\n",
      "loss: 1.8280\n",
      "loss: 1.7588\n",
      "loss: 1.8705\n",
      "loss: 1.7862\n",
      "loss: 1.7775\n",
      "loss: 1.7717\n",
      "loss: 1.8224\n",
      "loss: 1.8200\n",
      "loss: 1.8641\n",
      "loss: 1.8545\n",
      "loss: 1.7639\n",
      "loss: 1.7736\n",
      "loss: 1.8280\n",
      "loss: 1.7653\n",
      "loss: 1.7755\n",
      "loss: 1.8216\n",
      "loss: 1.7840\n",
      "loss: 1.7251\n",
      "loss: 1.7466\n",
      "loss: 1.8783\n",
      "loss: 1.7765\n",
      "loss: 1.7904\n",
      "loss: 1.8268\n",
      "loss: 1.7991\n",
      "loss: 1.7047\n",
      "loss: 1.7155\n",
      "loss: 1.7693\n",
      "loss: 1.8160\n",
      "loss: 1.8804\n",
      "loss: 1.7812\n",
      "loss: 1.8191\n",
      "loss: 1.7796\n",
      "loss: 1.7781\n",
      "loss: 1.8009\n",
      "loss: 1.8103\n",
      "loss: 1.7777\n",
      "loss: 1.7534\n",
      "loss: 1.7748\n",
      "loss: 1.7492\n",
      "loss: 1.7470\n",
      "loss: 1.7840\n",
      "loss: 1.8245\n",
      "loss: 1.7622\n",
      "loss: 1.7846\n",
      "loss: 1.7340\n",
      "loss: 1.7478\n",
      "loss: 1.7943\n",
      "loss: 1.7740\n",
      "loss: 1.8417\n",
      "loss: 1.7471\n",
      "loss: 1.7688\n",
      "loss: 1.7131\n",
      "loss: 1.8301\n",
      "loss: 1.8089\n",
      "loss: 1.7332\n",
      "loss: 1.8105\n",
      "loss: 1.8188\n",
      "loss: 1.8181\n",
      "loss: 1.7643\n",
      "loss: 1.7466\n",
      "loss: 1.8221\n",
      "loss: 1.8020\n",
      "loss: 1.7842\n",
      "loss: 1.8433\n",
      "loss: 1.8490\n",
      "loss: 1.8741\n",
      "loss: 1.8447\n",
      "loss: 1.7906\n",
      "loss: 1.8628\n",
      "loss: 1.8161\n",
      "loss: 1.8070\n",
      "loss: 1.8369\n",
      "loss: 1.8441\n",
      "loss: 1.8109\n",
      "loss: 1.7542\n",
      "loss: 1.7959\n",
      "loss: 1.8048\n",
      "loss: 1.8066\n",
      "loss: 1.7989\n",
      "loss: 1.7936\n",
      "loss: 1.8734\n",
      "loss: 1.8398\n",
      "loss: 1.8219\n",
      "loss: 1.8411\n",
      "loss: 1.7869\n",
      "loss: 1.7899\n",
      "loss: 1.8381\n",
      "loss: 1.7723\n",
      "loss: 1.7982\n",
      "loss: 1.8288\n",
      "loss: 1.7167\n",
      "loss: 1.8450\n",
      "loss: 1.7911\n",
      "loss: 1.7450\n",
      "loss: 1.7723\n",
      "loss: 1.7922\n",
      "loss: 1.8678\n",
      "loss: 1.7755\n",
      "loss: 1.7794\n",
      "loss: 1.6412\n",
      "loss: 1.9281\n",
      "loss: 1.8779\n",
      "loss: 1.7925\n",
      "loss: 1.7298\n",
      "loss: 1.8836\n",
      "loss: 1.8198\n",
      "loss: 1.7939\n",
      "loss: 1.8496\n",
      "loss: 1.7610\n",
      "loss: 1.8913\n",
      "loss: 1.7776\n",
      "loss: 1.8266\n",
      "loss: 1.7586\n",
      "loss: 1.7662\n",
      "loss: 1.8084\n",
      "loss: 1.7702\n",
      "loss: 1.7902\n",
      "loss: 1.8589\n",
      "loss: 1.7464\n",
      "loss: 1.7878\n",
      "loss: 1.8075\n",
      "loss: 1.8407\n",
      "loss: 1.8290\n",
      "loss: 1.7582\n",
      "loss: 1.7793\n",
      "loss: 1.7678\n",
      "loss: 1.8200\n",
      "loss: 1.7218\n",
      "loss: 1.8373\n",
      "loss: 1.8145\n",
      "loss: 1.8507\n",
      "loss: 1.8251\n",
      "loss: 1.9087\n",
      "loss: 1.7484\n",
      "loss: 1.8171\n",
      "loss: 1.8695\n",
      "loss: 1.7410\n",
      "loss: 1.8256\n",
      "loss: 1.8205\n",
      "loss: 1.8125\n",
      "loss: 1.8320\n",
      "loss: 1.8050\n",
      "loss: 1.7816\n",
      "loss: 1.8397\n",
      "loss: 1.7914\n",
      "loss: 1.8188\n",
      "loss: 1.8953\n",
      "loss: 1.8480\n",
      "loss: 1.7792\n",
      "loss: 1.8490\n",
      "loss: 1.8800\n",
      "loss: 1.7964\n",
      "loss: 1.8012\n",
      "loss: 1.7894\n",
      "loss: 1.8295\n",
      "loss: 1.8902\n",
      "loss: 1.8457\n",
      "loss: 1.7190\n",
      "loss: 1.7975\n",
      "loss: 1.9044\n",
      "loss: 1.7509\n",
      "loss: 1.8612\n",
      "loss: 1.7978\n",
      "loss: 1.8799\n",
      "loss: 1.8971\n",
      "loss: 1.7890\n",
      "loss: 1.9192\n",
      "loss: 1.7682\n",
      "loss: 1.8399\n",
      "loss: 1.7142\n",
      "loss: 1.8036\n",
      "loss: 1.8707\n",
      "loss: 1.7872\n",
      "loss: 1.8330\n",
      "loss: 1.8408\n",
      "loss: 1.8175\n",
      "loss: 1.8606\n",
      "loss: 1.7996\n",
      "loss: 1.8416\n",
      "loss: 1.8000\n",
      "loss: 1.8197\n",
      "loss: 1.9092\n",
      "loss: 1.7510\n",
      "loss: 1.9104\n",
      "loss: 1.8390\n",
      "loss: 1.9024\n",
      "loss: 1.8326\n",
      "loss: 1.8876\n",
      "loss: 1.8811\n",
      "loss: 1.8803\n",
      "loss: 1.8247\n",
      "loss: 1.8194\n",
      "loss: 1.8450\n",
      "loss: 1.7881\n",
      "loss: 1.8109\n",
      "loss: 1.7511\n",
      "loss: 1.7579\n",
      "loss: 1.7928\n",
      "loss: 1.8366\n",
      "loss: 1.8197\n",
      "loss: 1.7807\n",
      "loss: 1.8777\n",
      "loss: 1.8579\n",
      "loss: 1.7091\n",
      "loss: 1.7865\n",
      "loss: 1.8069\n",
      "loss: 1.8695\n",
      "loss: 1.8174\n",
      "loss: 1.8359\n",
      "loss: 1.8685\n",
      "loss: 1.8590\n",
      "loss: 1.9053\n",
      "loss: 1.8096\n",
      "loss: 1.7708\n",
      "loss: 1.8523\n",
      "loss: 1.8528\n",
      "loss: 1.8006\n",
      "loss: 1.9262\n",
      "loss: 1.8191\n",
      "loss: 1.7312\n",
      "loss: 1.7474\n",
      "loss: 1.8787\n",
      "loss: 1.9004\n",
      "loss: 1.7030\n",
      "loss: 1.7880\n",
      "loss: 1.7983\n",
      "loss: 1.8116\n",
      "loss: 1.8654\n",
      "loss: 1.8240\n",
      "loss: 1.8907\n",
      "loss: 1.8595\n",
      "loss: 1.7887\n",
      "loss: 1.6936\n",
      "loss: 1.8318\n",
      "loss: 1.7596\n",
      "loss: 1.8079\n",
      "loss: 1.8583\n",
      "loss: 1.7744\n",
      "loss: 1.8237\n",
      "loss: 1.8457\n",
      "loss: 1.8197\n",
      "loss: 1.8240\n",
      "loss: 1.8513\n",
      "loss: 1.7964\n",
      "loss: 1.9070\n",
      "loss: 1.8379\n",
      "loss: 1.8250\n",
      "loss: 1.8903\n",
      "loss: 1.8895\n",
      "loss: 1.7750\n",
      "loss: 1.8647\n",
      "loss: 1.8887\n",
      "loss: 1.8680\n",
      "loss: 1.8611\n",
      "loss: 1.8477\n",
      "loss: 1.8178\n",
      "loss: 1.8308\n",
      "loss: 1.9180\n",
      "loss: 1.9132\n",
      "loss: 1.8377\n",
      "loss: 1.8714\n",
      "loss: 1.8597\n",
      "loss: 1.8395\n",
      "loss: 1.9639\n",
      "loss: 1.8779\n",
      "loss: 1.8013\n",
      "loss: 1.8341\n",
      "loss: 1.8429\n",
      "loss: 1.8681\n",
      "loss: 1.9464\n",
      "loss: 1.8337\n",
      "loss: 1.8956\n",
      "loss: 1.8506\n",
      "loss: 1.8610\n",
      "loss: 1.9570\n",
      "loss: 1.9643\n",
      "loss: 1.9479\n",
      "loss: 1.8247\n",
      "loss: 1.8872\n",
      "loss: 1.8454\n",
      "loss: 1.8677\n",
      "loss: 1.9196\n",
      "loss: 1.8001\n",
      "loss: 1.8962\n",
      "loss: 1.8564\n",
      "loss: 1.9101\n",
      "loss: 1.8372\n",
      "loss: 1.8536\n",
      "loss: 1.8662\n",
      "loss: 1.8796\n",
      "loss: 1.8664\n",
      "loss: 1.8706\n",
      "loss: 1.8926\n",
      "loss: 1.8132\n",
      "loss: 1.7903\n",
      "loss: 1.8482\n",
      "loss: 1.8755\n",
      "loss: 1.9097\n",
      "loss: 1.8250\n",
      "loss: 1.7914\n",
      "loss: 1.8496\n",
      "loss: 1.8396\n",
      "loss: 1.8373\n",
      "loss: 1.8996\n",
      "loss: 1.8618\n",
      "loss: 1.8980\n",
      "loss: 1.8784\n",
      "loss: 1.8960\n",
      "loss: 1.9312\n",
      "loss: 1.8420\n",
      "loss: 1.7614\n",
      "loss: 1.8708\n",
      "loss: 1.8778\n",
      "loss: 1.7880\n",
      "loss: 1.7894\n",
      "loss: 1.7596\n",
      "loss: 1.8789\n",
      "loss: 1.8510\n",
      "loss: 1.8396\n",
      "loss: 1.9093\n",
      "loss: 1.8768\n",
      "loss: 1.9065\n",
      "loss: 1.8188\n",
      "loss: 1.8332\n",
      "loss: 1.7512\n",
      "loss: 1.9112\n",
      "loss: 1.8296\n",
      "loss: 1.8829\n",
      "loss: 1.8580\n",
      "loss: 1.8196\n",
      "loss: 1.8199\n",
      "loss: 1.8412\n",
      "loss: 1.8104\n",
      "loss: 1.8592\n",
      "loss: 1.8213\n",
      "loss: 1.9555\n",
      "loss: 1.8996\n",
      "loss: 1.8192\n",
      "loss: 1.7613\n",
      "loss: 1.8315\n",
      "loss: 1.8493\n",
      "loss: 1.8774\n",
      "loss: 1.7684\n",
      "loss: 1.8280\n",
      "loss: 1.8292\n",
      "loss: 1.7982\n",
      "loss: 1.8405\n",
      "loss: 1.7708\n",
      "loss: 1.8412\n",
      "loss: 1.8245\n",
      "loss: 1.7511\n",
      "loss: 1.8509\n",
      "loss: 1.8581\n",
      "loss: 1.8394\n",
      "loss: 1.8112\n",
      "loss: 1.8312\n",
      "loss: 1.8402\n",
      "loss: 1.8119\n",
      "loss: 1.8212\n",
      "loss: 1.8812\n",
      "loss: 1.8581\n",
      "loss: 1.8996\n",
      "loss: 1.8406\n",
      "loss: 1.7811\n",
      "loss: 1.7894\n",
      "loss: 1.8417\n",
      "loss: 1.8196\n",
      "loss: 1.8609\n",
      "loss: 1.7296\n",
      "loss: 1.8693\n",
      "loss: 1.8561\n",
      "loss: 1.8718\n",
      "loss: 1.8196\n",
      "loss: 1.8211\n",
      "loss: 1.8908\n",
      "loss: 1.8625\n",
      "loss: 1.7848\n",
      "loss: 1.8624\n",
      "loss: 1.8296\n",
      "loss: 1.8513\n",
      "loss: 1.7712\n",
      "loss: 1.8308\n",
      "loss: 1.7712\n",
      "loss: 1.8980\n",
      "loss: 1.8812\n",
      "loss: 1.8695\n",
      "loss: 1.8193\n",
      "loss: 1.8609\n",
      "loss: 1.8211\n",
      "loss: 1.8411\n",
      "loss: 1.7812\n",
      "loss: 1.8776\n",
      "loss: 1.9009\n",
      "loss: 1.7996\n",
      "loss: 1.9612\n",
      "loss: 1.8603\n",
      "loss: 1.9412\n",
      "loss: 1.7711\n",
      "loss: 1.8312\n",
      "loss: 1.9496\n",
      "loss: 1.8211\n",
      "loss: 1.8596\n",
      "loss: 1.8112\n",
      "loss: 1.8097\n",
      "loss: 1.9252\n",
      "loss: 1.8612\n",
      "loss: 1.8093\n",
      "loss: 1.7812\n",
      "loss: 1.8912\n",
      "loss: 1.8612\n",
      "loss: 1.8783\n",
      "loss: 1.9696\n",
      "loss: 1.9312\n",
      "loss: 1.9196\n",
      "loss: 1.8296\n",
      "loss: 1.8294\n",
      "loss: 1.8512\n",
      "loss: 1.9596\n",
      "loss: 1.9238\n",
      "loss: 1.9210\n",
      "loss: 1.9808\n",
      "loss: 1.9493\n",
      "loss: 1.7996\n",
      "loss: 1.8694\n",
      "loss: 1.8712\n",
      "loss: 1.9412\n",
      "loss: 1.9112\n",
      "loss: 1.8593\n",
      "loss: 1.8812\n",
      "loss: 1.8812\n",
      "loss: 1.8812\n",
      "loss: 1.8512\n",
      "loss: 1.8211\n",
      "loss: 1.8312\n",
      "loss: 1.8612\n",
      "loss: 1.9412\n"
     ]
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(trainloader, 0):\n",
    "    pre_softmax = cnn(X.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "    output = softmax(pre_softmax)\n",
    "    cost = torch.nn.functional.cross_entropy(output, y)\n",
    "    cost.backward()\n",
    "    print('loss: %.4f' % cost)\n",
    "    #print('a: %.4f' % a)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_pre(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = conv_layer(X, w_conv1, p_drop_input)\n",
    "    X = conv_layer(X, w_conv2, p_drop_input)\n",
    "    X = conv_layer(X, w_conv3, p_drop_input)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 1, 1])\n",
      "torch.Size([100, 128])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 10])\n",
      "----------\n",
      "tensor([  -32.1970, -1772.8342,   271.9752,  -784.3312, -1048.0977,\n",
      "          589.0019,  -326.3761,  -128.4423,    18.9680,   281.6931])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((mb_size, 1, 28, 28)) # standard mnist tensor size\n",
    "# get output size\n",
    "X = cnn_pre(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.8)\n",
    "print(X.size())\n",
    "# reshape\n",
    "X = X.reshape(mb_size, number_of_output_pixels)\n",
    "print(X.size())\n",
    "X_test = torch.randn((mb_size, 1, 28, 28))\n",
    "pre_soft = cnn(X_test, w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "print(pre_soft.size())\n",
    "# apply softmax\n",
    "print(softmax(pre_soft).size())\n",
    "print('----------')\n",
    "print(pre_soft[0])\n",
    "print(softmax(pre_soft)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for _ in trainloader:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 128]' is invalid for input of size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-cd85b9d472b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpre_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-bc75da66ff47>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m(X, w_conv1, w_conv2, w_conv3, w_h2, w_o, p_drop_input, p_drop_hidden)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_conv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_output_pixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_drop_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 128]' is invalid for input of size 128"
     ]
    }
   ],
   "source": [
    "mb_size = 100\n",
    "idx = 19\n",
    "data_test = trainloader.dataset[idx][0] # pick first x from dataloader\n",
    "target_test = trainloader.dataset[idx][1] # pick first y from dataloader\n",
    "print(target_test.size())\n",
    "#target_test = target_test.view(-1)\n",
    "#target_test = target_test.unsqueeze(0)\n",
    "target_test.unsqueeze_(0)\n",
    "print(target_test.size())\n",
    "pre_softmax = cnn(data_test.reshape(-1, 1, 28, 28), w_conv1, w_conv2, w_conv3, w_h2, w_o, 0.8, 0.7)\n",
    "output = softmax(pre_softmax)\n",
    "print(pre_softmax.size())\n",
    "print(output.size())\n",
    "print(pre_softmax)\n",
    "print(output)\n",
    "print(output.sum())\n",
    "print(target_test.size())\n",
    "loss = torch.nn.functional.cross_entropy(pre_softmax, target_test)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4796,  1.7019,  0.2371],\n",
      "        [-0.1884,  0.3799,  0.1316],\n",
      "        [ 0.4490,  0.6000,  2.7649],\n",
      "        [-1.3500, -0.1985, -1.3028],\n",
      "        [ 0.7108, -1.0738, -0.4936]])\n",
      "(tensor([[ 1.7019],\n",
      "        [ 0.3799],\n",
      "        [ 2.7649],\n",
      "        [-0.1985],\n",
      "        [ 0.7108]]), tensor([[ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 1],\n",
      "        [ 0]]))\n",
      "tensor([[ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 1],\n",
      "        [ 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((5,3))\n",
    "print(x)\n",
    "print(x.data.max(1, keepdim = True))\n",
    "print(x.data.max(1, keepdim = True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.eq?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
